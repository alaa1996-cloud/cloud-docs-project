-- phpMyAdmin SQL Dump
-- version 5.2.0
-- https://www.phpmyadmin.net/
--
-- Host: 127.0.0.1
-- Generation Time: 05 يونيو 2025 الساعة 21:00
-- إصدار الخادم: 10.4.24-MariaDB
-- PHP Version: 8.0.19

SET SQL_MODE = "NO_AUTO_VALUE_ON_ZERO";
START TRANSACTION;
SET time_zone = "+00:00";


/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8mb4 */;

--
-- Database: `cloud_docs_db`
--

-- --------------------------------------------------------

--
-- بنية الجدول `categories`
--

CREATE TABLE `categories` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `name` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `keywords` varchar(255) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- إرجاع أو استيراد بيانات الجدول `categories`
--

INSERT INTO `categories` (`id`, `name`, `keywords`, `created_at`, `updated_at`) VALUES
(1, 'Health', NULL, NULL, NULL),
(2, 'Education', NULL, NULL, NULL),
(3, 'Technology', NULL, NULL, NULL),
(4, 'Real estate', NULL, NULL, NULL),
(16, 'غير ذلك', NULL, '2025-06-04 10:23:08', '2025-06-04 10:23:08'),
(17, 'علوم', NULL, '2025-06-04 11:35:45', '2025-06-04 11:35:45');

-- --------------------------------------------------------

--
-- بنية الجدول `documents`
--

CREATE TABLE `documents` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `title` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `filename` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `file_hash` varchar(64) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `filepath` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `content` longtext COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `size` int(11) DEFAULT NULL,
  `category_id` bigint(20) UNSIGNED DEFAULT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- إرجاع أو استيراد بيانات الجدول `documents`
--

INSERT INTO `documents` (`id`, `title`, `filename`, `file_hash`, `filepath`, `content`, `size`, `category_id`, `created_at`, `updated_at`) VALUES
(24, 'مستند 3', 'Course Key Outlines.pdf', NULL, 'documents/mhDtOhPUVjymX7eackaNIP8PxJBG5B6GZuyEWdaM.pdf', 'Software Development Methodologies \nCourse Key Outlines \nChapter 1: Software Processes Chapter 2: Waterfall Chapter 3: Spiral 	Chapter 4: Prototyping Chapter 5: RAD \n• Why we need a Software Process? \n• Process vs. Model \n• Four Development Activities \n• Software Process Descriptions \n• Process Structure \n• Plan-Driven and Agile Processes \n• Process Activities \n• Software Specification \n• Design & Implementation \n• Software Validation \n• Stages of Testing \n• Software Evolution \n• Coping with Change \n• Reducing the Costs of Rework \n \n• Introduction \n• Waterfall Model \n• 1. Requirements Phase \n• 2. Design Phase \n• 3. Implementation Phase \n• 4. Testing Phase \n• System testing \n• 5. Maintenance Phase \n• Three types of maintenance \n• When to use Waterfall? \n• Advantages & disadvantages  \n• Why Waterfall may fail? \n• Iterative Waterfall Model \n• Introduction \n• Description of Spiral Model \n• 1. Objectives Setting \n• 2. Risks Assessment & Reduction \n• 3. Development & Validation \n• 4. Planning \n• How it works? \n• When to use Spiral? \n• Advantages & disadvantages \n• Prototyping & prototype definitions \n• Where it can be used? \n• Description of the model \n• Types of the prototyping \n• 1. Throw-away prototyping \n• 2. Evolutionary Prototyping \n• 3. Incremental Prototyping \n• 4. Extreme Prototyping \n• Ways of Preparing a Prototype \n• Forms of Prototyping \n• Low-Fidelity vs. High-Fidelity \n• When to use Prototyping? \n• Advantages & disadvantages \n• Introduction \n• RAD Model Description \n• 1. Requirements Planning \n• 2. User Design \n• 3. Construction \n• 4. Cutover \n• When to use RAD? \n• Advantages & disadvantages \nChapter 6: Agile Approach Chapter 7: XP 	Chapter 8: Scrum 	Chapter 9: Kanban  \n• Introduction \n• Agile Manifesto \n• Values (4) \n• Principles (12) \n• Agile vs. Traditional \n• Agile Methods \n• Introduction \n• Values, Principles & Practices \n• Values (5) \n• Practices (12) \n• Roles in XP \n• Process of XP \n• User Stories \n• When to use \n• Advantages & disadvantages \n• Introduction \n• Values (5) \n• Scrum Terms \n• Process of Scrum \n• Practices (5) \n• Scrum Team \n• Roles in Scrum \n• Scrum artifacts (3) \n• When to use \n• Advantages & disadvantages \n \n• Introduction \n• Values (6) & Principles (4) \n• Kanban Board \n• Practices (6) \n• Kanban Measurements \n• Kanban & Scrum \n• When to use \n• Advantages & disadvantages \n \n \nX', NULL, 3, '2025-05-29 09:30:56', '2025-05-29 09:30:56'),
(27, 'مستند 5', 'مراجعه.pdf', NULL, 'documents/Ad2NhDrcYVQ8XuOCoSxf896nwtKBlk4O05eDK0ka.pdf', 'Ch 6 \n \n \n  \nDisadvantages \nof RAD \nAdvantages \nof RAD \nWhen to use RAD RAD Model Goals of RAD \n ديكأتلا صقن-1\n تابلطتملا ىلع\nةيفيظولا ريغ \n ميمصتلا -2\nفيعض \n ةداع زكرت-3\n عورشملا قيرف\n ىلا ريغصلا\nمجحلا طسوتم \n هراهملا بلطتي-4\nهيلاعلا هينهملا \n5- Less control \n \n رثكأ-1\n عيراشملا\n متي يتلا\n يف زاجنالا\n تقولا\n ددحملا\n لخادو\nةينازيملا \n دعاسي -2\n مادختسا\n تانوكملا\n ةلباقلا\n ةداعإلل\n مادختسالا\n ليلقت ىلع\n ةرود تقو\nعورشملا \n ةيذغتلا -3\n نم ةعجارلا\n ةحاتم ليمعلا\n ةلحرملا يف\nةيلوألا.  \n1- امدنع\n جاتحت\n ىلا\n زاجنالا\n عورشملا\n لكشب\nعيرس  \n2- •  امدنع\n نوكي\n ىدل\n ليمعلا\n تابلطتم\nةفورعم. \n3- •  امدنع\n بلطي ال\n ءادألا\nيلاعلا. \n \n4- • ماظنلا ةئزجت نكمي امدنع. \n \n1- Requirements \nPlanning \n2- User Design \n3- Construction \n4- Cutover \n1- صاقنإل\n تقولا\n يذلا\n هقرغتسي\n ةفلكتلاو\n يتلا\n اهلمحتي\n مظن\nتايجمربلا \n \n2- ديدحتل\n فيلاكت\n تابلطلا\nريغت يتلا \n \n3- ليلقتل\n ةوجف\n لصاوتلا\n ليمعلا نيب\nنيروطملاو\n\nCh5 \n \n \n \n \nDisadvantages \nof Prototyping \nAdvantages \nof \nPrototyping \nWhen to use Prototyping PrototypingModel Types of Prototyping Models \n1- يف فالتخالا\n لك دعب تابلطتملا\n جذومنلا رابتخا\nيبيرجتلا \n2- قئاثولا\n ببسب ةفيعضلا\n يتلا تابلطتملا\nرارمتساب ريغت \n3- ةيؤرلا دعب\n جذومنلا نم ىلوألا\n دق ،يبيرجتلا\n ءالمعلا بلطي\n يقيقحلا جتنملا\n  ابيرق \n4- رسخي دق\n مامتهالا ليمعلا\n ريغ ناك اذإ جتنملاب\n جذومنلا نع ايضار\nيلوألا \n1- نيسحت\nماظن مادختسا \n2- رثكأ ةقباطم\n تايجاحل\n نيمدختسملا\nةيقيقحلا \n نيسحت-3\nميمصتلا ةدوج \n نيسحت-4\nةنايصلا \n نم دحلا-5\n ريوطتلا تقو\nدهجلاو \n نكمي-6\n يف همادختسا\n ةهيبش عيراشم\nىرخأ \n نع فشكلا-7\n ءاطخألا\n يف لكاشملاو\nركبم \n  \n1- بجي\n مادختسا\n جذومن\n ميمصتلا\n يبيرجتلا\n ال امدنع\n ىلا لصوتي\n تابلطتم\n جتنملا\n وا حوضوب\n نوكي امدنع\nرقتسم ريغ. \n2- نكمي\n ا\n \nضيأ\n همادختسا\n ريغت امدنع\n تابلطتملا\nةعرسب. \n3- نكمي\n همادختسا\n جاتحي امدنع\n ريثكلا ىلا\n لعافتلا نم\n عم\nنيمدختسملا. \n4- نكمي\n مادختسا\n اذه جذومن\n ريوطتل حاجنب\n تاهجاو\nمدختسملا. \n1-\nRequirements \n2-Quick \nDesign \n3-Build \nPrototype \n4-User \nEvaluation \n5-Refining \nPrototype \n6-Implement \n1. Rapid Throwaway \nPrototyping  \n2. Evolutionary \nPrototyping 3. \nIncremental \nPrototyping 4. Extreme \nPrototyping\n\nCh8 \n*XP has evolved from the problems caused by the lengthy cycles of traditional methods. \n \n \n \n \n \n \n \n \n \n \n \n \n*It is intended to satisfy customers through improving the software quality \n*New versions may be built several times per day \n*Increments are delivered to customers every 2 weeks; \n*XP is built on an assumption about five values. The values are supported by \nfifteen principles which are implemented with twelve practices. \n \nXP Values  \n1-Communication  2-Feedback  3-Courage  4-Simplicity  5-Respect \n \n \nDisadvantages \nof XP \nAdvantages \nof XP \nWhen to use XP \n1- قرفلل هممصم\n ةطسوتملاو هريغصلا\n مجحلا \n2- دوكلا ىلع زيكرتلا\n ميمصتلا نم رثكا \n3- نايحلأا ضعب يف\n نوكي نا نكمي لا\nحاتم ليمعلا \n4- بلطهفرتحم قرف \n1- ريوطت\nةعرسب جمانربلا \n2- ضيفخت\n لكاشم\nعورشملا \n نم دحلا-3\nفيلاكتلا \n ميمصتلا-4\nطيسبلا \n طيطختلا-5\nلقألا \n ةرادإلا لهس-6\nعورشملا \n نيسحت-7\nجمانربلا ةدوج \n1- عيراشم\n ىلإ ةريغص\nةطسوتم \n2-\n تابلطتملا\nريغت يتلا \n3- ميلستلا\n ركبملا\nمئادلاو \n4- زيكرتلا\n ةدوج ىلع\nجمانربلا\n\nPlanning Game \n• Similar to release plans and iteration plans.  \n• The customer and team come with the list of required features.  \n• Each feature is written as a single story.  \n• The team estimate the effort and time needed for each story.  \n• Project velocity = the number of committed days per week \n • Restricted to 1 to 2 weeks. \n \nPair Programmin \n• Two developers are involved. \n• They work on one story at the same time \n \nOn-Site Customer \n• A representative of the end-user of the system (the Customer) should be \navailable full time for the use of the XP team. \n \nCoding Standards \nExamples: • Code • Documents • Naming conventions \n \nCollective Ownership \nAny developer can change any line of code to: \n • Add functionality  \n• Fix bugs  \n• Improve designs  \n• Refactor\n\nSystem Metaphor \n \n Eliminate technical speak.  \n• People should speak using metaphor not technical shortcuts.  \n• Business people do not always understand the technical terms. \n \nContinuous Integration \n*The changes are integrated into the code continuously \n \nRefactoring \n• It is a practice that allows to improve without changing or breaking its \nfunctionality. \n* • Fixing code smells - دوكلا نيسحت \n• Weak code \n • Understandable code \n • Poor structure  \n• Too complex \n \nHow refactoring is done?  \n• Rename a method • Move a method • Change a method \nSmall Releases \n• XP stresses on delivering small release to the customer. \n• So, it focuses on Minimum Market Functional (MMF)\n\n40 Hours per Week  \n• Large amounts of overtime are not considered acceptable.  \n• Enable a good working environment for the team.  \n• Support the team.  \n• Allow the teams go back home at specified times. \n • Do not always put the team under pressure (1 extra week only) \n \nProcess of XP \n• In exploration phase, the customers write out the story cards they wish to be \nincluded in the first release. \n • In planning phase, the stories are prioritized and an agreement of the \nfunctionalities of the first release is made. \n \nA Spike is an attempt to reduce the risk associated with an unknown \narea \n• The system needs additional testing of the performance in a productionizing \nphase. \n • The maintenance phase needs an extra effort for customer support tasks.  \n• The final phase is death phase and takes place when customer does no longer \nhas any stories to be developed \n \nUser Stories \n* User requirements are expressed as user stories or scenarios\n\nScrum Ch9 \n \nScrum is a lightweight framework that helps people, teams and organizations \ngenerate value through adaptive solutions for complex problems. \n**One of the most popular agile methodologies which helps companies to \nmanage projects. \nScrum Values : \n1-Commitment: \n2-Focus \n3-Openness \n4-Respect \n5-Courage \n \n***Scrum Terms*** \n1-Product Backlog \n2-Sprint \n3-Sprint Backlog \n4-Burn-down Chart \n5-User Stories \n**Scrum Terms***  \nProduct Backlog. A list of product features to be developed. \n• Sprint. A single development cycle in Scrum to develop an increment from the \nproduct. It takes to 4 weeks. \n• Sprint Backlog. A list of product features to be developed in the sprint. \n• Burn-down Chart. A progress after each sprint\n\n• User Stories. The features requested by customer. \nProcess of Scrum \n1. Sprint Planning Meeting 2. Sprint 3. Daily Scrum Meeting 4. Sprint Review 5. Retrospective \nMeeting \n1. Sprint Planning Meeting : \n• Goals:لمعلا قيرف لك عم نواعتلا لالخ نم اهدادعإ متي ةطخلا . \nDuration: 8 hours for a sprint of one month. \nOutcomes:  \nThe selection of sprint backlog from the product backlog. \n ▪ Determining the sprint goals. \n \n \n2. Sprint: \nGoals \nAll the work necessary to achieve the Product Goal, including Sprint Planning, Daily \nScrums, Sprint Review, and Sprint Retrospective, happen within Sprints. \n \n• Duration ▪ \n 1 to 4 weeks \nOutcomes \n ▪ Increment. \n \n \nNo changes are made that would endanger the sprint goal. \n • Target quality can not be decreased.  \n• The product backlog is refined as needed.  \n• Scope may be clarified and renegotiated with the product owner as more is learned. \nEach Sprint may be considered a short project. \n \n3. Daily Scrum Meeting: \nGoals: \n لل لمعلا ةطخ نوروطيو اهب نوموقي يتلا ماهملا لوح تامولعملا نوكراشي ريوطتلا قيرف ءاضعأ\nةمداقلا ةعاس 24. \nDuration \n ▪ 15 minutes daily.\n\n4. Sprint Review \nGoals  \n▪ It is to inspect the outcome of the Sprint and determine future adaptations. \nDuration  \n▪ 4 hours for a sprint of one month.  \n• Outcomes \n ▪ Product release [increment] \n \n4- 5. Retrospective Meeting \n• Goals: \nto increase quality and effectiveness \n \n• Duration \n ▪ 3 hours for a sprint of one mont \n \n**Scrum Team** \n--The scrum team consists of one Product Owner, one Scrum Master, and \nDevelopers. \n--Within a scrum team, there are no sub-teams or hierarchies. \n-- Scrum teams are cross-functional  راودلأا ددعتم \n-- They are also self-managing  ايتاذ نوريدي \n--  The team is typically 10 or fewer people \n-- The scrum team is responsible for all product-related activities  ةطشنألا لك نع لوؤسم\nجتنملاب ةقلعتملا \n \n** Product Owner: \nProduct Owner is accountable for maximizing the value of the product \n ةيلوؤسمProduct Owner: \n1- 	جتنملا ميلستل مزاللا ينمزلا لودجلا ديدحت. \n2-  ريياعملا عم جتنملا ميلستل مزاللا ينمزلا لودجلا ةقباطم\nلوبقلا. \n3- 	جتنملاب صاخلا فدهلا ريبعتو ريوطت. \nميدقتو مييقتلا نع لوؤسم وه ،لمعلا ةرتف لك ةياهن يف .قيرفلا نم ءزج وه ،لحارملا ضعب يف\n\nScrum Master: \n The Scrum Master is accountable for establishing Scrum as defined in the Scrum Guide. \nThe Scrum Master is accountable for the scrum team’s effectiveness \n \nDevelopers: \nDevelopers are the people in the scrum team that are committed to creating any aspect of a \nusable increment each sprint. \n \nDevelopers accountable:نوروطملا ماهم \n1- Creating a plan for the Sprint. \n2- Breaking down the work into several tasks. \n3- Instilling quality. \n** Product Backlog** \nThe Product Backlog is an emergent, ordered list \n \nUser Stories \nThe user stories includes 3 key aspects: \n1-Card 2-Conversation 3-Confirmation \nWhen to use Scrum? \nتابلطتملا حوضو مدع دنع.  \n• عورشملا جئاتن نم عقوتلا مدع دنع. \n • تارادصإ ةدع ىلإ جتنملا ميسقت دنع. \n • ةيريوطتلا ةطشنألا ةرادإ ىلع زيكرتلا ىلإ ةجاحلا دنع.\n\nAdvantages of Scrum \nنرمو فيكتم وه.  \n• لضفأ ةدوج لمع ىلإ ا\n \nبلاغ يدؤي.  \n• قوسلا ىلع رودصلل مزاللا تقولا نسحي \n. • ةرمتسملا تاقيلعتلا نمضي.  \n• ءالمعلا اضر نسحي.  \n• ىضر رثكأ نيفظوم ىلإ  ةداع يدؤي. \n • فيلاكتلا نم للقي. \n \nDisadvantages of Scrum \nلماشلا بيردتلا بلطتي.  \n• عيسوتلا بعصلا نم نوكي دق. \n • ةسسؤملا يف ةريبكلا تالوحتلا بلطتي دق. \n • قيرفلا ءاضعأ عيمج نم يوق مازتلا بلطتي.  \n• عيراشملا ةرادإل ةيديلقتلا ةقيرطلا عم جمدلا بعصلا نم نوكي دق. \n \nCh 9 Kanban \nWhen to use Kanban? \n ةريطخلا قرطلا مدختسا• \n ةيفافشلا ىلإ جاتحي \n •  هب مايقلا بجي يذلا لمعلا ىلع زيكرتلا• \n ةيلمعلا ديدحت \n •  لمعلا قفدت نيسحت• ةءافك رثكأ لكشب لمعلا\n\nAdvantages of Kanban \n نرم•  \nيئرم \n •  لضفأ نواعت• ةءافكلا نيسحت \n•  لضفأ ةيؤر• \n ةيجاتنإلا ةدايز \n \nDisadvantages of Kanban \nةلقتسم ةادأك اهمادختسا نكمي ال. \n • ةثدحم مكحتلا ةحول نوكي نأ بجي.  \n•   ادج ةدقعم مكحتلا ةحول حبصت دق ،نايحألا ضعب يف. \n • ةينمز تاراطإ دجوي ال.  \n•  حيرصلا راركتلا صقن \n•  جاتنإلا قفدت يف لكاشم• \n ةفيعض ةدوجب ةديج جئاتن', NULL, 3, '2025-06-01 03:26:14', '2025-06-01 03:26:14'),
(30, 'مستند 8', 'Acrynoms  Terms.pdf', NULL, 'documents/ipRCJrmFikYdRgSFbfJczdlbL0OrwpFhaWWwogpb.pdf', 'Software Development Methodologies \n \nAcronyms \nAcronym Stands for \nV & V Verification and Validation \nSRS Software Requirements Specification \nSDD Software Design Document \nMVP Minimum Viable Product \nRAD Rapid Application Development \nRAB Rapid Application Building \nSDLC Systems Development Life Cycle \nJAD Joint Application Design \nXP Extreme Programming \nMMF Minimum Market Functional \nPBI Product Backlog Items  \nDoD Definition of Done \nDoW Definition of Work \nWIP Work in Progress\n\nTerms \nTerm 	Meaning \nSoftware Process \n- A structured set of activities required to develop a software system. \n- A representation of how you put the lifecycle activities together in a sequence over time. \nsoftware process model \n(methodology) \n- An abstract representation of a process. It presents a description of a process from some particular perspective. \n- It describes “how” to perform the development activities. \nPhase 	A collection of logically related activities \nActivity 	A set of purposeful tasks \nTask 	A piece of work to be done \nPlan-driven processes Processes where all of the process activities are planned in advance and progress is measured against this plan. \nSoftware Specification The process of establishing what services are required and the constraints on the system’s operation and development \nDesign and Implementation The process of converting the system specification into an executable system \nDebugging 	The activity of finding program faults and correcting these faults. \nSystem prototyping \nA version of the system or part of the system is developed quickly to check the customer’s requirements and the feasibility of design \ndecisions. \nIncremental delivery A type of delivery where the system increments are delivered to the customer for comment and experimentation. \nUnit testing 	Small modules are tested in isolation initially. \nIntegration testing Modules are tested by writing some code to check the interaction between these modules. \nAlpha testing 	A system testing performed by the development team. \nBeta testing 	A system testing performed by a friendly set of customers. \nAcceptance testing A testing to determine whether to accept the delivered software or reject it. \nCorrective Maintenance A type of maintenance is carried out to correct errors that were not discovered during the product development phase. \nPerfective Maintenance A type of maintenance is carried out to enhance the functionalities of the system based on the customer’s request. \nAdaptive Maintenance \nA type of maintenance is usually required for porting the software to work in a new environment such as working on a new computer \nplatform or with a new operating system. \nIterative Waterfall Model Incorporating the necessary changes to the classical waterfall model to make it usable in practical software development projects. \nSpiral Model 	A risk-driven software process framework that integrates risk management and incremental development. \nPrototyping \n- An experimental process where design teams implement ideas into tangible forms from paper to digital. \n- The process of developing a working replication of a product or system that has to be engineered. \nPrototype 	An initial version of a system used to demonstrate concepts and try out design options. \nThrow-away prototyping A type of prototyping useful for exploring ideas and getting customer feedback for each of them. \nEvolutionary Prototyping The prototype developed initially is incrementally refined on the basis of customer feedback till it finally gets accepted.\n\nTerm 	Meaning \nIncremental Prototyping A type of Prototyping, the final expected product is broken into different small pieces of prototypes and being developed individually \nExtreme Prototyping A type of Prototyping is mainly used for web development \nPair programming \nTwo developers are involved in activity where they work on one story at the same time. One drive and one navigate, they often \nswitch. \nOn-Site Customer A representative of the end-user of the system (the Customer) should be available full time for the use of the XP team. \nCollective Ownership Encourage everyone to contribute new ideas to all the segments of the project \nSystem Metaphor People should speak using metaphor not technical shortcuts. \nContinuous Integration The changes are integrated into the code continuously and a number of tests should be performed before and after the integration. \nRefactoring 	It is a practice that allows to improve without changing or breaking its functionality. \nSpike \nAn attempt to reduce the risk associated with an unknown area. It may involve allocating some timeframes for some investigation \nand research. \nUser Story 	A User requirement is written in a small card. \nScrum 	An agile method that focuses on managing iterative development rather than specific agile practices. \nProduct Backlog \n- A list of all product features to be developed. \n- An emergent, ordered list of what is needed to improve the product. \nSprint 	A single development cycle in Scrum to develop an increment from the product. It takes to 4 weeks \nSprint Backlog \n- A list of product features to be developed in the sprint. \n- is composed of the Sprint Goal (why), the set of Product Backlog items selected for the Sprint (what), as well as an actionable plan \nfor delivering the Increment (how). \nProduct Owner \n Is accountable for maximizing the value of the product. He is the source of system requirements and is also responsible of effective \nProduct Backlog management \nScrum Master  Is accountable for establishing Scrum as defined in the Scrum Guide \nScrum Development team Are the people in the scrum team that are committed to creating any aspect of a usable increment each sprint. \nProduct Backlog refinement The act of breaking down and further defining Product Backlog items into smaller more precise items. \nStory card 	A small piece of paper in which the story is written, estimated time, priority and some other information. \nDefinition of Done A formal description of the state of the Increment when it meets the quality measures required for the product. \nKanban 	A lean methodology that describes methods for improving any process or workflow. \nWork in Progress (WIP) Refers to the work items the team has started but has not yet finished.    \nWork in Progress (WIP) limit Mechanism to limit the number of cards at any column \nThroughput 	The number of work items finished per unit of time. \nCycle Time 	The amount of elapsed time between when a work item started and when a work item finished.', NULL, 3, '2025-06-01 07:09:07', '2025-06-01 07:09:07'),
(33, 'مستند 44', 'عقد ايجار شقة سكنية (2).docx', NULL, 'documents/2PLdQTWSjpoz4XbKu38jcPwEj942qFkP6xwD5JvH.docx', NULL, NULL, 4, '2025-06-01 07:38:34', '2025-06-01 07:38:34'),
(39, 'مستند تجريبي', 'Assignment_02 (1).pdf', NULL, 'documents/weXfa6v1cA5CfiLgRfaO5tCk7oVWwqye5opqir3x.pdf', 'Best Wishes \n     Academic Year: 2022/2023 \n     Semester: First \n     Faculty: Information Technology \n     Department: Software Development \n \nCourse No.: SDEV 3309 \nCourse: Software Development Methods \nInstructor: Dr. Tamer Nazir Madi \nDate: 17/10/2022  \n \n \n \nAssignment No. 2: Spiral Model \n \nThe goal of this assignment is to introduce the students to the implementation of the Spiral Model. \nThis assignment is a group assignment where each group should not exceed 3 students. Each group \nshould select one of the following projects’ ideas and implement the spiral model to the selected \nproject as described in the attached template. \n• Online banking system \n• Insulin pump system \n• Railway/aircraft operating and control system \n• Navigation system \n• Emergency communications system \n• Electric power system \n• Traffic management system \n \nAssignment Policy: \n▪ Each group must use the attached template, but they can add more sections. \n▪ The student’s answer sheet must be submitted as a pdf file on Moodle. \n▪ Submission after the due date will be rejected. \n▪ Copy and paste will be given a Big Zero. \n▪ Due Date: 27/10/2022', 105, 2, '2025-06-01 17:28:52', '2025-06-01 17:28:52'),
(41, 'test', 'Course Key Outlines (2).pdf', NULL, 'documents/Zi6sn8iRPknebwEBNvyYKvvkylT6xc8yqBF4Mk97.pdf', 'Software Development Methodologies \nCourse Key Outlines \nChapter 1: Software Processes Chapter 2: Waterfall Chapter 3: Spiral 	Chapter 4: Prototyping Chapter 5: RAD \n• Why we need a Software Process? \n• Process vs. Model \n• Four Development Activities \n• Software Process Descriptions \n• Process Structure \n• Plan-Driven and Agile Processes \n• Process Activities \n• Software Specification \n• Design & Implementation \n• Software Validation \n• Stages of Testing \n• Software Evolution \n• Coping with Change \n• Reducing the Costs of Rework \n \n• Introduction \n• Waterfall Model \n• 1. Requirements Phase \n• 2. Design Phase \n• 3. Implementation Phase \n• 4. Testing Phase \n• System testing \n• 5. Maintenance Phase \n• Three types of maintenance \n• When to use Waterfall? \n• Advantages & disadvantages  \n• Why Waterfall may fail? \n• Iterative Waterfall Model \n• Introduction \n• Description of Spiral Model \n• 1. Objectives Setting \n• 2. Risks Assessment & Reduction \n• 3. Development & Validation \n• 4. Planning \n• How it works? \n• When to use Spiral? \n• Advantages & disadvantages \n• Prototyping & prototype definitions \n• Where it can be used? \n• Description of the model \n• Types of the prototyping \n• 1. Throw-away prototyping \n• 2. Evolutionary Prototyping \n• 3. Incremental Prototyping \n• 4. Extreme Prototyping \n• Ways of Preparing a Prototype \n• Forms of Prototyping \n• Low-Fidelity vs. High-Fidelity \n• When to use Prototyping? \n• Advantages & disadvantages \n• Introduction \n• RAD Model Description \n• 1. Requirements Planning \n• 2. User Design \n• 3. Construction \n• 4. Cutover \n• When to use RAD? \n• Advantages & disadvantages \nChapter 6: Agile Approach Chapter 7: XP 	Chapter 8: Scrum 	Chapter 9: Kanban  \n• Introduction \n• Agile Manifesto \n• Values (4) \n• Principles (12) \n• Agile vs. Traditional \n• Agile Methods \n• Introduction \n• Values, Principles & Practices \n• Values (5) \n• Practices (12) \n• Roles in XP \n• Process of XP \n• User Stories \n• When to use \n• Advantages & disadvantages \n• Introduction \n• Values (5) \n• Scrum Terms \n• Process of Scrum \n• Practices (5) \n• Scrum Team \n• Roles in Scrum \n• Scrum artifacts (3) \n• When to use \n• Advantages & disadvantages \n \n• Introduction \n• Values (6) & Principles (4) \n• Kanban Board \n• Practices (6) \n• Kanban Measurements \n• Kanban & Scrum \n• When to use \n• Advantages & disadvantages \n \n \nX', 86, 3, '2025-06-02 15:26:28', '2025-06-02 15:26:28'),
(44, 'test 2', 'مراجعه.docx', NULL, 'documents/I5zPwKhHfsrwSVnIrVTU8umrg1VZaWlU6IOm87Dw.docx', 'Disadvantages of RAD Advantages of RAD When to use RAD RAD Model Goals of RAD 1- نقص التأكيد على المتطلبات غير الوظيفية 2- التصميم ضعيف 3-تركز عادة فريق المشروع الصغير الى متوسط الحجم 4-يتطلب المهاره المهنيه العاليه 5-  Less control 1- أكثر المشاريع التي يتم الانجاز في الوقت المحدد وداخل الميزانية 2-  يساعد استخدام المكونات القابلة للإعادة الاستخدام على تقليل وقت دورة المشروع 3-  التغذية الراجعة من العميل متاحة في المرحلة الأولية .   عندما تحتاج الى الانجاز المشروع بشكل سريع   •  عندما يكون لدى العميل متطلبات معروفة . •  عندما لا يطلب الأداء العالي . 4- •  عندما يمكن تجزئة النظام . Requirements Planning User Design Construction Cutover لإنقاص الوقت الذي يستغرقه والتكلفة التي يتحملها نظم البرمجيات لتحديد تكاليف الطلبات التي تغير لتقليل فجوة التواصل بين العميل والمطورين Ch 6 Disadvantages of  Prototyping Advantages of  Prototyping When to use  Prototyping Prototyping Model Types of Prototyping Models 1 - الاختلاف في المتطلبات بعد كل اختبار النموذج التجريبي 2- الوثائق الضعيفة بسبب المتطلبات التي تغير باستمرار 3- بعد الرؤية الأولى من النموذج التجريبي، قد يطلب العملاء المنتج الحقيقي قريباً 4- قد يخسر العميل الاهتمام بالمنتج إذا كان غير راضيا عن النموذج الأولي 1 - تحسين استخدام نظام 2- مطابقة أكثر لحاجيات المستخدمين الحقيقية 3- تحسين جودة التصميم 4- تحسين الصيانة 5- الحد من وقت التطوير والجهد 6- يمكن استخدامه في مشاريع شبيهة أخرى 7- الكشف عن الأخطاء والمشاكل في مبكر   1- يجب استخدام نموذج التصميم التجريبي عندما لا يتوصل الى متطلبات المنتج بوضوح او عندما يكون غير مستقر . 2- يمكن أيضًا استخدامه عندما تغير المتطلبات بسرعة . 3- يمكن استخدامه عندما يحتاج الى الكثير من التفاعل مع المستخدمين . 4- يمكن استخدام نموذج هذا بنجاح لتطوير واجهات المستخدم . 1- Requirements 2- Quick Design 3- Build Prototype 4- User Evaluation 5- Refining Prototype 6- Implement 1. Rapid Throwaway Prototyping  2. Evolutionary Prototyping 3. Incremental Prototyping 4. Extreme Prototyping Ch5 Disadvantages of  XP Advantages of  XP When to use  XP 1- مصممه للفرق الصغيره والمتوسطة الحجم  2-التركيز على الكود  اكثر  من التصميم  3-في بعض الأحيان لا يمكن ان يكون العميل متاح 4-طلب فرق محترفه 1- تطوير البرنامج بسرعة 2- تخفيض مشاكل المشروع 3- الحد من التكاليف 4- التصميم البسيط 5- التخطيط الأقل 6- سهل الإدارة المشروع 7- تحسين جودة البرنامج 1- مشاريع صغيرة إلى متوسطة 2- المتطلبات التي تغير 3- التسليم المبكر والدائم 4- التركيز على جودة البرنامج Ch8 *XP has evolved from the problems caused by the  lengthy cycles  of traditional methods. *It is intended to  satisfy customers  through improving the  software quality * New versions  may be built  several times per day * Increments  are delivered to customers every  2 weeks ; * XP is built on an assumption about  five values . The values are supported by  fifteen principles  which are implemented with  twelve practices . XP Values   1-Communication  2-Feedback  3-Courage  4-Simplicity  5-Respect Planning Game • Similar to release plans and iteration plans.  • The customer and team come with the list of required features.  • Each feature is written as a single story.  • The team estimate the effort and time needed for each story.  • Project velocity = the number of committed days per week  • Restricted to 1 to 2 weeks. Pair Programmin • Two developers are involved. • They work on one story at the same time On-Site Customer • A representative of the end-user of the system ( the Customer ) should be available  full time  for the use of the XP team. Coding Standards Examples: • Code • Documents • Naming conventions Collective Ownership Any developer can change any line of code to:  • Add functionality  • Fix bugs  • Improve designs  • Refactor System Metaphor Eliminate technical speak.  • People should speak using metaphor not technical shortcuts.  • Business people do not always understand the technical terms. Continuous Integration *The  changes  are integrated into the code  continuously Refactoring • It is a practice that allows to  improve without changing  or breaking its functionality. *  • Fixing code smells  - تحسين الكود   • Weak code  • Understandable code  • Poor structure  • Too complex How refactoring is done?   • Rename a method • Move a method • Change a method Small Releases • XP stresses on delivering  small release  to the customer. • So, it focuses on  Minimum Market Functional  (MMF) 40 Hours per Week   • Large amounts of overtime are not considered acceptable.  • Enable a good working environment for the team.  • Support the team.  • Allow the teams go back home at specified times.  • Do not always put the team under pressure (1 extra week only) Process of XP • In  exploration phase , the customers  write out the story  cards they wish to be included in the first release.  • In  planning phase , the stories are prioritized and  an agreement  of the functionalities of the first release is made. A  Spike  is an attempt to reduce the risk associated with an unknown area • The system needs  additional testing of the performance  in a  productionizing phase.  •  The maintenance phase  needs an extra effort for customer support tasks.  • The final phase is death phase and takes place when customer does no longer has any stories to be developed User Stories *   User requirements  are expressed as  user stories  or  scenarios                                                                Scrum  C h 9 Scrum   is a lightweight  framework  that helps people, teams and organizations generate value through adaptive solutions for complex problems. ** One of the  most popular   agile methodologies which helps companies to manage projects. Scrum  Values  : 1- Commitment: 2-Focus 3-Openness 4-Respect 5-Courage ***Scrum Terms*** 1 - Product Backlog 2-Sprint 3-Sprint Backlog 4-Burn-down Chart 5-User Stories ** Scrum Terms ***   Product Backlog . A list of product features to be developed . • Sprint.   A single development cycle in Scrum to develop an increment from the product. It takes to 4 weeks. •  Sprint Backlog.   A list of product features to be developed in the sprint. •  Burn-down Chart .  A progress after each sprint •  User Stories.   The features requested by customer . Process of Scrum Sprint Planning Meeting   2. Sprint 3. Daily Scrum Meeting 4. Sprint Review 5. Retrospective Meeting Sprint Planning  Meeting : • Goals:   الخطة يتم إعدادها من خلال التعاون مع كل فريق العمل . Duration :   8 hours for a sprint of one month. Outcomes :   The selection of sprint backlog from the product backlog.  ▪ Determining the sprint goals. Sprint : Goals All the work necessary to achieve the Product Goal, including Sprint Planning, Daily Scrums, Sprint Review, and Sprint Retrospective, happen within Sprints. •  Duration  ▪  1 to 4 weeks Outcomes  ▪ Increment. No changes  are made that would endanger the sprint goal.  •  Target quality  can not be decreased.  • The  product backlog  is refined as needed.  • Scope may be clarified and renegotiated with  the product owner  as more is learned. Each Sprint may be considered a  short project . Daily Scrum Meeting : Goals: أعضاء فريق التطوير يشاركون المعلومات حول المهام التي يقومون بها ويطورون خطة العمل لل 24 ساعة القادمة . Duration  ▪ 15 minutes daily. Sprint Review Goals  ▪ It is to inspect the outcome of the Sprint and determine  future adaptations . Duration   ▪ 4 hours for a sprint of one month.  • Outcomes  ▪ Product release [increment] 5. Retrospective Meeting • Goals: to increase quality and effectiveness • Duration  ▪ 3 hours for a sprint of one mont ** Scrum Team ** --The scrum team consists of one  Product Owner , one  Scrum Master , and  Developers. --Within a scrum team, there are  no sub-teams  or  hierarchies . --   Scrum teams are cross-functional  متعدد الأدوار  --   They are also self-managing  يديرون ذاتيا  --    The  team is typically 10 or fewer people --   The scrum team is responsible for all product-related activities  مسؤول عن كل الأنشطة المتعلقة بالمنتج ** Product Owner : Product Owner is accountable for  maximizing the value  of the product مسؤولية  Product Owner : تحديد الجدول الزمني اللازم لتسليم المنتج . مطابقة الجدول الزمني اللازم لتسليم المنتج مع المعايير القبول . تطوير وتعبير الهدف الخاص بالمنتج . في بعض المراحل، هو جزء من الفريق. في نهاية كل فترة العمل، هو مسؤول عن التقييم وتقديم Scrum Master :   The Scrum Master is  accountable for establishing  Scrum as defined in the Scrum Guide. The Scrum Master  is  accountable for the scrum team’s effectiveness Developers: Developers are the people in the scrum team that are committed to  creating any aspect  of a usable increment each sprint. Developers   accountable : مهام  المطورون Creating a plan for the Sprint. Breaking down the work into several tasks. Instilling quality . ** Product Backlog** The Product Backlog is an emergent,  ordered list User Stories The user stories  includes  3 key aspects : 1- Card  2- Conversation  3- Confirmation When to use Scrum? عند عدم وضوح المتطلبات .  •  عند عدم التوقع من نتائج المشروع .  •  عند تقسيم المنتج إلى عدة إصدارات .  •  عند الحاجة إلى التركيز على إدارة الأنشطة التطويرية . Advantages of Scrum هو متكيف ومرن .  •  يؤدي غالبًا إلى عمل جودة أفضل .  •  يحسن الوقت اللازم للصدور على السوق . •  يضمن التعليقات المستمرة .  •  يحسن رضا العملاء .  •  يؤدي عادةً إلى موظفين أكثر رضى .  •  يقلل من التكاليف . Disadvantages of Scrum يتطلب التدريب الشامل .  •  قد يكون من الصعب التوسيع .  •  قد يتطلب التحولات الكبيرة في المؤسسة .  •  يتطلب التزام قوي من جميع أعضاء الفريق .  •  قد يكون من الصعب الدمج مع الطريقة التقليدية لإدارة المشاريع . Ch 9  Kanban When to use Kanban? استخدم الطرق الخطيرة  •   يحتاج إلى الشفافية   •  التركيز على العمل الذي يجب القيام به  •   تحديد العملية   •  تحسين تدفق العمل  •  العمل بشكل أكثر كفاءة Advantages of Kanban مرن  •  مرئي   •  تعاون أفضل  •  تحسين الكفاء ة •  رؤية أفضل  •   زيادة الإنتاجية Disadvantages of Kanban لا يمكن استخدامها كأداة مستقلة .  •  يجب أن يكون لوحة التحكم محدثة .  •  في بعض الأحيان، قد تصبح لوحة التحكم معقدة جداً .  •  لا يوجد إطارات زمنية .  •  نقص التكرار الصريح  •  مشاكل في تدفق الإنتاج  •   نتائج جيدة بجودة ضعيفة', 91, 2, '2025-06-03 12:04:29', '2025-06-03 12:04:29'),
(45, 'test 3', 'Alis-Resume-imp (1).pdf', NULL, 'documents/P0zULT5ynIu1F4kLRCr3XU9tNMvFYxVxUo9AqhhR.pdf', 'Ali Y. M. Dalloul \nOutstanding Freelance Translator, Proofreader, Editer, Localizer, Subtitler, Transcriber,\nTutor, Teacher, Copywriter, Content Writer, and Data Entry Clerk \nI will be the spark that will take your business to the next level, I hate using ordinary ways of translation and writing, but\nrather I care about quality, not quantity, your client\'s satisfaction will be my ﬁrst priority and if you are looking for a\nprofessional freelancer, you have just come to the right place. \nAliY.dalloul@gmail.com 	+970593459481 \nTal el hawa st., Gaza, Palestine 	April 26, 2003 \nlinkedin.com/in/%D8%B9%D9%84%D9%8A-\n%D8%A7%D8%A8%D9%88-\n%D8%B3%D9%84%D9%85%D8%A7%D9%86-919538233 \ntwitter.com/dall16318917 \nlive:.cid.d35ba80d1b9b1571 	facebook.com/ola.zeyad.3 \ninstagram.com/ali_dalloul_p7 \nWORK EXPERIENCE \nTuter of English&Math \nSelf-employed \n07/2021 - 02/2022,  	Gaza, Palestine \nKeep a record of students’ attendance and grades \nSubtitler& Transcriper \nself-employed \n05/2022 - Present,  	Gaza, Palestine \nAdhering to all conﬁdential guidelines and respecting sensitive information \nArabic<>English Translation \nSelf-employed \n02/2020 - Present,  	Gaza, Palestine \nReading material and researching every terminology \nContent Writer&Copy Writer \nSelf-employed \n05/2017 - Present,  	Gaza, Palestine \nPrepare well-structured drafts using Content Management Systems \nData Entry Clerk \nSelf-employed \n03/2015 - Present,  	Gaza, Palestine \nCorrecting errors and organizing the information in a manner that will optimize swift\nand accurate capturing. \nEDUCATION \nB.A. in Arabic<>English Translation \nAl-Azhar University of Gaza \n09/2021 - Present,  	Gaza ,Palestine \nLegal Translatin Cource Busniss Translation Courcr \nMedia Translaion Cource English Poetry Cource \nApplied Linguistics Cource Oral communicatin skills course\nSKILLS \nTranslation Editing Proofreading subtitling \nTranscrption formatting Revision Data Entry \nCopy Writing Content Writing Research \nSummarizing Teaching Tutoring Paraphrasing \nLANGUAGES \nArabic \nEnglish \nTurkish \nFrench \nTOOLS \nMicrosoft Oﬃce Kilgary MemoQ SDL Trados Studio \nMemsource Aegisub Handbrake Subtitle Edit \nFormat Factory WordFast Pro Foxit phantom PDF \nZoom \nCourses', 68, 2, '2025-06-03 12:05:23', '2025-06-03 12:05:23'),
(46, 'test 4', 'Data Mining.docx', NULL, 'documents/pVznfNkM3nAq5XIOQQxlXs79GCKlSNaKL4EeV1AF.docx', 'Data Mining Data Science Data Mining is a technique. Data Science is an  area. .   علم البيانات هو  مجال . It is about collection, processing, analyzing and utilizing of data into various operations. It is more conceptual.   يتعلق الأمر بجمع ومعالجة وتحليل واستخدام البيانات في عمليات مختلفة. إنه مفاهيمي أكثر . It is about extracting the vital and valuable information from the data. يتعلق الأمر باستخراج المعلومات الحيوية والقيمة من البيانات It is a field of study just like the Computer Science, Applied Statistics or Applied Mathematics. إنه مجال للدراسة تمامًا مثل علوم الكمبيوتر أو الإحصاء التطبيقي أو الرياضيات التطبيقية. It is a technique which is a part of the Knowledge Discovery in Data Base processes (KDD).   إنها تقنية تعد جزءًا من اكتشاف المعرفة في عمليات قاعدة البيانات ( KDD ). data mining mostly deals with structured data.   يتعامل التنقيب عن البيانات في الغالب مع البيانات المنظمة. data science deals with every type of data whether structured, semi-structured, or unstructured.  يتعامل علم البيانات مع كل نوع من البيانات سواء كانت منظمة أو شبه منظمة أو غير منظمة.', 15, 3, '2025-06-03 12:06:06', '2025-06-03 12:06:06'),
(47, 'test 4', '120142001.docx', NULL, 'documents/ScOmR8Jo8IBPtyvssJIWwiBxIT93TpyRjlzWi9tq.docx', 'Map/Reduce What is Map/Reduce and why was it designed? MapReduce is a batch processing programming paradigm that enables massive scalability across a large number of servers in a Hadoop cluster. It was published in 2004 and was called “the algorithm that makes Google so massively scalable.” How does the model works?  MapReduce works by dividing the job into two tasks. However, the complete execution process is controlled by two types of entities: 1.jobtracker:   Acts like a  master. 2. multiple task trackers:   Acts like  slaves. For every submitted job, there is one  Jobtracker  that resides on  Namenode , and there are multiple task trackers that reside on  Datanode . Give an example of how it works? Consider the problem of counting the number of occurrences of each word in a large collection of documents. The user would write code similar to the following pseudo-code: map( String key, String value):  // key: document name  // value: document contents  for each word w in value:   EmitIntermediate ( w, \"1\"); reduce( String key, Iterator values):  // key: a word  // values: a list of counts int result = 0;  for each v in values: result +=  ParseInt (v);  Emit( AsString (result)); What new and recent implementations/realizations of MapReduce? This section describes an implementation targeted to the computing environment in wide use at Google: 1. Machines are typically dual-processor x86 processors running Linux, with 2-4 GB of memory per machine.   (2) Commodity networking hardware is used – typically either 100 megabits/second or 1 gigabit/second at the machine level, but averaging considerably less in  over all  bisection bandwidth. (3) A cluster consists of hundreds or thousands of  ma chines , and therefore machine failures are common. (4) Storage is provided by inexpensive IDE disks at  tached  directly to individual machines. A distributed file system [8] developed in-house is used to manage the data stored on these disks (5) Users submit jobs to a scheduling system. Each job consists of a set of tasks, and is mapped by the scheduler to a set of available machines within a cluster.', 14, 2, '2025-06-03 12:06:47', '2025-06-03 12:06:47');
INSERT INTO `documents` (`id`, `title`, `filename`, `file_hash`, `filepath`, `content`, `size`, `category_id`, `created_at`, `updated_at`) VALUES
(53, 'سؤيؤيسر', 'chap5_alternative_classification.pdf', NULL, 'documents/VplPML26XY9AiVpZL7GMJBoanNuhnN6f9cKPJJ65.pdf', 'Data Mining \nClassification: Alternative Techniques\nLecture Notes for Chapter 5\nIntroduction to Data Mining\nby\nTan, Steinbach, Kumar\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               1\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               2\nRule-Based Classifier\n\0OClassify records by using a collection of \n“if…then…” rules\n\0ORule:    (Condition) →y\n–where \n\0‹Conditionis a conjunctions of attributes \n\0‹yis the class label\n–LHS: rule antecedent or condition\n–RHS: rule consequent\n–Examples of classification rules:\n\0‹(Blood Type=Warm) ∧(Lay Eggs=Yes) →Birds\n\0‹(Taxable Income < 50K) ∧(Refund=Yes) →Evade=No\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               3\nRule-based Classifier (Example)\nR1: (Give Birth = no) ∧(Can Fly = yes) →Birds\nR2: (Give Birth = no) ∧(Live in Water = yes) →Fishes\nR3: (Give Birth = yes) ∧(Blood Type = warm) →Mammals\nR4: (Give Birth = no) ∧(Can Fly = no) →Reptiles\nR5: (Live in Water = sometimes) →Amphibians\nName Blood Type Give Birth Can Fl	yLive in Water Class\nhuman warm yes no no mammals\npython cold no no no reptiles\nsalmon cold no no yes fishes\nwhale warm yes no yes mammals\nfrog cold no no sometimes amphibians\nkomodo cold no no no reptiles\nbat warm yes yes no mammals\npigeon warm no yes no birds\ncat warm yes no no mammals\nleopard shark cold yes no yes fishes\nturtle cold no no sometimes reptiles\npenguin warm no no sometimes birds\nporcupine warm yes no no mammals\neel cold no no yes fishes\nsalamander cold no no sometimes amphibians\ngila monster cold no no no reptiles\nplatypus warm no no no mammals\nowl warm no yes no birds\ndolphin warm yes no yes mammals\neagle warm no yes no birds\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               4\nApplication of Rule-Based Classifier\n\0OA rule rcoversan instance x if the attributes of \nthe instance satisfy the condition of the rule\nR1: (Give Birth = no) ∧(Can Fly = yes) →Birds\nR2: (Give Birth = no) ∧(Live in Water = yes) →Fishes\nR3: (Give Birth = yes) ∧(Blood Type = warm) →Mammals\nR4: (Give Birth = no) ∧(Can Fly = no) →Reptiles\nR5: (Live in Water = sometimes) →Amphibians \nThe rule R1 covers a hawk => Bird\nThe rule R3 covers the grizzly bear => Mammal\nName Blood Type Give Birth Can Fl	yLive in Water Class\nhawk warm no yes no ?\ngrizzly bear warm yes no no ?\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               5\nRule Coverage and Accuracy\n\0OCoverage of a rule:\n–Fraction of records \nthat satisfy the \nantecedent of a rule\n\0OAccuracy of a rule:\n–Fraction of records \nthat satisfy both the \nantecedent and \nconsequent of a \nrule\nTid Refund Marital \nStatus \nTaxable \nIncome Class \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n \n(Status=Single) →No\nCoverage = 40%,  Accuracy = 50%\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               6\nHow does Rule-based Classifier Work?\nR1: (Give Birth = no) ∧(Can Fly = yes) →Birds\nR2: (Give Birth = no) ∧(Live in Water = yes) →Fishes\nR3: (Give Birth = yes) ∧(Blood Type = warm) →Mammals\nR4: (Give Birth = no) ∧(Can Fly = no) →Reptiles\nR5: (Live in Water = sometimes) →Amphibians \nA lemur triggers rule R3, so it is classified as a mammal\nA turtle triggers both R4 and R5\nA dogfish shark triggers none of the rules\nName Blood Type Give Birth Can Fly Live in Wate	r Class\nlemur warm yes no no ?\nturtle cold no no sometimes ?\ndogfish shark cold yes no yes ?\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               7\nCharacteristics of Rule-Based Classifier\n\0OMutually exclusive rules\n–Classifier contains mutually exclusive rules if \nthe rules are independent of each other\n–Every record is covered by at most one rule\n\0OExhaustive rules\n–Classifier has exhaustive coverage if it \naccounts for every possible combination of \nattribute values\n–Each record is covered by at least one rule\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               8\nFrom Decision Trees To Rules\nYESYESNONO\nNONO\nNONO\nYes No\n{Married}\n{Single, \nDivorced}\n< 80K > 80K\nTaxable \nIncome\nMarital \nStatus\nRefund\nClassification Rules\n(Refund=Yes) ==> No\n(Refund=No, Marital Status={Single,Divorced},\nTaxable Income<80K) ==> No\n(Refund=No, Marital Status={Single,Divorced},\nTaxable Income>80K) ==> Yes\n(Refund=No, Marital Status={Married}) ==> No\nRules are mutually exclusive and exhaustive\nRule set contains as much information as the \ntree\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               9\nRules Can Be Simplified\nYESYESNONO\nNONO\nNONO\nYes No\n{Married}\n{Single, \nDivorced}\n< 80K > 80K\nTaxable \nIncome\nMarital \nStatus\nRefund\nTid Refund Marital \nStatus \nTaxable \nIncome Cheat \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n \nInitial Rule:           (Refund=No) ∧(Status=Married) →No\nSimplified Rule:   (Status=Married) →No\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               10\nEffect of Rule Simplification\n\0ORules are no longer mutually exclusive\n–A record may trigger more than one rule \n–Solution?\n\0‹Ordered rule set\n\0‹Unordered rule set – use voting schemes\n\0ORules are no longer exhaustive\n–A record may not trigger any rules\n–Solution?\n\0‹Use a default class\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               11\nOrdered Rule Set\n\0ORules are rank ordered according to their priority\n–An ordered rule set is known as a decision list\n\0OWhen a test record is presented to the classifier \n–It is assigned to the class label of the highest ranked rule it has \ntriggered\n–If none of the rules fired, it is assigned to the default class\nR1: (Give Birth = no) ∧(Can Fly = yes) →Birds\nR2: (Give Birth = no) ∧(Live in Water = yes) →Fishes\nR3: (Give Birth = yes) ∧(Blood Type = warm) →Mammals\nR4: (Give Birth = no) ∧(Can Fly = no) →Reptiles\nR5: (Live in Water = sometimes) →Amphibians \nName Blood Type Give Birth Can Fly Live in Water Class\nturtle cold no no sometimes ?\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               12\nRule Ordering Schemes\n\0ORule-based ordering\n–Individual rules are ranked based on their quality\n\0OClass-based ordering\n–Rules that belong to the same class appear together\nRule-based Ordering\n(Refund=Yes) ==> No\n(Refund=No, Marital Status={Single,Divorced},\nTaxable Income<80K) ==> No\n(Refund=No, Marital Status={Single,Divorced},\nTaxable Income>80K) ==> Yes\n(Refund=No, Marital Status={Married}) ==> No\nClass-based Ordering\n(Refund=Yes) ==> No\n(Refund=No, Marital Status={Single,Divorced},\nTaxable Income<80K) ==> No\n(Refund=No, Marital Status={Married}) ==> No\n(Refund=No, Marital Status={Single,Divorced},\nTaxable Income>80K) ==> Yes\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               13\nBuilding Classification Rules\n\0ODirect Method: \n\0‹Extract rules directly from data\n\0‹e.g.: RIPPER, CN2, Holte’s 1R\n\0OIndirect Method:\n\0‹Extract rules from other classification models (e.g. \ndecision trees, neural networks, etc).\n\0‹e.g: C4.5rules\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               14\nDirect Method: Sequential Covering\n1.Start from an empty rule\n2.Grow a rule using the Learn-One-Rule function\n3.Remove training records covered by the rule\n4.Repeat Step (2) and (3) until stopping criterion \nis met\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               15\nExample of Sequential Covering\n(i) Original Data	(ii) Step 1\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               16\nExample of Sequential Covering…\n(iii) Step 2\nR1\n(iv) Step 3\nR1\nR2\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               17\nAspects of Sequential Covering\n\0ORule Growing\n\0OInstance Elimination\n\0ORule Evaluation\n\0OStopping Criterion\n\0ORule Pruning\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               18\nRule Growing\n\0OTwo common strategies \nStatus =\nSingle\nStatus =\nDivorced\nStatus =\nMarried\nIncome\n> 80K...\nYes: 3\nNo:  4\n{ }\nYes: 0\nNo:  3\nRefund=\nNo\nYes: 3\nNo:  4\nYes: 2\nNo:  1\nYes: 1\nNo:  0\nYes: 3\nNo:  1\n(a) General-to-specific\nRefund=No,\nStatus=Single,\nIncome=85K\n(Class=Yes)\nRefund=No,\nStatus=Single,\nIncome=90K\n(Class=Yes)\nRefund=No,\nStatus = Single\n(Class = Yes)\n(b) Specific-to-general\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               19\nRule Growing (Examples)\n\0OCN2 Algorithm:\n–Start from an empty conjunct:  {}\n–Add conjuncts that minimizes the entropy measure:     {A}, {A,B}, …\n–Determine the rule consequent by taking majority class of instances \ncovered by the rule\n\0ORIPPER Algorithm:\n–Start from an empty rule: {} => class\n–Add conjuncts that maximizes FOIL’s information gain measure:\n\0‹R0:  {} => class   (initial rule)\n\0‹R1:  {A} => class (rule after adding conjunct)\n\0‹Gain(R0, R1) = t [  log (p1/(p1+n1)) – log (p0/(p0 + n0)) ]\n\0‹where   t: number of positive instances covered by both R0 and R1\np0: number of positive instances covered by R0\nn0: number of negative instances covered by R0\np1: number of positive instances covered by R1\nn1: number of negative instances covered by R1\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               20\nInstance Elimination\n\0OWhy do we need to \neliminate instances?\n–Otherwise, the next rule is \nidentical to previous rule\n\0OWhy do we remove \npositive instances?\n–Ensure that the next rule is \ndifferent\n\0OWhy do we remove \nnegative instances?\n–Prevent underestimating \naccuracy of rule\n–Compare rules R2 and R3 \nin the diagram\nclass = +\nclass = -\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n-\n-\n-\n-\n--\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n+\n+\n+\n+\n+\n+\n+\nR1\nR3 R2\n+\n+\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               21\nRule Evaluation\n\0OMetrics:\n–Accuracy\n–Laplace\n–M-estimate\nkn\nn\nc\n++\n=\n1\nkn\nkpn\nc\n++\n=\nn : Number of instances \ncovered by rule\nn\nc\n: Number of instances \ncovered by rule\nk: Number of classes\np: Prior probability\nn\nn\nc\n=\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               22\nStopping Criterion and Rule Pruning\n\0OStopping criterion\n–Compute the gain\n–If gain is not significant, discard the new rule\n\0ORule Pruning\n–Similar to post-pruning of decision trees\n–Reduced Error Pruning: \n\0‹Remove one of the conjuncts in the rule \n\0‹Compare error rate on validation set before and \nafter pruning\n\0‹If error improves, prune the conjunct\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               23\nSummary of Direct Method\n\0OGrow a single rule\n\0ORemove Instances from rule\n\0OPrune the rule (if necessary)\n\0OAdd rule to Current Rule Set\n\0ORepeat\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               24\nDirect Method: RIPPER\n\0OFor 2-class problem, choose one of the classes as \npositive class, and the other as negative class\n–Learn rules for positive class\n–Negative class will be default class\n\0OFor multi-class problem\n–Order the classes according to increasing class \nprevalence (fraction of instances that belong to a \nparticular class)\n–Learn the rule set for smallest class first, treat the rest \nas negative class\n–Repeat with next smallest class as positive class\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               25\nDirect Method: RIPPER\n\0OGrowing a rule:\n–Start from empty rule\n–Add conjuncts as long as they improve FOIL’s\ninformation gain\n–Stop when rule no longer covers negative examples\n–Prune the rule immediately using incremental reduced \nerror pruning\n–Measure for pruning:  v = (p-n)/(p+n)\n\0‹p: number of positive examples covered by the rule in\nthe validation set\n\0‹n: number of negative examples covered by the rule in\nthe validation set\n–Pruning method: delete any final sequence of \nconditions that maximizes v\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               26\nDirect Method: RIPPER\n\0OBuilding a Rule Set:\n–Use sequential covering algorithm\n\0‹Finds the best rule that covers the current set of \npositive examples\n\0‹Eliminate both positive and negative examples \ncovered by the rule\n–Each time a rule is added to the rule set, \ncompute the new description length\n\0‹stop adding new rules when the new description \nlength is d bits longer than the smallest description \nlength obtained so far\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               27\nDirect Method: RIPPER\n\0OOptimize the rule set:\n–For each rule rin the rule set R\n\0‹Consider 2 alternative rules:\n– Replacement rule (r*): grow new rule from scratch\n– Revised rule(r’): add conjuncts to extend the rule r \n\0‹Compare the rule set for r against the rule set for r* \nand r’ \n\0‹Choose rule set that minimizes MDL principle\n–Repeat rule generation and rule optimization \nfor the remaining positive examples\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               28\nIndirect Methods\nRule Set\nr1: (P=No,Q=No) ==> \n-\nr2: (P=No,Q=Yes) ==> +\nr3: (P=Yes,R=No) ==> +\nr4: (P=Yes,R=Yes,Q=No) ==> -\nr5: (P=Yes,R=Yes,Q=Yes) ==> +\nP\nQ R\nQ-++\n-+\nNo No\nNo\nYes Yes\nYes\nNo Yes\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               29\nIndirect Method: C4.5rules\n\0OExtract rules from an unpruned decision tree\n\0OFor each rule, r: A →y, \n–consider an alternative rule r’: A’ →y where A’\nis obtained by removing one of the conjuncts \nin A\n–Compare the pessimistic error rate for r \nagainst all r’s\n–Prune if one of the r’s has lower pessimistic \nerror rate\n–Repeat until we can no longer improve \ngeneralization error\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               30\nIndirect Method: C4.5rules\n\0OInstead of ordering the rules, order subsets of \nrules(class ordering)\n–Each subset is a collection of rules with the \nsame rule consequent (class)\n–Compute description length of each subset\n\0‹Description length = L(error) + g L(model)\n\0‹g is a parameter that takes into account the \npresence of redundant attributes in a rule set \n(default value = 0.5)\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               31\nExample\nName Give Birth Lay Eggs Can Fly Live in Water Have Legs Class\nhuman yes no no no yes mammals\npythonnoyesnononoreptiles\nsalmon no yes no yes no fishes\nwhale yes no no yes no mammals\nfrog no yes no sometimes yes amphibians\nkomodo no yes no no yes reptiles\nbat yes no yes no yes mammals\npigeon no yes yes no yes birds\ncat yes no no no yes mammals\nleopard shark yes no no yes no fishes\nturtle no yes no sometimes yes reptiles\npenguin no yes no sometimes yes birds\nporcupine yes no no no yes mammals\neel no yes no yes no fishes\nsalamander no yes no sometimes yes amphibians\ngila monster no yes no no yes reptiles\nplatypus no yes no no yes mammals\nowl no yes yes no yes birds\ndolphin yes no no yes no mammals\neagle no yes yes no yes birds\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               32\nC4.5 versus C4.5rules versus RIPPER\nC4.5rules:\n(Give Birth=No, Can Fly=Yes) →Birds\n(Give Birth=No, Live in Water=Yes) →Fishes\n(Give Birth=Yes) →Mammals\n(Give Birth=No, Can Fly=No, Live in Water=No) →Reptiles\n( ) →Amphibians\nGive\nBirth?\nLive In\nWater?\nCan\nFly?\nMammals\nFishes Amphibians\nBirds Reptiles\nYes No\nYes\nSometimes\nNo\nYes No\nRIPPER:\n(Live in Water=Yes) →Fishes\n(Have Legs=No) →Reptiles\n(Give Birth=No, Can Fly=No, Live In Water=No) \n→Reptiles\n(Can Fly=Yes,Give Birth=No) →Birds\n() →Mammals\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               33\nC4.5 versus C4.5rules versus RIPPER\nPREDICTED CLASS\n 	Amphibians Fishes Reptiles Birds Mammal s\nACTUAL Amphibians	00002\nCLASS Fishes	03000\nReptiles	00301\nBirds	00121\nMammals	02104\nPREDICTED CLASS\n 	Amphibians Fishes Reptiles Birds Mammal s\nACTUAL Amphibians	20000\nCLASS Fishes	02001\nReptiles	10300\nBirds	10030\nMammals	00106\nC4.5 and C4.5rules:\nRIPPER:\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               34\nAdvantages of Rule-Based Classifiers\n\0OAs highly expressive as decision trees\n\0OEasy to interpret\n\0OEasy to generate\n\0OCan classify new instances rapidly\n\0OPerformance comparable to decision trees\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               35\nInstance-Based Classifiers\nAtr1\n……...\nAtrNClass\nA\nB\nB\nC\nA\nC\nB\nSet of Stored Cases\nAtr1\n……...\nAtrN\nUnseen Case\n•Store the training records \n•Use training records to \npredict the class label of \nunseen cases\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               36\nInstance Based Classifiers\n\0OExamples:\n–Rote-learner\n\0‹Memorizes entire training data and performs \nclassification only if attributes of record match one of \nthe training examples exactly\n–Nearest neighbor\n\0‹Uses k “closest” points (nearest neighbors) for \nperforming classification\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               37\nNearest Neighbor Classifiers\n\0OBasic idea:\n–If it walks like a duck, quacks like a duck, then \nit’s probably a duck\nTraining \nRecords\nTest \nRecord\nCompute \nDistance\nChoose k of the 	“nearest” records\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               38\nNearest-Neighbor Classifiers\n\0ORequires three things\n–The set of stored records\n–Distance Metric to compute \ndistance between records\n–The value of \nk, the number of \nnearest neighbors to retrieve\n\0OTo classify an unknown record:\n–Compute distance to other \ntraining records\n–Identify \nknearest neighbors \n–Use class labels of nearest \nneighbors to determine the \nclass label of unknown record \n(e.g., by taking majority vote)\nUnknown record\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               39\nDefinition of Nearest Neighbor\nX	X	X\n(a) 1-nearest neighbor (b) 2-nearest neighbor (c) 3-nearest neighbor\nK-nearest neighbors of a record x are data points \nthat have the k smallest distance to x\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               40\n1 nearest-neighbor\nVoronoi Diagram\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               41\nNearest Neighbor Classification\n\0OCompute distance between two points:\n–Euclidean distance \n\0ODetermine the class from nearest neighbor list\n–take the majority vote of class labels among \nthe k-nearest neighbors\n–Weigh the vote according to distance\n\0‹weight factor, w = 1/d\n2\n∑−=\ni\nii\nqpqpd\n2\n)(),(\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               42\nNearest Neighbor Classification…\n\0OChoosing the value of k:\n–If k is too small, sensitive to noise points\n–If k is too large, neighborhood may include points from \nother classes\nX\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               43\nNearest Neighbor Classification…\n\0OScaling issues\n–Attributes may have to be scaled to prevent \ndistance measures from being dominated by \none of the attributes\n–Example:\n\0‹height of a person may vary from 1.5m to 1.8m\n\0‹weight of a person may vary from 90lb to 300lb\n\0‹income of a person may vary from $10K to $1M\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               44\nNearest Neighbor Classification…\n\0OProblem with Euclidean measure:\n–High dimensional data \n\0‹curse of dimensionality\n–Can produce counter-intuitive results\n1 1 1 1 1 1 1 1 1 1 1 0\n0 1 1 1 1 1 1 1 1 1 1 1\n1 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 1\nvs\nd = 1.4142 d = 1.4142\n\0‹Solution: Normalize the vectors to unit length\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               45\nNearest neighbor Classification…\n\0Ok-NN classifiers are lazy learners \n–It does not build models explicitly\n–Unlike eager learners such as decision tree \ninduction and rule-based systems\n–Classifying unknown records are relatively \nexpensive\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               46\nExample: PEBLS\n\0OPEBLS: Parallel Examplar-Based Learning \nSystem (Cost & Salzberg)\n–Works with both continuous and nominal \nfeatures\n\0‹For nominal features, distance between two \nnominal values is computed using modified value \ndifference metric (MVDM)\n–Each record is assigned a weight factor\n–Number of nearest neighbor, k = 1\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               47\nExample: PEBLS\n142No\n102Yes\nDivorcedMarriedSingle\nMarital Status\nClass\n∑ −=\ni\nii n\nn\nn\nn\nVVd\n2\n2\n1\n1\n21\n),(\nDistance between nominal attribute values:\nd(Single,Married) \n=  | 2/4 – 0/4 | + | 2/4 – 4/4 | =  1\nd(Single,Divorced) \n=  | 2/4 – 1/2 | + | 2/4 – 1/2 | =  0\nd(Married,Divorced) \n=  | 0/4 – 1/2 | + | 4/4 – 1/2 | =  1\nd(Refund=Yes,Refund=No)\n= | 0/3 – 3/7 | + | 3/3 – 4/7 | = 6/7\nTidRefundMarital\nStatus\nTaxable\nIncomeCheat\n1Yes Single 125K No\n2No Married100K No\n3No Single 70K No\n4Yes Married120K No\n5No Divorced95K Yes\n6No Married60K No\n7Yes Divorced220K No\n8No Single 85K Yes\n9No Married75K No\n10No Single 90K Yes\n10\n43No\n30Yes\nNoYes\nRefund\nClass\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               48\nExample: PEBLS\n∑\n=\n=∆\nd\ni\niiYX\nYXdwwYX\n1\n2\n),(),(\nTid Refund Marital \nStatus \nTaxable \nIncome Cheat\nX Yes Single 125K No \nY No Married 100K No \n10 \n \nDistance between record X and record Y: \nwhere:\ncorrectly predicts X  timesofNumber \npredictionfor  used is X  timesofNumber \n=\nXw\nw\nX\n≅1 if X makes accurate prediction most of the time\nw\nX\n> 1 if X is not reliable for making predictions\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               49\nBayes Classifier\n\0OA probabilistic framework for solving classification \nproblems\n\0OConditional Probability:\n\0OBayes theorem:\n)(\n)()|(\n)|(\nAP\nCPCAP\nACP =\n)(\n),(\n)|(\n)(\n),(\n)|(\nCP\nCAP\nCAP\nAP\nCAP\nACP\n=\n=\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               50\nExample of Bayes Theorem\n\0OGiven: \n–A doctor knows that meningitis causes stiff neck 50% of the \ntime\n–Prior probability of any patient having meningitis is 1/50,000\n–Prior probability of any patient having stiff neck is 1/20\n\0OIf a patient has stiff neck, what’s the probability \nhe/she has meningitis?\n0002.0\n20/1\n50000/15.0\n)(\n)()|(\n)|(	=\n×\n==\nSP\nMPMSP\nSMP\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               51\nBayesian Classifiers\n\0OConsider each attribute and class label as random \nvariables\n\0OGiven a record with attributes (A\n1, A\n2,…,A\nn) \n–Goal is to predict class C\n–Specifically, we want to find the value of C that \nmaximizes P(C| A\n1, A\n2,…,A\nn )\n\0OCan we estimate P(C| A\n1, A\n2,…,A\nn ) directly from \ndata?\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               52\nBayesian Classifiers\n\0OApproach:\n–compute the posterior probability P(C | A\n1, A\n2, …, A\nn) for \nall values of C using the Bayes theorem\n–Choose value of C that maximizes \nP(C | A\n1, A\n2, …, A\nn)\n–Equivalent to choosing value of C that maximizes\nP(A\n1, A\n2, …, A\nn|C) P(C)\n\0OHow to estimate P(A\n1, A\n2, …, A\nn | C )?\n)(\n)()|(\n)|(\n21\n21\n21\nn\nn\nn\nAAAP	CPCAAAP\nAAACP\nK\nK\nK =\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               53\nNaïve Bayes Classifier\n\0OAssume independence among attributes A\ni\nwhen class is \ngiven:    \n–P(A\n1, A\n2, …, A\nn |C) = P(A\n1| C\nj) P(A\n2| C\nj)… P(A\nn| C\nj)\n–Can estimate P(A\ni\n| C\nj\n) for all A\ni\nand C\nj\n.\n–New point is classified to C\njif  P(C\nj) ΠP(A\ni| C\nj)  is \nmaximal.\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               54\nHow to Estimate Probabilities from Data?\n\0OClass:  P(C) = N\nc/N\n–e.g.,  P(No) = 7/10, \nP(Yes) = 3/10\n\0OFor discrete attributes:\nP(A\ni| C\nk) = |A\nik|/ N\nc \n–where |A\nik| is number of \ninstances having attribute \nA\niand belongs to class C\nk\n–Examples:\nP(Status=Married|No) = 4/7\nP(Refund=Yes|Yes)=0\nk\nTid Refund Marital \nStatus \nTaxable \nIncome Evade\n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n \nc	c	c\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               55\nHow to Estimate Probabilities from Data?\n\0OFor continuous attributes: \n–Discretizethe range into bins \n\0‹one ordinal attribute per bin\n\0‹violates independence assumption\n–Two-way split:(A < v) or (A > v)\n\0‹choose only one of the two splits as new attribute\n–Probability density estimation:\n\0‹Assume attribute follows a normal distribution\n\0‹Use data to estimate parameters of distribution \n(e.g., mean and standard deviation)\n\0‹Once probability distribution is known, can use it to \nestimate the conditional probability P(A\ni|c)\nk\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               56\nHow to Estimate Probabilities from Data?\n\0ONormal distribution:\n–One for each (A\ni,c\ni) pair\n\0OFor (Income, Class=No):\n–If Class=No\n\0‹sample mean = 110\n\0‹sample variance = 2975\nTid Refund Marital \nStatus \nTaxable \nIncome Evade\n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n \n2\n2\n2\n)(\n2\n2\n1\n)|(\nij\niji\nA\nij\nji\necAP\nσ\nµ\nπσ\n−\n−\n=\n0072.0\n)54.54(2\n1\n)|120(\n)2975(2\n)110120(\n2\n===\n−\n−\neNoIncomeP\nπ\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               57\nExample of Naïve Bayes Classifier\nP(Refund=Yes|No) = 3/7\nP(Refund=No|No) = 4/7\nP(Refund=Yes|Yes) = 0\nP(Refund=No|Yes) = 1\nP(Marital Status=Single|No) = 2/7\nP(Marital Status=Divorced|No)=1/7\nP(Marital Status=Married|No) = 4/7\nP(Marital Status=Single|Yes) = 2/7\nP(Marital Status=Divorced|Yes)=1/7\nP(Marital Status=Married|Yes) = 0\nFor taxable income:\nIf class=No: sample mean=110\nsample variance=2975\nIf class=Yes: sample mean=90\nsample variance=25\nnaive Bayes Classifier:\n120K)IncomeMarried,\nNo,Refund(	===X\n\0OP(X|Class=No) = P(Refund=No|Class=No)\n×P(Married| Class=No)\n×P(Income=120K| Class=No)\n= 4/7 ×4/7 ×0.0072 = 0.0024\n\0OP(X|Class=Yes) = P(Refund=No| Class=Yes)\n×P(Married| Class=Yes)\n×P(Income=120K| Class=Yes)\n= 1 ×0 ×1.2 ×10\n-9\n= 0\nSince P(X|No)P(No) > P(X|Yes)P(Yes)\nTherefore P(No|X) > P(Yes|X)\n=> Class = No\nGiven a Test Record:\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               58\nNaïve Bayes Classifier\n\0OIf one of the conditional probability is zero, then \nthe entire expression becomes zero\n\0OProbability estimation:\nmN\nmpN\nCAP\ncN\nN\nCAP\nN\nN\nCAP\nc\nic\ni\nc\nic\ni\nc\nic\ni+\n+\n=\n+\n+\n=\n=\n)|(:estimate-m\n1\n)|(:Laplace\n)|( :Original\nc: number of classes\np: prior probability\nm: parameter\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               59\nExample of Naïve Bayes Classifier\nName Give Birth Can Fly Live in Water Have Legs Class\nhuman yes no no yes mammals\npython no no no no non-mammals\nsalmon no no yes no non-mammals\nwhale yes no yes no mammals\nfrog no no sometimes yes non-mammals\nkomodo no no no yes non-mammals\nbat yes yes no yes mammals\npigeon no yes no yes non-mammals\ncat yes no no yes mammals\nleopard shark yes no yes no non-mammals\nturtle no no sometimes yes non-mammals\npenguin no no sometimes yes non-mammals\nporcupine yes no no yes mammals\neel no no yes no non-mammals\nsalamander no no sometimes yes non-mammals\ngila monster no no no yes non-mammals\nplatypus no no no yes mammals\nowl no yes no yes non-mammals\ndolphin yes no yes no mammals\neagle no yes no yes non-mammals\nGive Birth Can Fly Live in Water Have Legs Class\nyes no yes no ?\n0027.0\n20\n13\n004.0)()|(\n021.0\n20\n7\n06.0)()|(\n0042.0\n13\n4\n13\n3\n13\n10\n13\n1\n)|(\n06.0\n7\n2\n7\n2\n7\n6\n7\n6\n)|(\n=×=\n=×=\n=×××=\n=×××=\nNPNAP\nMPMAP\nNAP\nMAP\nA: attributes\nM: mammals\nN: non-mammals\nP(A|M)P(M) > P(A|N)P(N)\n=> Mammals\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               60\nNaïve Bayes (Summary)\n\0ORobust to isolated noise points\n\0OHandle missing values by ignoring the instance \nduring probability estimate calculations\n\0ORobust to irrelevant attributes\n\0OIndependence assumption may not hold for some \nattributes\n–Use other techniques such as Bayesian Belief \nNetworks (BBN)\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               61\nArtificial Neural Networks (ANN)\nX\n1X\n2X\n3Y\n1000\n1011\n1101\n1111\n0010\n0100\n0111\n0000\nX\n1\nX\n2\nX\n3\nY\nBlack box\nOutput\nInput\nOutput Y is 1 if at least two of the three inputs are equal to 1.\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               62\nArtificial Neural Networks (ANN)\nX\n1X\n2X\n3Y\n1000\n1011\n1101\n1111\n0010\n0100\n0111\n0000\nΣ\nX\n1\nX\n2\nX\n3\nY\nBlack box\n0.3\n0.3\n0.3\nt=0.4\nOutput\nnode\nInput\nnodes\n\n\n\n=\n>−++=\notherwise0\n trueis  if1\n)( where\n)04.03.03.03.0(\n321\nz\nzI\nXXXIY\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               63\nArtificial Neural Networks (ANN)\n\0OModel is an assembly of \ninter-connected nodes \nand weighted links\n\0OOutput node sums up \neach of its input value \naccording to the weights \nof its links\n\0OCompare output node \nagainst some threshold t\nΣ\nX\n1\nX\n2\nX\n3\nY\nBlack box\nw\n1\nt\nOutput\nnode\nInput\nnodes\nw\n2\nw\n3\n)( tXwIY\ni\nii\n−=\n∑\nPerceptron Model\n)( tXwsignY\ni\nii\n−=\n∑\nor\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               64\nGeneral Structure of ANN\nActivation\nfunction\ng(S\ni \n)\nS\ni\nO\ni\nI\n1\nI\n2\nI\n3\nw\ni1\nw\ni2\nw\ni3\nO\ni\nNeuron iInput Output\nthreshold, t\nInput\nLayer\nHidden\nLayer\nOutput\nLayer\nx\n1\nx\n2\nx\n3\nx\n4\nx\n5\ny\nTraining ANN means learning \nthe weights of the neurons\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               65\nAlgorithm for learning ANN\n\0OInitialize the weights (w\n0, w\n1, …, w\nk)\n\0OAdjust the weights in such a way that the output \nof ANN is consistent with class labels of training \nexamples\n–Objective function:\n–Find the weights w\ni’s that minimize the above \nobjective function\n\0‹e.g., backpropagation algorithm (see lecture notes)\n[ ]\n2\n),(∑−=\ni\niii\nXwfYE\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               66\nSupport Vector Machines\n\0OFind a linear hyperplane (decision boundary) that will separate the data\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               67\nSupport Vector Machines\n\0OOne Possible Solution\nB\n1\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               68\nSupport Vector Machines\n\0OAnother possible solution\nB\n2\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               69\nSupport Vector Machines\n\0OOther possible solutions\nB\n2\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               70\nSupport Vector Machines\n\0OWhich one is better? B1 or B2?\n\0OHow do you define better?\nB\n1\nB\n2\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               71\nSupport Vector Machines\n\0OFind hyperplanemaximizesthe margin => B1 is better than B2\nB\n1\nB\n2\nb\n11\nb\n12\nb\n21\nb\n22\nmargin\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               72\nSupport Vector Machines\nB\n1\nb\n11\nb\n12\n0=+•bxw\nrr\n1−=+•bxw\nrr	1+=+•bxw\nrr\n\n\n\n−≤+•−\n≥+•\n=\n1bxw if1\n1bxw if1\n)(	rr\nrr\nr\nxf\n2\n||||\n2\n Margin\nw\nr=\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               73\nSupport Vector Machines\n\0OWe want to maximize:\n–Which is equivalent to minimizing:\n–But subjected to the following constraints:\n\0‹This is a constrained optimization problem\n– Numerical approaches to solve it (e.g., quadratic programming)\n2\n||||\n2\n Margin\nw\nr=\n\n\n\n−≤+•−\n≥+•\n=\n1bxw if1\n1bxw if1\n)(	i\ni\nrr\nrr\nr\nixf\n2\n||||\n)(\n2\nw\nwL\nr\n=\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               74\nSupport Vector Machines\n\0OWhat if the problem is not linearly separable?\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               75\nSupport Vector Machines\n\0OWhat if the problem is not linearly separable?\n–Introduce slack variables\n\0‹Need to minimize:\n\0‹Subject to: \n\n\n\n+−≤+•−\n≥+•\n=	ii\nii\n1bxw if1\n-1bxw if1\n)(ξ\nξ\nrr\nrr\nr\nixf\n\n\n\n\n\n\n+=∑\n=\nN\ni\nk\ni\nC\nw\nwL\n1\n22\n||||\n)(\nξ\nr\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               76\nNonlinear Support Vector Machines\n\0OWhat if decision boundary is not linear?\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               77\nNonlinear Support Vector Machines\n\0OTransform data into higher dimensional space\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               78\nEnsemble Methods\n\0OConstruct a set of classifiers from the training \ndata\n\0OPredict class label of previously unseen records \nby aggregating predictions made by multiple \nclassifiers\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               79\nGeneral Idea\nOriginal\nTraining data\n....\nD\n1\nD\n2 D\nt-1\nD\nt\nD\nStep 1:\nCreate Multiple\nData Sets\nC\n1\nC\n2\nC\nt -1\nC\nt\nStep 2:\nBuild Multiple\nClassifiers\nC\n*\nStep 3:\nCombine\nClassifiers\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               80\nWhy does it work?\n\0OSuppose there are 25 base classifiers\n–Each classifier has error rate, \nε= 0.35\n–Assume classifiers are independent\n–Probability that the ensemble classifier makes \na wrong prediction:\n∑\n=\n−\n=−\n\n\n\n\n\n\n\n\n25\n13\n25\n06.0)1(\n25\ni\niii\nεε\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               81\nExamples of Ensemble Methods\n\0OHow to generate an ensemble of classifiers?\n–Bagging\n–Boosting\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               82\nBagging\n\0OSampling with replacement\n\0OBuild classifier on each bootstrap sample\n\0OEach sample has probability (1 – 1/n)\nn\nof being \nselected\nOriginal Data 12345678910\nBagging (Round 1) 7810825101059\nBagging (Round 2) 1491232732\nBagging (Round 3) 18510559637\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               83\nBoosting\n\0OAn iterative procedure to adaptively change \ndistribution of training data by focusing more on \npreviously misclassified records\n–Initially, all N records are assigned equal \nweights\n–Unlike bagging, weights may change at the \nend of boosting round\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               84\nBoosting\n\0ORecords that are wrongly classified will have their 	weights increased\n\0ORecords that are classified correctly will have \ntheir weights decreased\nOriginal Data 12345678910\nBoosting (Round 1) 73287941063\nBoosting (Round 2) 5494251742\nBoosting (Round 3) 44810454634\n• Example 4 is hard to classify\n• Its weight is increased, therefore it is more \nlikely to be chosen again in subsequent rounds\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               85\nExample: AdaBoost\n\0OBase classifiers: C\n1, C\n2, …, C\nT\n\0OError rate:\n\0OImportance of a classifier: \n()∑\n=\n≠=\nN\nj\njjiji\nyxCw\nN\n1\n)(\n1δε\n\n\n\n\n\n\n\n−\n=\ni\ni\niε\nε\nα\n1\nln\n2\n1\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               86\nExample: AdaBoost\n\0OWeight update:\n\0OIf any intermediate rounds produce error rate \nhigher than 50%, the weights are reverted back \nto 1/n and the resampling procedure is repeated\n\0OClassification:\nfactor ionnormalizat  theis   where\n)( ifexp\n)( ifexp\n)(\n)1(\nj\niij\niij\nj\nj\nij\ni\nZ\nyxC\nyxC\nZ\nw\nw\nj\nj\n\n\n\n\n\n≠\n=\n=\n−\n+\nα\nα\n()∑\n=\n==\nT\nj\njj\ny\nyxCxC\n1\n)(maxarg)(*δα\n\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               87\nBoosting\nRound 1+++ --- - - --\n0.0094 0.0094 0.4623\nB1\nα = 1.9459\nIllustrating AdaBoost\nData points \nfor training\nInitial weights for each data point\nOriginal\nData +++ --- - -++\n0.1	0.1 0.1\n© Tan,Steinbach, Kumar Introduction to Data Mining        4/18/2004               88\nIllustrating AdaBoost\nBoosting\nRound 1+++ --- - - --\nBoosting\nRound 2 --- --- - - ++\nBoosting\nRound 3\n+++ +++ + + ++\nOverall+++ --- - -++\n0.0094 0.0094 0.4623\n0.3037 0.0009 0.0422\n0.0276 0.1819 0.0038\nB1\nB2\nB3\nα = 1.9459\nα = 2.9323\nα = 3.8744', 665, 1, '2025-06-04 07:12:27', '2025-06-04 07:12:27'),
(56, 'سؤس', 'Comparison Among the Most Popular Methodologies.pdf', NULL, 'documents/w7spSGWoJQdshI7wTGxwsB4II6xnIkhexGkLGser.pdf', 'Software Development Methodologies \nComparison Among the Most Popular Methodologies \n \n \nWaterfall Spiral Prototyping RAD XP Scrum Kanban \nType Traditional Traditional Traditional → Agile Agile Agile \nFocus Plan-driven Risk-driven Feedback-driven Speed-driven \nProgramming-\ndriven \nManagement-\ndriven \nFlow-driven \nAccept change? No Yes Yes Yes Yes Yes Yes \nProcess Predictive Adaptive Adaptive Adaptive Adaptive Adaptive Adaptive \nCycles Sequential/Linear Iterative Iterative Iterative Iterative Mixed Mixed \nUser involvement Low Medium High High High High Medium \nSmall team? No No No Yes Yes Yes Yes \nTeam Prescribes roles Prescribes roles Prescribes roles Prescribes roles Prescribes roles Prescribes roles Optional \nDocumentation? Maximal Maximal Minimal Minimal Minimal Minimal Minimal \nPlanning Long term Long & Short Short term Short term Short term Short term Short term \nComplete \nRequirements \nYes No No No No No No \nProject Size Large Large Small to Medium Small to Medium Small to Medium Small to Medium Small to Medium', 79, 3, '2025-06-04 07:17:27', '2025-06-04 07:17:27'),
(57, 'dcdsc', 'MidTerm Exam.docx', NULL, 'documents/MwMCqCbOw1sEREszUGYBHNFcivv42703JhPVBdS1.docx', 'Islamic University - Gaza Faculty of   Information Technology Department of Computer Science. SDEV   3304  – Data Mining  M i dterm Exam Su nday 2 6 / 11 / 2011 Student ID : --------- - ------ - -------       Student Name : -------------------------------------------------------- (14  points) Choose the correct answer: Correlation analysis methods always used as ________ methods. data cleaning data integration data reduction Knowledge = Information + _______________ . Context   Rules Experiences Pre-pruning and post-pruning are two approaches to overcome the _______ problem. classification underfitting overfitting ______ is a kind of Numerical data type and it has no true zero point. Quantity Interval Ratio Expectation of the wind velocities is a(an) ______ tasks. association classification prediction The hard-limit activation function’s, used within Neural Networks, result belong to _______. {1, 0} [0, 1] (- , ) Data mining is differ than statistics in __________ kind of data kinds of methods all the previous Symmetric and asymmetric are two types of the _______ data type. numerical nominal binary _____ is the percentages of tuples that the classifier labeled as positive are actually positive. Coverage Accuracy Precision Conflict resolution is required as a solution when more than one classification rule are ________. extracted triggered occurred ------------ is a data transformation method. Smoothing Discretization Clustering Find the natural grouping of instances given unlabeled data is a called ________. association clustering classification Data Integration combines data from multiple sources but it yields some problems. _______ is not one of these problems. Inconsistent values Entity identification Redundant attribute Classifier ______ refers to the ability of the classifier or predictor to make correct predictions given noisy data. robustness scalability interpretability   ( 16  points) Do as required (2  points)   Discretize the age attribute by considering  the minimum  occurrence of the class is 4 . Age 81 64 70 72 72 83 75 75 68 69 85 80 71 65 Class Y Y Y N Y N Y Y Y Y N N N N (2 points) Apply stratified sampling on the following data to reduce the dataset by 50% . Age 81 64 70 72 72 83 75 75 68 69 85 80 71 65 Class Y Y Y N Y N Y Y Y Y N N N N (3 points)  Consider  the following contingency table  for gender and handedness (right- or left-handed), then compute the Support, confidence and lift for [ Female   Right-handed ]  association rule. Right-handed  Left-handed  Totals Males  43 9 52 Females  44 4 48 Totals 87 13 100 ( 2  points) Consider the following table, then use the K-NN  Algorithm with k=1 to classify a query instance with Temp=5 and Humidity =5. Temp Humidity P lay 3 10 No 7 10 Yes 3 7 Yes (2  points) By considering  the previous table, extract a classification rule without any computation. (5 points) Consider the following dataset, by using the Information Gain which features is the decision tree root. Name Gender Height Class Kristina F <=1.7 Short Jim M >=2 Tall Magge F (1.7… 1.9] Medium Martha F (1.7… 1.9] Medium Stephanie F <=1.7 Short Bob M (1.7… 1.9] Medium Kathy F <=1.7 Short Dave M <=1.7 Short Worth M >=2 Tall Steven M >=2 Tall Debbi F (1.7… 1.9] Medium Todd M (1.7… 1.9] Medium Kim F (1.7… 1.9] Medium Amy F (1.7… 1.9] Medium Wynette F (1.7… 1.9] Medium The following table may be useful in to simplify the computation process. x 1/6 1/5 1/4 1/3 2/5 1/2 2/3 3/4 4/15 8/18 Log 2 (x) -2.58 -2.32 -2.00 -1.58 -1.32 -1.00 -0.58 -0.42 -1.91 -1.17', 65, 2, '2025-06-04 07:20:01', '2025-06-04 07:20:01'),
(58, 'بريبر', 'SDM_Syllabus_12022 (1).pdf', NULL, 'documents/xr5LdXHf7kVd5h0jJgSNNoUVaw7RfI9c8KYOiqXT.pdf', '1 \n \nFaculty of Information Technology – Islamic University of Gaza \nSoftware Development Methodologies – SDEV 3309 \nCourse Syllabus-First Semester 2022/2023 \n \nCourse Description \nThis course introduces the students to the role of software development processes in \ndeveloping a successful software project. The course covers the following topics: introduction \nto software processes, current practice of software processes, improving the software \nprocesses, and the most popular development methodologies such as: Waterfall methodology, \nand the methodologies under the umbrella of Agile approach like: Scrum, XP, FDD, TDD, \nCrystal, Lean and Kanban. \nCourse Objectives \nThis course aims to give students an in-depth understanding of software development \nmethodologies and the key activities for building software products in each methodology. \nUpon successful completion of this course students should be able to: \n \n• Describe what a software process is, which purpose it serves, how it can be improved, \nand terms related to them.  \n• Describe the core elements of a software methodology and its content, including \ngoals, activities, tasks, roles, artifacts, etc. \n• Discuss the advantages and disadvantages of different lifecycles, including Waterfall \nand Agile. \n• Apply core software development practices related to the software methodologies at \nthe conceptual level for a given software project problem.   \n• Evaluate a software development project, suggest a plan for software development \nmethodology improvement based on the evaluation. \n \n \nInstructor:  \nTamer Nazir Madi \nOffice: \n I306 \n \nE-mail: \ntmadi@iugaza.edu.ps \nHomepage: \nhttp://site.iugaza.edu.ps/tmadi \n \nClass Time: \nSat/Mon/Wed: \n201: 09:00--10:00 Room: L308 \n102: 10:00--11:00 Room: K508 \n101: 12:00--13:00 Room: K403 \n \n  \nOffice Hours: \nSat/Mon/Wed: \n08:00--09:00 \n11:00--12:00 \n \nCourse Moodle Homepage: \nhttps://moodle.iugaza.edu.ps/course/view.php?id=8197\n\n2 \nCourse Content \nThe following table describes the distribution of the course content throughout the semester: \n \nWeek Topic \n1 Course Intro \n2 Introduction to Software Processes \n3 Traditional Processes \n4 Traditional Methodologies: Waterfall  \n5 Traditional Methodologies: Spiral \n6 Traditional Methodologies: V-Model \n7 Rapid Process: Agile Approach \n8 Agile Methodologies: Scrum \n9 Agile Methodologies: XP \n10 Agile Methodologies: FDD \n11 Agile Methodologies: Lean \n12 Agile Methodologies: Kanban \n13 Comparison of Methodologies \n14 Final Project Presentation \n  \nCourse Teaching Method \nThe course will be taught as a combination of lectures, discussion sessions, presentations, \nexercises and discussion of real-world cases. An online learning using Moodle is adopted in \nthe course as a combination with the traditional class. A practical project will be given to \nstudents to practice what they have learned from the course. \n \nResources \n• Lecture Notes \n• Ian Sommerville (2016), Software Engineering, 10th edition, Addison Wesley. \n• Capers Jones (2018), Software Methodologies: A Quantitative Guide, \nAuerbach Publications, CRC Press. \n• Roger Pressman and Bruce Maxim (2019), Software Engineering: A \nPractitioner\'s Approach, McGraw-Hill Education. \n• Ian Sommerville (2020), Engineering Software Products: An Introduction to \nModern Software Engineering, Pearson. \n• Some additional resources \n \n \nGrading \n \nCriterion 	Percentage \nAssignments and Quizzes  25% \nProject  	15% \nMidterm Exam 	20% \nFinal Exam 	40% \nTotal 	100%\n\n3 \nCourse and Class Rules \n• Students should not miss more than 25% of the classes, otherwise the course will be \ndropped. \n• Students should arrive at the classroom punctually or before the class starts to avoid \ndisturbing others. \n• Phones should be silent \n• Students should always follow up the course page on moodle. \n• No excuses for missing the quizzes or the assignments. \n• Copy and paste in any of the course assignment will be given a Big Zero. \n• Students should do the requested assignments as they described. \n• All students’ submissions must be via moodle only.', 152, 2, '2025-06-04 07:47:18', '2025-06-04 07:47:18');
INSERT INTO `documents` (`id`, `title`, `filename`, `file_hash`, `filepath`, `content`, `size`, `category_id`, `created_at`, `updated_at`) VALUES
(59, 'يؤيؤ', 'Course Key Outlines (2) (1).pdf', NULL, 'documents/ZoHxdVYk6zzYBrlL9IQTQr2Q8PkFHWkQJS5bALat.pdf', 'Software Development Methodologies \nCourse Key Outlines \nChapter 1: Software Processes Chapter 2: Waterfall Chapter 3: Spiral 	Chapter 4: Prototyping Chapter 5: RAD \n• Why we need a Software Process? \n• Process vs. Model \n• Four Development Activities \n• Software Process Descriptions \n• Process Structure \n• Plan-Driven and Agile Processes \n• Process Activities \n• Software Specification \n• Design & Implementation \n• Software Validation \n• Stages of Testing \n• Software Evolution \n• Coping with Change \n• Reducing the Costs of Rework \n \n• Introduction \n• Waterfall Model \n• 1. Requirements Phase \n• 2. Design Phase \n• 3. Implementation Phase \n• 4. Testing Phase \n• System testing \n• 5. Maintenance Phase \n• Three types of maintenance \n• When to use Waterfall? \n• Advantages & disadvantages  \n• Why Waterfall may fail? \n• Iterative Waterfall Model \n• Introduction \n• Description of Spiral Model \n• 1. Objectives Setting \n• 2. Risks Assessment & Reduction \n• 3. Development & Validation \n• 4. Planning \n• How it works? \n• When to use Spiral? \n• Advantages & disadvantages \n• Prototyping & prototype definitions \n• Where it can be used? \n• Description of the model \n• Types of the prototyping \n• 1. Throw-away prototyping \n• 2. Evolutionary Prototyping \n• 3. Incremental Prototyping \n• 4. Extreme Prototyping \n• Ways of Preparing a Prototype \n• Forms of Prototyping \n• Low-Fidelity vs. High-Fidelity \n• When to use Prototyping? \n• Advantages & disadvantages \n• Introduction \n• RAD Model Description \n• 1. Requirements Planning \n• 2. User Design \n• 3. Construction \n• 4. Cutover \n• When to use RAD? \n• Advantages & disadvantages \nChapter 6: Agile Approach Chapter 7: XP 	Chapter 8: Scrum 	Chapter 9: Kanban  \n• Introduction \n• Agile Manifesto \n• Values (4) \n• Principles (12) \n• Agile vs. Traditional \n• Agile Methods \n• Introduction \n• Values, Principles & Practices \n• Values (5) \n• Practices (12) \n• Roles in XP \n• Process of XP \n• User Stories \n• When to use \n• Advantages & disadvantages \n• Introduction \n• Values (5) \n• Scrum Terms \n• Process of Scrum \n• Practices (5) \n• Scrum Team \n• Roles in Scrum \n• Scrum artifacts (3) \n• When to use \n• Advantages & disadvantages \n \n• Introduction \n• Values (6) & Principles (4) \n• Kanban Board \n• Practices (6) \n• Kanban Measurements \n• Kanban & Scrum \n• When to use \n• Advantages & disadvantages \n \n \nX', 86, 3, '2025-06-04 07:49:18', '2025-06-04 07:49:18'),
(60, 'سؤسؤ', '05 Traditional Methodologies - Prototyping (2).pdf', NULL, 'documents/mvnqfkf9DvFSCOnWG6oNWnYSNwEOGnAgTKQUTD9v.pdf', 'Traditional Methodologies: \nPrototyping\nDr. Tamer Madi\nSDEV 3309 –Software Development Methodologies\n\nTopics Covered\n•Introduction\n•Phases of Prototyping\n•Types of Prototyping\n•Ways of Preparing a Prototype\n•Forms of Prototyping\n•When to use\n•Advantages and Disadvantages\n2SDEV 3309 -Software Development Methodologies\n\nIntroduction\n3SDEV 3309 -Software Development Methodologies\nIdeas & Concepts	Concreate Design\nPrototyping\n\nIntroduction\n•Prototypingisanexperimentalprocesswheredesignteamsimplement\nideasintotangibleformsfrompapertodigital.\n•Itisdefinedastheprocessofdevelopingaworkingreplicationofa\nproductorsystemthathastobeengineered.\n•Teamsbuildprototypesofvaryingdegreesoffidelitytocapturedesign\nconceptsandtestwithusers.Withprototypes,youcanrefineandvalidate\nyourdesignssoyourbrandcanreleasetherightproducts.\n•Itismainlyusedforobtainingcustomerfeedbackasdescribedinthe\nfollowingfigure:\n4SDEV 3309 -Software Development Methodologies\n\nIntroduction\n5SDEV 3309 -Software Development Methodologies\nDevelop/Refine \nprototype\nTesting with \ncustomer\nCustomer \nfeedback\nused forresults into\nis used to\n\nIntroduction\n•ThePrototypingModelisoneofthemostpopularlyusedSoftware\nDevelopmentLifeCycleModels.\n•Thismodelisusedwhenthecustomersdonotknowtheexact\nprojectrequirementsbeforehand.\n•Inthismodel,aprototypeoftheendproductisfirstdeveloped,\ntestedandrefinedaspercustomerfeedbackrepeatedlytillafinal\nacceptableprototypeisachievedwhichformsthebasisfor\ndevelopingthefinalproduct.\n6SDEV 3309 -Software Development Methodologies\n\nPrototyping Model\n•Aprototypeisaninitialversionofasystemusedtodemonstrate\nconceptsandtryoutdesignoptions.\n•Aprototypecanbeusedin:\n•Therequirementsengineeringprocesstohelpwithrequirementselicitation\nandvalidation;\n•IndesignprocessestoexploreoptionsanddevelopaUIdesign;\n•Inthetestingprocesstorunback-to-backtests.\n7SDEV 3309 -Software Development Methodologies\n\nPrototyping Model\n•Inthisprocessmodel,thesystemispartiallyimplementedbeforeor\nduringtheanalysisphasetherebygivingthecustomersanopportunityto\nseetheproductearlyinthelifecycle.\n•Theprocessstartsbyinterviewingthecustomersanddevelopingthe\nincompletehigh-levelpapermodel.\n•Thisdocumentisusedtobuildtheinitialprototypesupportingonlythe\nbasicfunctionalityasdesiredbythecustomer.Oncethecustomerfigures\nouttheproblems,theprototypeisfurtherrefinedtoeliminatethem.\n•Theprocesscontinuesuntiltheuserapprovestheprototypeandfindsthe\nworkingmodeltobesatisfactory.\n8SDEV 3309 -Software Development Methodologies\n\nPrototyping Model\n9SDEV 3309 -Software Development Methodologies\nRequirements\nQuick \nDesign\nImplement \n& Maintain\nBuild \nPrototype\nUser \nEvaluation\nRefining \nPrototype\n\nPrototyping Model\n10SDEV 3309 -Software Development Methodologies\nEstablish \nprototype \nobjectives\nDefine \nprototype \nfunctionality\nDevelop \nprototype\nEvaluate \nprototype\nPrototyping\nplan\nOutline\ndefinition\nExecutable \nprototype\nEvaluation\nreport\n\nPrototyping Model\n•Maybebasedonrapidprototypinglanguagesortools\n•Mayinvolveleavingoutfunctionality\n•Prototypeshouldfocusonareasoftheproductthatarenotwell-understood;\n•Errorcheckingandrecoverymaynotbeincludedintheprototype;\n•Focusonfunctionalratherthannon-functionalrequirementssuchas\nreliabilityandsecurity\n11SDEV 3309 -Software Development Methodologies\n\nTypes of Prototyping Models\n•Therearefourtypesofmodelsavailable:\n1.RapidThrowawayPrototyping\n2.EvolutionaryPrototyping\n3.IncrementalPrototyping\n4.ExtremePrototyping\n12SDEV 3309 -Software Development Methodologies\n\n1. Throw-away prototyping\n•Itisalsoknownasclose-endedprototyping.\n•Inthismethod,theprototypeisdevelopedrapidlybasedontheinitial\nrequirementsandgiventothecustomerforreview.\n•Itoffersausefulmethodofexploringideasandgettingcustomerfeedback\nforeachofthem.\n•Adevelopedprototypeneednotnecessarilybeapartoftheultimately\nacceptedprototype.\n•Customerfeedbackhelpsinpreventingunnecessarydesignfaultsand\nhence,thefinalprototypedevelopedisofbetterquality.\n13SDEV 3309 -Software Development Methodologies\n\n•Prototypesshouldbediscardedafterdevelopmentastheyarenota\ngoodbasisforaproductionsystem:\n•Itmaybeimpossibletotunethesystemtomeetnon-functional\nrequirements;\n•Prototypesarenormallyundocumented;\n•Theprototypestructureisusuallydegradedthroughrapidchange;\n•Theprototypeprobablywillnotmeetnormalorganizationalquality\nstandards.\n14SDEV 3309 -Software Development Methodologies\n1. Throw-away prototyping\n\n15SDEV 3309 -Software Development Methodologies\n1. Throw-away prototyping\nOutline \nRequirements\nDevelop \nPrototype\nEvaluate \nPrototype\nSpecify\nSystem\nDevelop\nSystem\nValidate\nSystem\nDelivered \nSystem\n\n•Inthismethod,theprototypedevelopedinitiallyisincrementallyrefined\nonthebasisofcustomerfeedbacktillitfinallygetsaccepted.\n•Thefirstiterationissimilartotheminimumviableproduct(MVP)which\nmeansaproductwiththeabsoluteminimumfunctionalitytomakeit\nworthwhile.\n•IncomparisontoRapidThrowawayPrototyping,itfollowsanincremental\ndevelopmentapproachanditoffersabetterapproachwhichsavestimeas\nwellaseffort.\n•Thisisbecausedevelopingaprototypefromscratchforeveryiterationof\ntheprocesscansometimesbeveryfrustratingforthedevelopers.\n16SDEV 3309 -Software Development Methodologies\n2. Evolutionary Prototyping\n\n2. Evolutionary Prototyping\n17SDEV 3309 -Software Development Methodologies\nOutline \nRequirements\nQuick Design\nBuild Prototype\nCustomer \nEvaluation\nSuggested \nImproved\nDesign\nCoding\nTesting\nMaintenance\nPrototype\nDevelopment\nIterative\nDevelopment\n\n3. Incremental Prototyping\n•InthistypeofincrementalPrototyping,thefinalexpectedproductisbrokeninto\ndifferentsmallpiecesofprototypesandbeingdevelopedindividually.\n•Intheend,whenallindividualpiecesareproperlydeveloped,thenthedifferent\nprototypesarecollectivelymergedintoasinglefinalproductintheirpredefinedorder.\n•It’saveryefficientapproachthatreducesthecomplexityofthedevelopmentprocess,\nwherethegoalisdividedintosub-partsandeachsub-partisdevelopedindividually.\n•Thetimeintervalbetweentheproject’sbeginningandfinaldeliveryissubstantially\nreducedbecauseallpartsofthesystemareprototypedandtestedsimultaneously.\n•Ofcourse,theremightbethepossibilitythatthepiecesjustdonotfittogetherdueto\nsomelackofnessinthedevelopmentphase–thiscanonlybefixedbycarefuland\ncompleteplottingoftheentiresystembeforeprototypingstarts.\n18SDEV 3309 -Software Development Methodologies\n\n19SDEV 3309 -Software Development Methodologies\n3. Incremental Prototyping\nPlanning\nRequirements \nAnalysis\nDesign\nDevelop/Build\nDevelop/Build\nDevelop/Build\nTest\nTest\nTest\nIntegration\n\n4. Extreme Prototyping\n•Thismethodismainlyusedforwebdevelopment.Itisconsistsofthree\nsequentialindependentphases:\n1.Inthisphaseabasicprototypewithalltheexistingstaticpagesarepresentedin\ntheHTMLformat.\n2.Inthesecondphase,functionalscreensaremadewithasimulateddataprocess.\n3.Thisisthefinalstepwherealltheservicesareimplementedandassociatedwith\nthefinalprototype.\n•ThisExtremePrototypingmethodmakestheprojectcyclinganddeliveryrobust\nandfast,andkeepstheteamfocuscentralizedonproductsdeliveriesratherthan\ndiscoveringallpossibleneedsandspecificationsandaddingunneededfeatures.\n20SDEV 3309 -Software Development Methodologies\n\n•Dynamic Pages\n•Calling databases and \nother resources.\n•All services\n•Dynamic Pages\n•Some functions\n•Navigation\n•interactive\n•Static Pages\n•HTML, CSS, JavaScripts\n•Structure\n•Limited interactivity\n4. Extreme Prototyping\n21SDEV 3309 -Software Development Methodologies\nStatic Prototype\nPhase\nDynamic Prototype \nPhase\nService Implementation \nPhase\n\nWays of Preparing a Prototype\n•Sketchesanddiagrams:Sketchesarebestfortranslatingideasattheearlystages\nofthetypicalprototypingprocessintoreality.Ontheotherhand,diagramscome\ninhandyindistillingcomplexsituations.\n•Storyboards:Asaprototypingmethod,storyboardingisusefulforgenerating\nnoteworthyideasandcultivatinganunderstandingofusers.\n•Roleplay:Theroleplaytechniqueinvolvesrecordinganddisplayingthe\nemotionalexperienceauserwillhavewhenusingaproduct.Roleplaysare\nversatile,buttheyarebestusedforstimulatingtheuser’senvironment.\n•Paperprototyping:Thismethodisrelevantintheprototypingofdigitalproducts.\nItisaffordable,simple,andeffectiveinuncoveringusabilitychallenges.\n22SDEV 3309 -Software Development Methodologies\n\nWays of Preparing a Prototype\n•Mockups:Amockupisasimplevisualrepresentationmadeforpresentation\npurposes.Theyareareplicaofafinishedproductatfirstglance.Theonly\ndifferenceis,theylackprominentfunctionalities.\n•Wireframes:Wireframesdealwiththestructuralpartsofaproduct’suser\ninterface(UI)anddefineafeature’sfunctionalprocesses.Theyarethecoreofa\nUI.Togetstartedwithwireframes,youcaneitheruseapaperandpenortakeit\nfurtherbyusingtoolssuchasProto.io,Balsamiq,andSketch.\n•Prototypingwithexplanatoryvideos:Withthehelpofexplanatoryvideosacting\nasanMVP,ateamcancommunicatethesimpledefinitionofaproduct’spurpose\nandfeatures.Userscanunderstandhowaproductworkswithoutusingitsfinal\nformyet.\n23SDEV 3309 -Software Development Methodologies\n\nForms of Prototyping\n•Theprototypestakethefollowingforms:\n•Physical:likePaperprototypes\n•Digital:\n•Low-Fidelityprototypes\n•High-Fidelityprototypes\n24SDEV 3309 -Software Development Methodologies\n\nLow-Fidelity (lo-fi) Prototypes\n•Sometimescalledwireframes.Thesequickandinexpensiveprototypesare\neffortlesstomake.\n•Theycanbeusedtogatheruserfeedbackduringtheearlystages.\n•Theonlydownsidetolo-fiprototypingisthelackofaestheticalextravaganza.\n•Low-fidelityprototypesareoptimalfor:\n•Workingonideasinprogress\n•Presentingthebasicideaofasolution\n•Displayingtheearlydevelopmentsofwork\n•Confirmingtheprimaryfunctionalitiesofaproductwiththewholeteam\n25SDEV 3309 -Software Development Methodologies\n\nHigh-Fidelity (hi-fi) Prototypes\n•Hi-fiprototypesdepicttheintendedsolutionasperfectlyaspossible.\n•Theypresentthefinalproductfunctionalitiesalongwithitsintendedaesthetic\ndisplay.\n•Ifdoneright,theyareeasilyaccessible,aestheticallypolished,andareadybase\nforfuturedevelopment.\n•High-fidelityprototypesarebestfor:\n•Presentingthenearlycompletedideatostakeholders.\n•Testingyourideabeforespendingahugebudget.\n•Completingthedesignprocess.\n•Evaluatingthedevelopedusabilityofyourintendedsolution.\n26SDEV 3309 -Software Development Methodologies\n\nWhen to use Prototyping?\n•ThePrototypingModelshouldbeusedwhentherequirementsoftheproductare\nnotclearlyunderstoodorareunstable.\n•Itcanalsobeusedifrequirementsarechangingquickly.\n•Suitedforcomplicatedsystemswhensolutionsarenotclear.\n•Itcanbeusedwhenalotofinteractionisneededwithusers.\n•Thismodelcanbesuccessfullyusedfordevelopinguserinterfaces.\n•Itisalsoaverygoodchoicetodemonstratethetechnicalfeasibilityofthe\nproduct.\n27SDEV 3309 -Software Development Methodologies\n\nAdvantages of Prototyping\n•Improvedsystemusability\n•Aclosermatchtousers’realneeds\n•Improveddesignquality\n•Improvedmaintainability\n•Reduceddevelopmenttimeandeffort\n•Canbeusedinothersimilarprojects\n•Detecterrorsandproblemsearly\n28SDEV 3309 -Software Development Methodologies\n\nDisadvantages of Prototyping\n•Variationinrequirementsaftereachprototypeevaluation.\n•PoorDocumentationduetocontinuouslychangingrequirements.\n•Difficultytoaccommodateallthechanges.\n•Uncertaintyindeterminingthenumberofiterations.\n•Afterseeinganearlyprototype,thecustomerssometimesdemandtheactual\nproducttobedeliveredsoon.\n•Teaminahurrytobuildprototypesmayendupwithsub-optimalsolutions.\n•Thecustomermightloseinterestintheproductifhe/sheisnotsatisfiedwiththe\ninitialprototype.\n29SDEV 3309 -Software Development Methodologies\n\nBest-used Prototyping Tools\n•Adobeexperiencedesign\n•InVision\n•Figma\n•Sketch\n•Origamistudio\n•Axure\n•Balsamiq\n•Justinmind\n•Webflow\n•Flinto\n30SDEV 3309 -Software Development Methodologies\n\nThank You\nThe main sources of the slides’ content are: \n•IanSommerville(2016),Software\nEngineering,10thedition,AddisonWesley.\n•Someonlinesources.', 287, 3, '2025-06-04 07:50:52', '2025-06-04 07:50:52'),
(61, 'شسؤشس', '09 Agile Methodologies - Scrum.pdf', NULL, 'documents/q8vM0wBfxBCYYrnpnjiXbXThkgUshOLxCwA6Uo4u.pdf', 'Agile Methodologies –\nScrum\nDr. Tamer Madi\nSDEV 3309 –Software Development Methodologies\n\nTopics Covered\n•Introduction\n•Values\n•Scrum process\n•Practices\n•Scrum team\n•Scrumartifacts\n•When to use\n•Advantages and disadvantages\n2SDEV 3309 -Software Development Methodologies\n\nIntroduction\n•LikeXP,Scrumisoneofthemostwell-known\nagilemethods.\n•Scrumisanagilemethodthatfocuseson\nmanagingiterativedevelopmentratherthan\nspecificagilepractices.\n•Scrumpaysmoreattentiononhowtheteam\nshouldworktoproducethesystemina\nconstantlychangingclimate..\n•Itwasdevelopedbeforethemanifestoby\nJeffSutherlandandKenSchwaber1999.\n3SDEV 3309 -Software Development Methodologies\nJeff Sutherland and Ken Schwaber\n\nIntroduction\n•Thenameisderivedfromanactivitythatoccursduringarugby\nmatch.\n•Theteamworksasasingleunittoachievepredefinedgoal.\n4SDEV 3309 -Software Development Methodologies\n\nIntroduction\n•Scrumisalightweightframeworkthathelpspeople,teamsand\norganizationsgeneratevaluethroughadaptivesolutionsforcomplex\nproblems.\n•Oneofthemostpopularagilemethodologieswhichhelpscompanies\ntomanageprojects.\n•Thereareseveralcommunitiessupportthismethodologysuchas\nscrum.orgwhichwasestablished2009.\n5SDEV 3309 -Software Development Methodologies\n\nScrum Values\n•Commitmenteachoneofthestakeholdercommitstoadheretohis/herworkand\nresponsibility.\n•Focusteamhastofocusonthestatedgoalsoftheiteration,withoutdistraction.\nThus,managementfocusesonprovidingtheteamwithresources,removingblocks,\nandavoidinginterrupting.\n•OpennesstheopenlyaccessibleProductBacklogmakesvisibletheworkandpriorities.\nTheDailyScrumsmakevisibletheoverallandindividualstatusandcommitments.\n•Respecttheindividualmembersonateamarerespectedfortheirdifferentstrengths\nandweaknesses.\n•Couragemanagementhasthecouragetotrustindividuals.Theteamhasthecourage\ntotakeresponsibilityforself-management.\n6SDEV 3309 -Software Development Methodologies\n\n•ProductBacklog.Alistofproductfeaturestobedeveloped.\n•Sprint.AsingledevelopmentcycleinScrumtodevelopanincrement\nfromtheproduct.Ittakesto4weeks.\n•SprintBacklog.Alistofproductfeaturestobedevelopedinthe\nsprint.\n•Burn-downChart.Aprogressaftereachsprint.\n•UserStories.Thefeaturesrequestedbycustomer.\n7SDEV 3309 -Software Development Methodologies\nScrum Terms\n\n•Scrumprocessisiterativeandincrementalinnature.\n•Theprojectisperformedthroughaseriesofcyclescalledsprints.\n•Eachsprinttakes1to4weekstodeliverareleaseoftheproduct.\n•Scrumprocessinvolvesthreephases:pre-game,development(game)\nandpost-gameasshowninthefollowingfigure:\n8SDEV 3309 -Software Development Methodologies\nProcess of Scrum\n\nContinuous \nIntegration\nRegular\nupdates\nProcess of Scrum\n9SDEV 3309 -Software Development Methodologies\nPre-Game	Game	Post-Game\nRelease\n•Priorities\n•Estimates\nSprint\n1-4 weeks\nProduct \nBacklog\nDaily\nScrum\n24 hours\nSprint \nBacklog\nFinal \nRelease\nSprint \nReview\nSprint \nPlanning\nRetrospective\n\n•Goals\n▪Planfortheworkneededinthesprint[sprintbacklog].\n▪Theplanispreparedthroughthecollaborationwithalltheteam.\n•Duration\n▪8hoursforasprintofonemonth.\n•Outcomes\n▪Theselectionofsprintbacklogfromtheproductbacklog.\n▪Determiningthesprintgoals.\n•Meeting\n▪Themeetingisdividedintotwopartseachoneisahalfoftheduration:\n10SDEV 3309 -Software Development Methodologies\n1. Sprint Planning Meeting\n\n•Q1.Whatwecanachieveduringthesprint?\n▪Theteamdiscusseswiththeproductownerthesprintgoalandtheitemsinthe\nproductbacklogthatcanachievethisgoal.\n▪Theteamestimatetheitemsthatcanbeincludedinthesprint.\n▪Afterselectingtheitemsforthesprint,theteamclarifiesthesprintgoaltoallthe\ndevelopmentteammembers.\n•Q2.Howwecanachieveit?\n▪Thedevelopmentteamplansthewayofdoingtheworktoconverttheitemsinthe\nsprintbacklogintoausableproduct.\n▪Theproductownershouldmakethingsclearandswapssomeitemswithothersif\nnecessary.\n▪Bytheendofthemeeting,thedevelopmentteamshouldbeabletoworkasaself-\norganizingteamtoachievethesprintgoal.\n11SDEV 3309 -Software Development Methodologies\n1. Sprint Planning Meeting\n\n•SprintGoal\n▪Itistheobjectivesetforthesprint,whichcanbeachievedthroughthe\ndevelopmentoftheitemsincludedinthesprintbacklog.\n▪Theteamfocusesonthegoalwhiledoingthework.\n▪Duringthesprint,iftherequiredworkisdifferentfromwhatthe\ndevelopmentteamexpects,theteamcommunicateswiththeproductowner\nandnegotiatesthescopeofthesprintwork.\n▪Thesprintgoalrepresentstheselectedstories[sprintbacklog].\n12SDEV 3309 -Software Development Methodologies\n1. Sprint Planning Meeting\n\n•Goals\n▪SprintsaretheheartbeatofScrum,whereideasareturnedintovalue.\n▪AlltheworknecessarytoachievetheProductGoal,includingSprintPlanning,\nDailyScrums,SprintReview,andSprintRetrospective,happenwithinSprints.\n•Duration\n▪1to4weeks.\n•Outcomes\n▪Increment.\n•Activity\n▪Duringtheactivitythefollowingareperformed:\n13SDEV 3309 -Software Development Methodologies\n2. Sprint\n\n•Nochangesaremadethatwouldendangerthesprintgoal.\n•Targetqualitycannotbedecreased.\n•Theproductbacklogisrefinedasneeded.\n•Scopemaybeclarifiedandrenegotiatedwiththeproductownerasmoreislearned.\n•Sprintsenablepredictabilitybyensuringinspectionandadaptationofprogress\ntowardaProductGoalatleasteverycalendarmonth.\n•Whenasprint’shorizonistoolongthesprintgoalmaybecomeinvalid,complexity\nmayrise,andriskmayincrease.ShorterSprintscanbeemployedtogeneratemore\nlearning.\n14SDEV 3309 -Software Development Methodologies\n2. Sprint\n\n•EachSprintmaybeconsideredashortproject.\n•Variouspracticesexisttoforecastprogress,likeburn-downs,burn-ups,or\ncumulativeflows.\n•Whileprovenuseful,thesedonotreplacetheimportanceofempiricism.\n•ASprintcouldbecancellediftheSprintGoalbecomesobsolete.OnlytheProduct\nOwnerhastheauthoritytocanceltheSprint.\n15SDEV 3309 -Software Development Methodologies\n2. Sprint\n\n•Goals\n▪Thedevelopmentteammemberssharetheinformationregardingthetasks\ntheyaredoinganddevelopanactionplanfortheupcoming24hours.\n•Duration\n▪15minutesdaily.\n•Outcomes\n▪Facilitatedtasks.\n•Meeting\n▪Everymeetingeachmemberinthedevelopmentteamshouldanswerthe\nfollowingquestions:\n16SDEV 3309 -Software Development Methodologies\n3. Daily Scrum Meeting\n\n•Whatdidyoudo\nyesterdaytohelp\nthedevelopment\nteammeetthe\nsprintgoal?\n17SDEV 3309 -Software Development Methodologies\n3. Daily Scrum Meeting\nYesterday	Today	Tomorrow\n•WhatwillIdotoday\nto help the\ndevelopmentteam\nmeetthesprint\ngoal?\n•WillIseeany\nobstacles that\npreventmeandthe\nteamfromreaching\nthegoaltomorrow?\n\n•Remarks:\n▪Theobstaclesandproblemscannotbediscussedinthedailymeeting,butare\nconveyedtothescrummaster.\n▪Theteamusesthedailymeetingtomeasuretheprogresstowardsthesprint\ngoal.\n▪Themanagermakessurethatonlythedevelopmentteamattendsthis\nmeetingandthatthemeetingdoesnotexceed15minutes.\n▪Thedailymeetingimprovesthecommunicationbetweentheteamand\neliminatestheneedforothermeetings.\n▪Themeetingidentifiesobstaclesthatneedtoberemovedandimprovesthe\nabilitytomakedecisionsquickly.\n18SDEV 3309 -Software Development Methodologies\n3. Daily Scrum Meeting\n\n•Goals\n▪ItistoinspecttheoutcomeoftheSprintanddeterminefutureadaptations.\n▪Theteampresentstheresultsoftheirworktokeystakeholdersandprogress\ntowardtheProductGoalisdiscussed.\n•Duration\n▪4hoursforasprintofonemonth.\n•Outcomes\n▪Productrelease[increment].\n•Meeting\n▪Duringthemeetingthefollowingactivitiesareperformed:\n19SDEV 3309 -Software Development Methodologies\n4. Sprint Review\n\n•Theteamandstakeholdersreviewwhatwasaccomplishedinthe\nsprintandwhathaschangedintheirenvironment.\n•Basedonthisinformation,attendeescollaborateonwhattodonext.\n•Theproductbacklogmayalsobeadjustedtomeetnew\nopportunities.\n•Thesprintreviewisaworkingsessionandtheteamshouldavoid\nlimitingittoapresentation.\n20SDEV 3309 -Software Development Methodologies\n4. Sprint Review\n\n•Goals\n▪Thepurposeofthesprintretrospectiveistoplanwaystoincreasequalityand\neffectivenessoftheupcomingsprints.\n•Duration\n▪3hoursforasprintofonemonth.\n•Outcomes\n▪Improvedprocess.\n•Meeting\n▪Duringthemeetingthefollowingactivitiesareperformed:\n21SDEV 3309 -Software Development Methodologies\n5. Retrospective Meeting\n\n•Theteaminspectshowthelastsprintwentwithregardstoindividuals,\ninteractions,processes,tools,andtheirDefinitionofDone.\n•Assumptionsthatledthemastrayareidentifiedandtheiroriginsexplored.\n•Theteamdiscusseswhatwentwellduringthesprint,whatproblemsit\nencountered,andhowthoseproblemswere(orwerenot)solved.\n•Theteamidentifiesthemosthelpfulchangestoimproveitseffectiveness.\nThemostimpactfulimprovementsareaddressedassoonaspossible.\n22SDEV 3309 -Software Development Methodologies\n5. Retrospective Meeting\n\nScrum Team\n23SDEV 3309 -Software Development Methodologies\n•ThescrumteamconsistsofoneProductOwner,oneScrumMaster,andDevelopers.\n•Withinascrumteam,therearenosub-teamsorhierarchies.Itisacohesiveunitof\nprofessionalsfocusedononeobjectiveatatime,theproductgoal.\n•Scrumteamsarecross-functional,meaningthemembershavealltheskillsnecessaryto\ncreatevalueeachSprint.\n•Theyarealsoself-managing,meaningtheyinternallydecidewhodoeswhat,when,and\nhow.\n•Theteamistypically10orfewerpeopleassmallerteamscommunicatebetterandare\nmoreproductive.\n•Thescrumteamisresponsibleforallproduct-relatedactivities.\n\nScrum Team\nRoles in Scrum\n24SDEV 3309 -Software Development Methodologies\nBusiness Owner\nScrum Master\nProduct Owner\n\n•TheProductOwnerisaccountableformaximizingthevalueoftheproduct.Heisthe\nsourceofsystemrequirementsandisalsoresponsibleofeffectiveProductBacklog\nmanagement,whichincludes:\n▪DevelopingandexplicitlycommunicatingtheProductGoal.\n▪CreatingandclearlycommunicatingProductBacklogitems.\n▪OrderingProductBacklogitems.\n▪EnsuringthattheProductBacklogistransparent,visibleandunderstood.\n▪Specifyingtherequiredscheduletodelivertheproduct.\n▪Matchingthesprints’incrementswiththeacceptancecriteria.\n•Insomephases,heisapartoftheteam.Attheendofeachsprintheisresponsibleof\nevaluationandprovidingsomefeedbackbutingeneralheisisolatedfromtheteam.\n25SDEV 3309 -Software Development Methodologies\nProduct Owner\n\n•TheScrumMasterisaccountableforestablishingScrumasdefinedintheScrumGuide.\nTheydothisbyhelpingeveryoneunderstandScrumtheoryandpractice,bothwithinthe\nscrumteamandtheorganization.\n•TheScrumMasterisaccountableforthescrumteam’seffectiveness.Theydothisby\nenablingthescrumteamtoimproveitspractices,withinthescrumframework.\n•TheScrumMasterservestheScrumTeaminseveralways,including:\n▪Coachingtheteammembersinself-managementandcross-functionality.\n▪HelpingtheScrumTeamfocusoncreatinghigh-valueIncrementsthatmeettheDefinitionofDone.\n▪CausingtheremovalofimpedimentstotheScrumTeam’sprogress.\n▪EnsuringthatallScrumeventstakeplaceandarepositive,productive,andkeptwithinthetimebox.\n26SDEV 3309 -Software Development Methodologies\nScrum Master\n\n•TheScrumMasterservestheProductOwnerinseveralways,\nincluding:\n▪HelpingfindtechniquesforeffectiveProductGoaldefinitionandProductBacklog\nmanagement.\n▪HelpingtheScrumTeamunderstandtheneedforclearandconciseProductBacklog\nitems.\n▪Helpingestablishempiricalproductplanningforacomplexenvironment.\n▪Facilitatingstakeholderscollaborationasrequestedorneeded.\n27SDEV 3309 -Software Development Methodologies\nScrum Master\n\n•TheScrumMasterservestheorganizationinseveralways,including:\n▪Leading,training,andcoachingtheorganizationinitsScrumadoption.\n▪PlanningandadvisingScrumimplementationswithintheorganization.\n▪Helpingemployeesandstakeholdersunderstandandenactanempiricalapproach\nforcomplexwork.\n▪RemovingbarriersbetweenstakeholdersandScrumTeams.\n28SDEV 3309 -Software Development Methodologies\nScrum Master\n\n•Developersarethepeopleinthescrumteamthatarecommittedtocreatingany\naspectofausableincrementeachsprint.\n•Thespecificskillsneededbythedevelopersareoftenbroadandwillvarywith\nthedomainofwork.\n•However,theDevelopersarealwaysaccountablefor:\n•CreatingaplanfortheSprint,theSprintBacklog.\n•Breakingdowntheworkintoseveraltasksandassignthemtoteammembers.\n•InstillingqualitybyadheringtoaDefinitionofDone.\n•AdaptingtheirplaneachdaytowardtheSprintGoal.\n•Holdingeachotheraccountableasprofessionals.\n29SDEV 3309 -Software Development Methodologies\nDevelopers\n\n•Scrum’sartifactsrepresentworkorvalue.Theyaredesignedto\nmaximizetransparencyofkeyinformation.\n•Eachartifactcontainsacommitmenttoensureitprovides\ninformationthatenhancestransparencyandfocusagainstwhich\nprogresscanbemeasured:\n•FortheProductBacklogitistheProductGoal.\n•FortheSprintBacklogitistheSprintGoal.\n•FortheIncrementitistheDefinitionofDone.\n30SDEV 3309 -Software Development Methodologies\nScrum Artifacts\n\n•TheProductBacklogisanemergent,orderedlistofwhatisneededtoimprove\ntheproduct.Itisthesinglesourceofworkundertakenbythescrumteam.\n•ProductBacklogitems(PBI)thatcanbeDonebythescrumteamwithinone\nsprintaredeemedreadyforselectioninaSprintPlanningevent.\n•Theyusuallyacquirethisdegreeoftransparencyafterrefiningactivities.\n•ProductBacklogrefinementistheactofbreakingdownandfurtherdefining\nProductBacklogitemsintosmallermorepreciseitems.\n•Thisisanongoingactivitytoadddetails,suchasadescription,order,andsize.\n31SDEV 3309 -Software Development Methodologies\nProduct Backlog\n\n•TheSprintBacklogiscomposedoftheSprintGoal(why),thesetofProduct\nBacklogitemsselectedfortheSprint(what),aswellasanactionableplanfor\ndeliveringtheIncrement(how).\n•TheSprintBacklogisaplanbyandfortheDevelopers.Itisahighlyvisible,real-\ntimepictureoftheworkthattheDevelopersplantoaccomplishduringtheSprint\ninordertoachievetheSprintGoal.\n•Consequently,theSprintBacklogisupdatedthroughouttheSprintasmoreis\nlearned.\n•ItshouldhaveenoughdetailthattheycaninspecttheirprogressintheDaily\nScrum.\n32SDEV 3309 -Software Development Methodologies\nSprint Backlog\n\n•Userstoriesareasimplifieddescriptionofproductrequirements.\n•Thestorydescribesthetypeofuser,whathewantsandwhy.\n•Theuserstoriesincludes3keyaspectscalled3Cs:\n▪Card:asmallpieceofpaperinwhichthestoryiswritten,estimatedtime,\npriorityandsomeotherinformation.\n▪Conversation:discussionsbetweenproductownerandthedevelopers.\n▪Confirmation:theacceptancecriteriafromtheperspectiveoftheproduct\nownertoacceptandfinishthestory.\n33SDEV 3309 -Software Development Methodologies\nUser Stories\n\n34SDEV 3309 -Software Development Methodologies\nUser Stories\n\n•AnIncrementisaconcretesteppingstonetowardtheProductGoal.\n•EachIncrementisadditivetoallpriorIncrementsandthoroughlyverified,\nensuringthatallIncrementsworktogether.Inordertoprovidevalue,the\nIncrementmustbeusable.\n•MultipleIncrementsmaybecreatedwithinaSprint.Thesumofthe\nIncrementsispresentedattheSprintReview.However,anIncrementmay\nbedeliveredtostakeholderspriortotheendoftheSprint.\n•WorkcannotbeconsideredpartofanIncrementunlessitmeetsthe\nDefinitionofDone.\n35SDEV 3309 -Software Development Methodologies\nIncrement\n\n•TheDefinitionofDoneisaformaldescriptionofthestateoftheIncrement\nwhenitmeetsthequalitymeasuresrequiredfortheproduct.\n•ThemomentaProductBacklogitemmeetstheDefinitionofDone,an\nIncrementisborn.\n•TheDefinitionofDonecreatestransparencybyprovidingeveryoneashared\nunderstandingofwhatworkwascompletedaspartoftheIncrement.\n•IfaProductBacklogitemdoesnotmeettheDefinitionofDone,itcannotbe\nreleasedorevenpresentedattheSprintReview.Instead,itreturnstothe\nProductBacklogforfutureconsideration.\n•TheDevelopersarerequiredtoconformtotheDefinitionofDone.\n36SDEV 3309 -Software Development Methodologies\nDefinition of Done\n\nWhen to use Scrum?\n•Whentherequirementsarenotclear.\n•Whenwecannotexpecttheprojectresults.\n•Whentheproductisdividedintoseveralreleases.\n•Whenweneedtofocusonmanagingthedevelopmentactivities.\n37SDEV 3309 -Software Development Methodologies\n\nAdvantages of Scrum\n•It\'sadaptableandflexible.\n•Itusuallyleadstobetterqualitywork.\n•ItimprovesTimetoMarket.\n•Itensurescontinuousfeedbacks.\n•Itimprovescustomersatisfaction.\n•Ittypicallyresultsinmoresatisfiedemployees.\n•Itreducescosts.\n38SDEV 3309 -Software Development Methodologies\n\nDisadvantages of Scrum\n•Itrequiresextensivetraining.\n•Itcanbedifficulttoscale.\n•Itmayrequiremajortransformationswithintheorganization.\n•Itrequiresstrongcommitmentfromallteammembers.\n•Itcanbedifficulttointegratewithaclassicprojectmanagement\napproach.\n39SDEV 3309 -Software Development Methodologies\n\nThank You\nThe main sources of the slides’ content are: \n•IanSommerville(2016),Software\nEngineering,10thedition,AddisonWesley.\n•ScrumGuide(2020)\n•Someonlinesources.', 923, 2, '2025-06-04 07:51:14', '2025-06-04 07:51:14'),
(62, 'سيرسير', '02 Introduction to software Processes (1).pdf', NULL, 'documents/T0HDn9kK1eq7B9hN1oi0RDyikajtQJdAfMplrxsV.pdf', 'Introduction to\nSoftware Processes\nDr. Tamer Madi\nSDEV 3309 –Software Development Methodologies\n\nTopics Covered\n•Software process models\n•Process activities\n•Coping with change\n•Process improvement [Last two weeks of semester]\n2SDEV 3309 -Software Development Methodologies\n\nWhy we need a Software Process?\n•Processisasystematicway.\n•Processisguidingstructure.\n•Processisformal.\n•Processoffersreliabilityandconsistency.\n•Processprovidesasharedconceptionofwhatwearedoing.\n3SDEV 3309 -Software Development Methodologies\n\nThe Software Process\n•Astructuredsetofactivitiesrequiredtodevelopa\nsoftwaresystem.\n•Manydifferentsoftwareprocessesbutallinvolve:\n•Specification–definingwhatthesystemshoulddo;\n•Designandimplementation–definingtheorganizationofthesystemandimplementingthe\nsystem;\n•Validation–checkingthatitdoeswhatthecustomerwants;\n•Evolution–changingthesysteminresponsetochangingcustomerneeds.\n•Asoftwareprocessmodel(methodology)isanabstractrepresentationofa\nprocess.Itpresentsadescriptionofaprocessfromsomeparticularperspective.\n4SDEV 3309 -Software Development Methodologies\n\nSoftware Process vs. Methodology\n•Asoftwaremethodologyisnotthesameasasoftwareprocess.\n•Asoftwareprocessis,inessence,the“what”ofthedevelopmentactivities.The\nprocessidentifiestheorderofphases.Itestablishesphasetransitioncriteriaand\nindicateswhatistobedoneineachphaseandwhentostop.\n•However,thetermsprocessmodelandmethodologyareoftenused\ninterchangeablyanditdescribes“how”toperformthedevelopmentactivities.\n•Forexample,thereisbothanagilesoftwareprocessandmanydifferentagile\nmethodologies.\n5SDEV 3309 -Software Development Methodologies\n\nSoftware Process Descriptions\n•Whenwedescribeanddiscussprocesses,weusuallytalkaboutthe\nactivitiesintheseprocessessuchasspecifyingadatamodel,designinga\nuserinterface,etc.andtheorderingoftheseactivities.\n•Processdescriptionsmayalsoinclude:\n•Products,whicharetheoutcomesofaprocessactivity;\n•Roles,whichreflecttheresponsibilitiesofthepeopleinvolvedintheprocess;\n•Pre-andpost-conditions,whicharestatementsthataretruebeforeandaftera\nprocessactivityhasbeenenactedoraproductproduced.Forexample,before\narchitecturaldesignbegins,apreconditionmaybethattheconsumerhasapproved\nallrequirements.\n6SDEV 3309 -Software Development Methodologies\n\nThe Software Process\n7SDEV 3309 -Software Development Methodologies\nProcess\nPhases\nActivities\nTasks\nRole Outcome Resources\nA representation of how you put the lifecycle \nactivities together in a sequence over time.\nA collection of logically related activities.\nA set of purposeful tasks.\nA piece of work to be done.\n\nPlan-Driven and Agile Processes\n•Plan-drivenprocessesareprocesseswherealloftheprocessactivities\nareplannedinadvanceandprogressismeasuredagainstthisplan.\n•Inagileprocesses,planningisincrementalanditiseasiertochange\ntheprocesstoreflectchangingcustomerrequirements.\n•Inpractice,mostpracticalprocessesincludeelementsofbothplan-\ndrivenandagileapproaches.\n•Therearenorightorwrongsoftwareprocesses.\n8SDEV 3309 -Software Development Methodologies\n\nProcess Activities\n•Realsoftwareprocessesareinter-leavedsequencesoftechnical,\ncollaborativeandmanagerialactivitieswiththeoverallgoalof\nspecifying,designing,implementingandtestingasoftwaresystem.\n•Thefourbasicprocessactivitiesofspecification,development,\nvalidationandevolutionareorganizeddifferentlyindifferent\ndevelopmentprocesses.\n•Forexample,inthewaterfallmodel,theyareorganizedinsequence,\nwhereasinincrementaldevelopmenttheyareinterleaved.\n9SDEV 3309 -Software Development Methodologies\n\nThe Requirements Engineering Process \n10SDEV 3309 -Software Development Methodologies\n\nSoftware Specification\n•Theprocessofestablishingwhatservicesarerequiredandthe\nconstraintsonthesystem’soperationanddevelopment.\n•Requirementsengineeringprocess:\n•Requirementselicitationandanalysis\n•Whatdothesystemstakeholdersrequireorexpectfromthesystem?\n•Requirementsspecification\n•Definingtherequirementsindetail\n•Requirementsvalidation\n•Checkingthevalidityoftherequirements\n11SDEV 3309 -Software Development Methodologies\n\nDesign and Implementation\n•The process of convertingthe system specification into an executable \nsystem.\n•Software design\n•Design a software structure that realizes the specification;\n•Implementation\n•Translate this structure into an executable program;\n•The activities of design and implementation are closely relatedand \nmay be inter-leaved.\n12SDEV 3309 -Software Development Methodologies\n\nA General Model of The Design Process \n13SDEV 3309 -Software Development Methodologies\n\nDesign Activities\n•Architecturaldesign,whereyouidentifytheoverallstructureofthe\nsystem,theprincipalcomponents(subsystemsormodules),their\nrelationshipsandhowtheyaredistributed.\n•Databasedesign,whereyoudesignthesystemdatastructuresand\nhowthesearetoberepresentedinadatabase.\n•Interfacedesign,whereyoudefinetheinterfacesbetweensystem\ncomponents.\n•Componentselectionanddesign,whereyousearchforreusable\ncomponents.Ifunavailable,youdesignhowitwilloperate.\n14SDEV 3309 -Software Development Methodologies\n\nImplementation\n•Thesoftwareisimplementedeitherbydevelopingaprogramor\nprogramsorbyconfiguringanapplicationsystem.\n•Designandimplementationareinterleavedactivitiesformosttypes\nofsoftwaresystem.\n•Programmingisanindividualactivitywithnostandardprocess.\n•Debuggingistheactivityoffindingprogramfaultsandcorrecting\nthesefaults.\n15SDEV 3309 -Software Development Methodologies\n\nSoftware Validation\n•Verificationandvalidation(V&V)isintendedtoshowthatasystem\nconformstoitsspecificationandmeetstherequirementsofthesystem\ncustomer.\n•Involvescheckingandreviewprocessesandsystemtesting.\n•Systemtestinginvolvesexecutingthesystemwithtestcasesthatare\nderivedfromthespecificationoftherealdatatobeprocessedbythe\nsystem.\n•TestingisthemostcommonlyusedV&Vactivity.\n•Validation:Doingtherightthing.\n•Verification:Doingthethingright.\n16SDEV 3309 -Software Development Methodologies\n\nStages of Testing \n17SDEV 3309 -Software Development Methodologies\n\n•Component(Unit)testing\n•Individualcomponentsaretestedindependently;\n•Componentsmaybefunctionsorobjectsorcoherentgroupingsofthese\nentities.\n•Systemtesting\n•Testingofthesystemasawhole.Testingofemergentpropertiesis\nparticularlyimportant.\n•Customer(Acceptance)testing\n•Testingwithcustomerdatatocheckthatthesystemmeetsthecustomer’s\nneeds.\nStages of Testing\n18SDEV 3309 -Software Development Methodologies\n\nSoftware Evolution\n•Software is inherently flexible and can change. \n•As requirements change through changing business circumstances, \nthe software that supports the business must also evolve and change.\n•Although there has been a demarcation between development and \nevolution (maintenance) this is increasingly irrelevant as fewer and \nfewer systems are completely new.\n19SDEV 3309 -Software Development Methodologies\n\nSoftware Evolution \n20SDEV 3309 -Software Development Methodologies\n\nCoping with Change\n•Changeisinevitableinalllargesoftwareprojects.\n•Businesschangesleadtonewandchangedsystemrequirements\n•Newtechnologiesopenupnewpossibilitiesforimprovingimplementations\n•Changingplatformsrequireapplicationchanges\n•Changeleadstoreworksothecostsofchangeincludebothrework\n(e.g.re-analyzingrequirements)aswellasthecostsofimplementing\nnewfunctionality\n21SDEV 3309 -Software Development Methodologies\n\nReducing the Costs of Rework\n•Changeanticipation,wherethesoftwareprocessincludesactivities\nthatcananticipatepossiblechangesbeforesignificantreworkis\nrequired.\n•Forexample,aprototypesystemmaybedevelopedtoshowsomekey\nfeaturesofthesystemtocustomers.\n•Changetolerance,wheretheprocessisdesignedsothatchangescan\nbeaccommodatedatrelativelylowcost.\n•Thisnormallyinvolvessomeformofincrementaldevelopment.Proposed\nchangesmaybeimplementedinincrementsthathavenotyetbeen\ndeveloped.Ifthisisimpossible,thenonlyasingleincrement(asmallpartof\nthesystem)mayhavebealteredtoincorporatethechange.\n22SDEV 3309 -Software Development Methodologies\n\nCoping with Changing Requirements\n•Systemprototyping,whereaversionofthesystemorpartofthe\nsystemisdevelopedquicklytocheckthecustomer’srequirements\nandthefeasibilityofdesigndecisions.Thisapproachsupportschange\nanticipation.\n•Incrementaldelivery,wheresystemincrementsaredeliveredtothe\ncustomerforcommentandexperimentation.Thissupportsboth\nchangeavoidanceandchangetolerance.\n23SDEV 3309 -Software Development Methodologies\n\nCoping with Changing Requirements\n•Systemprototyping,whereaversionofthesystemorpartofthe\nsystemisdevelopedquicklytocheckthecustomer’srequirements\nandthefeasibilityofdesigndecisions.Thisapproachsupportschange\nanticipation.\n•Incrementaldelivery,wheresystemincrementsaredeliveredtothe\ncustomerforcommentandexperimentation.Thissupportsboth\nchangeavoidanceandchangetolerance.\n24SDEV 3309 -Software Development Methodologies\n\nThank You\nThe main source of the slides’ content is \nthe book of Ian Sommerville (2016), \nSoftware Engineering, 10th edition, \nAddison Wesley.', 372, 2, '2025-06-04 07:52:34', '2025-06-04 07:52:34'),
(63, 'لابلاب', 'MidTerm Exam (1).docx', NULL, 'documents/kbrpgweEK1ekJiOjbSAyS5WBVUxW18MGyPTYmn2C.docx', 'Islamic University - Gaza Faculty of   Information Technology Department of Computer Science. SDEV   3304  – Data Mining  M i dterm Exam Su nday 2 6 / 11 / 2011 Student ID : --------- - ------ - -------       Student Name : -------------------------------------------------------- (14  points) Choose the correct answer: Correlation analysis methods always used as ________ methods. data cleaning data integration data reduction Knowledge = Information + _______________ . Context   Rules Experiences Pre-pruning and post-pruning are two approaches to overcome the _______ problem. classification underfitting overfitting ______ is a kind of Numerical data type and it has no true zero point. Quantity Interval Ratio Expectation of the wind velocities is a(an) ______ tasks. association classification prediction The hard-limit activation function’s, used within Neural Networks, result belong to _______. {1, 0} [0, 1] (- , ) Data mining is differ than statistics in __________ kind of data kinds of methods all the previous Symmetric and asymmetric are two types of the _______ data type. numerical nominal binary _____ is the percentages of tuples that the classifier labeled as positive are actually positive. Coverage Accuracy Precision Conflict resolution is required as a solution when more than one classification rule are ________. extracted triggered occurred ------------ is a data transformation method. Smoothing Discretization Clustering Find the natural grouping of instances given unlabeled data is a called ________. association clustering classification Data Integration combines data from multiple sources but it yields some problems. _______ is not one of these problems. Inconsistent values Entity identification Redundant attribute Classifier ______ refers to the ability of the classifier or predictor to make correct predictions given noisy data. robustness scalability interpretability   ( 16  points) Do as required (2  points)   Discretize the age attribute by considering  the minimum  occurrence of the class is 4 . Age 81 64 70 72 72 83 75 75 68 69 85 80 71 65 Class Y Y Y N Y N Y Y Y Y N N N N (2 points) Apply stratified sampling on the following data to reduce the dataset by 50% . Age 81 64 70 72 72 83 75 75 68 69 85 80 71 65 Class Y Y Y N Y N Y Y Y Y N N N N (3 points)  Consider  the following contingency table  for gender and handedness (right- or left-handed), then compute the Support, confidence and lift for [ Female   Right-handed ]  association rule. Right-handed  Left-handed  Totals Males  43 9 52 Females  44 4 48 Totals 87 13 100 ( 2  points) Consider the following table, then use the K-NN  Algorithm with k=1 to classify a query instance with Temp=5 and Humidity =5. Temp Humidity P lay 3 10 No 7 10 Yes 3 7 Yes (2  points) By considering  the previous table, extract a classification rule without any computation. (5 points) Consider the following dataset, by using the Information Gain which features is the decision tree root. Name Gender Height Class Kristina F <=1.7 Short Jim M >=2 Tall Magge F (1.7… 1.9] Medium Martha F (1.7… 1.9] Medium Stephanie F <=1.7 Short Bob M (1.7… 1.9] Medium Kathy F <=1.7 Short Dave M <=1.7 Short Worth M >=2 Tall Steven M >=2 Tall Debbi F (1.7… 1.9] Medium Todd M (1.7… 1.9] Medium Kim F (1.7… 1.9] Medium Amy F (1.7… 1.9] Medium Wynette F (1.7… 1.9] Medium The following table may be useful in to simplify the computation process. x 1/6 1/5 1/4 1/3 2/5 1/2 2/3 3/4 4/15 8/18 Log 2 (x) -2.58 -2.32 -2.00 -1.58 -1.32 -1.00 -0.58 -0.42 -1.91 -1.17', 65, 2, '2025-06-04 08:08:02', '2025-06-04 08:08:02');
INSERT INTO `documents` (`id`, `title`, `filename`, `file_hash`, `filepath`, `content`, `size`, `category_id`, `created_at`, `updated_at`) VALUES
(69, 'جددديد', 'Becker Select Courses 10-5-20.pdf', NULL, 'documents/h6zc6OKIYE27RGvKsxdYpUYCDn3Dp4cbI9iTDvFf.pdf', 'UPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\n1	Accounting Accounting for Goodwill and Acquisition Related Expenses	3.5 Basic 03/31/21\n2	Accounting Accounting for Income Taxes (ASC 740): Overview and Special Issues	1.5 Intermediate 03/31/21\n3	Accounting Advanced Derivative and Hedge Accounting Concepts	3.0 Intermediate 03/31/21\n4	Accounting Bankruptcy: How to Effectively Address Financial Reporting Issues and Bankruptcy Code Provis 2.0 Intermediate 03/31/21\n5	Accounting Bankruptcy: How to Effectively Address Financial Reporting Issues and Bankruptcy Code Provis 2.0 Intermediate 03/31/21\n6	Accounting Business Combinations and Consolidations, Part 2 (ASC 805 & 810)	2.0 Intermediate 03/31/21\n7	Accounting Debt – Accounting and Financial Reporting Risks	3.0 Basic 03/31/21\n8	Accounting Inspecting the Annual Report, Part 1	2.0 Basic 03/31/21\n9	Accounting Inspecting the Annual Report, Part 2	2.0 Basic 03/31/21\n10	Accounting Liquidation Basis of Accounting (ASC 205-30)	1.5 Intermediate 03/31/21\n11	Accounting Managing the Company’s Cash and Credit	1.5 Basic 03/31/21\n12	Accounting Mastering Accounting for Income Taxes (ASC 740)	2.0 Basic 03/31/21\n13	Accounting Meeting SEC Disclosure Requirements: Management\'s Discussion & Analysis of financial condit 1.0 Basic 03/31/21\n14	Accounting\nMeeting SEC Disclosure Requirements: Management\'s Discussion & Analysis of financial \ncondition and results of operations, part 2\n1.0 Basic 03/31/21\n15	Accounting\nMeeting SEC Disclosure Requirements: Management\'s Discussion & Analysis of financial \ncondition and results of operations, Part 3\n1.0 Basic 03/31/21\n16	Accounting Nonmonetary Transactions	2.0 Intermediate 03/31/21\n17	Accounting Other Comprehensive Basis of Accounting (OCBOA)	3.5 Basic 03/31/21\n18	Accounting SEC Initial Public Offering Requirements 	2.0 Intermediate 03/31/21\n19	Accounting SEC Reporting Requirements, Part 1	2.0 Basic 03/31/21\n20	Accounting SEC Reporting Requirements, Part 2	1.5 Intermediate 03/31/21\n21	Accounting SEC Reporting Requirements, Part 3	1.5 Intermediate 03/31/21\n22	Accounting Accounting & Auditing Update – Q1 2020	1.5 Basic 04/30/21\n23	Accounting Business Combinations and Consolidations, Part 1 (ASC 805 & 810)	1.5 Intermediate 04/30/21\n24	Accounting Construction Industry - Advanced Topics	1.0 Basic 04/30/21\n25	Accounting Construction Industry - Overall Landscape	1.5 Basic 04/30/21\n26	Accounting Deep Dive into Financial Instruments	2.0 Intermediate 04/30/21\n27	Accounting Equity (ASC 505)	2.5 Basic 04/30/21\n28	Accounting Financial reporting challenges - Q1 2020 with coronavirus (COVID-19) impact 1.0 Basic 04/30/21\n29	Accounting Financial Reporting Framework for Small- and Medium-Sized Entities 3.0 Basic 04/30/21\n30	Accounting Leases – ASC 842	2.0 Intermediate 04/30/21\n31	Accounting XBRL: Today\'s Language of Business Reporting	2.5 Basic 04/30/21\n32	Accounting Cracking the Codification: U.S. GAAP Research Made Easy	1.5 Basic 05/31/21\n33	Accounting Financial Instruments: Derivatives and Hedging	1.5 Basic 05/31/21\n34	Accounting Foreign Currency Accounting (ASC 830)	2.0 Overview 05/31/21\n35	Accounting Foundations of Business Combinations and Noncontrolling Interests	3.0 Basic 05/31/21\n36	Accounting Non-GAAP Financial Measures	1.5 Intermediate 05/31/21\nBecker Select - Courses\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n37	Accounting Private Company Financial Reporting	3.0 Basic 05/31/21\n38	Accounting Segment Reporting (ASC 280)	2.0 Intermediate 05/31/21\n39	Accounting Standard Costing	2.0 Intermediate 05/31/21\n40	Accounting Accounting for stock options & other stock-based compensation (ASC 718) 1.5 Basic 06/30/21\n41	Accounting Activity-Based Costing to Manage Capacity	1.5 Basic 06/30/21\n42	Accounting Financial Instruments—Credit Losses (ASU 2016-13)	1.5 Basic 06/30/21\n43	Accounting Introduction to Financing an Entity Using Equity Instruments	3.5 Basic 06/30/21\n44	Accounting Revenue Recognition: ASC 606 Analysis for the Life Sciences Industry 1.0 Advanced 06/30/21\n45	Accounting Revenue, capitalization, and expense recognition for software companies 2.0 Basic 06/30/21\n46	Accounting Sustainability Accounting and Integrated Reporting	2.0 Basic 06/30/21\n47	Accounting Accounting Changes and Error Corrections (ASC 250)	2.0 Intermediate 07/31/21\n48	Accounting Auditing Inventory	1.5 Basic 07/31/21\n49	Accounting Business Combinations and Goodwill	2.5 Basic 07/31/21\n50	Accounting Comprehensive Income	1.5 Basic 07/31/21\n51	Accounting Earnings per Share (ASC 260)	2.5 Intermediate 07/31/21\n52	Accounting Forecasting for the Start-Up Business	3.0 Basic 07/31/21\n53	Accounting Gross-to-Net Revenue Adjustments for the Pharmaceutical Industry	2.5 Basic 07/31/21\n54	Accounting Impairment of Tangibles, Intangibles, and Goodwill (effective for SEC filers)1.5 Basic 07/31/21\n55	Accounting Inventory: Techniques to Manage, Account for, and Value	2.0 Basic 07/31/21\n56	Accounting Meeting SEC Disclosure Requirements: Compensation Discussion and Analysis 1.0 Basic 07/31/21\n57	Accounting PCAOB Inspection Findings	1.5 Basic 07/31/21\n58	Accounting Revenue Recognition: ASC 606 Analysis for the Technology Industry	1.0 Advanced 07/31/21\n59	Accounting Revenue Recognition—Disclosures	2.0 Basic 07/31/21\n60	Accounting What Is Integrated Reporting?	1.5 Basic 07/31/21\n61	Accounting Accounting for Stock-Based Compensation	3.0 Basic 08/31/21\n62	Accounting Building a Persuasive Case	2.0 Basic 08/31/21\n63	Accounting Consolidation of VIEs, Part 1 (ASC 810)	1.0 Intermediate 08/31/21\n64	Accounting Consolidation of VIEs, Part 2 (ASC 810)	2.0 Intermediate 08/31/21\n65	Accounting GAAP financial statements (ASC 205-235)	1.5 Basic 08/31/21\n66	Accounting Business Restructuring: Part 1	2.0 Basic 09/30/21\n67	Accounting Business Restructuring: Part 1	2.0 Basic 09/30/21\n68	Accounting Business Restructuring: Part 2	2.0 Basic 09/30/21\n69	Accounting Business Restructuring: Part 2	2.0 Basic 09/30/21\n70	Accounting Financial Instruments—Recognition and Measurement (ASU 2016-01)	1.0 Intermediate 09/30/21\n71	Accounting Financial Reporting Challenges for 2020 - Q2	2.0 Basic 09/30/21\n72	Accounting Financial Reporting Challenges for 2020 - Q2	2.0 Basic 09/30/21\n73	Accounting Impairment of Tangibles, Intangibles, and Goodwill	2.0 Basic 09/30/21\n74	Accounting Impairment of Tangibles, Intangibles, and Goodwill	2.0 Basic 09/30/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n75	Accounting Introduction to Bookkeeping	2.0 Basic 09/30/21\n76	Accounting Not-for-Profit Entities: Financial Statement Presentation (ASU 2016-14) 1.5 Basic 09/30/21\n77	Accounting Opening a New Chapter: Fresh-Start Accounting and Subsequent Events 1.5 Intermediate 09/30/21\n78	Accounting Preparing the Statement of Cash Flows	3.0 Basic 09/30/21\n79	Accounting AICPA Valuation and Consulting Standards	2.0 Basic 10/31/21\n80	Accounting Asset Retirement and Environmental Obligations (ASC 410)	2.5 Basic 10/31/21\n81	Accounting Evaluating the Quality of Earnings	1.0 Intermediate 11/30/21\n82	Accounting FASB’s Disclosure Framework Project	2.0 Basic 11/30/21\n83	Accounting Introduction to Financial Reporting Quality	1.5 Intermediate 11/30/21\n84	Accounting Pension accounting & reporting requirements (ASC 715 & 712)	1.0 Basic 11/30/21\n85	Accounting Quality Control Standards, Part 1	1.0 Basic 11/30/21\n86	Accounting Quality Control Standards, Part 2	1.0 Basic 11/30/21\n87	Accounting Quality Control Standards, Part 3	1.0 Basic 11/30/21\n88	Accounting Quality Control Standards, Part 4	1.0 Basic 11/30/21\n89	Accounting Quality Control Standards, Part 5	1.0 Basic 11/30/21\n90	Accounting Standard Costing	2.5 Intermediate 11/30/21\n91	Accounting Transfers and Servicing of Financial Assets (ASC 860)	1.5 Intermediate 11/30/21\n92	Accounting Mergers and Acquisitions Due Diligence	2.0 Basic 12/30/21\n93	Accounting Introduction to Bookkeeping	2.0 Basic 01/31/22\n94 Accounting (Governmental) GASB Other Postemployment Benefits (OPEB)	2.0 Basic 03/31/21\n95 Accounting (Governmental) 2019 Not-For-Profit Accounting & Auditing Update	2.0 Update 04/30/21\n96 Accounting (Governmental) GASB Statement 87: Leases	2.0 Basic 04/30/21\n97 Accounting (Governmental) Federal Government Contracting—An Introduction	2.0 Basic 08/31/21\n98 Accounting (Governmental) Accounting for Revenues in Government (Emphasis on Non-Exchange Transactions)3.0 Intermediate 09/30/21\n99 Accounting (Governmental) Federal Appropriation Principles-An Overview	1.5 Basic 09/30/21\n100 Accounting (Governmental) Intermediate Governmental Accounting	2.5 Intermediate 09/30/21\n101 Accounting (Governmental) GASB 84: Fiduciary Activities	1.0 Basic 10/31/21\n102	Auditing	Accountants’ Responsibilities Regarding Fraud, Part 1	2.0 Basic 03/31/21\n103	Auditing	AICPA Control Risk Assessment Requirements	2.0 Basic 03/31/21\n104	Auditing	AICPA Engagement Quality Control Review (EQCR)	2.0 Basic 03/31/21\n105	Auditing	AICPA Risk Assessment Requirements	2.0 Basic 03/31/21\n106	Auditing	Analytical Procedures Used by Auditors 	3.0 Basic 03/31/21\n107	Auditing	Asset Misappropriation Schemes	1.5 Intermediate 03/31/21\n108	Auditing	Auditing Prepaid Expenses	1.5 Basic 03/31/21\n109	Auditing	Corruption, Part 1	1.5 Intermediate 03/31/21\n110	Auditing	Corruption, Part 2 	2.0 Intermediate 03/31/21\n111	Auditing	Healthcare Industry, Part 1—Overview and Update	2.5 Basic 03/31/21\n112	Auditing	Introduction to Audit Sampling	2.0 Basic 03/31/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n113	Auditing	Managing Professional Liability Risk in Nonattest Services 	3.0 Basic 03/31/21\n114	Auditing	Accountants\' Responsibilities Regarding Fraud, Part 2	2.0 Basic 04/30/21\n115	Auditing	Audit Sampling, Part 1: Introduction to Basic Sampling Concepts and Terms 2.5 Basic 04/30/21\n116	Auditing\nAudit Sampling, Part 2: Attribute Sampling for Tests of Controls and Selecting a \nRepresentative Sample\n2.0 Basic 04/30/21\n117	Auditing	Fraud in Not-for-Profits	1.5 Basic 04/30/21\n118	Auditing	Navigating System and Organization Control (SOC) Reports 	2.0 Basic 04/30/21\n119	Auditing	Professional Skepticism for Public Accountants	3.5 Basic 04/30/21\n120	Auditing	Required Auditor Communications	3.5 Basic 04/30/21\n121	Auditing	Upcoming Peer Review, Part 1	1.5 Basic 04/30/21\n122	Auditing	Audit Sampling, Part 3: Substantive Audit Sampling – An Introduction 2.5 Basic 05/31/21\n123	Auditing	Auditing Accounts Receivable	2.0 Basic 05/31/21\n124	Auditing	Auditing Fair Value	2.0 Intermediate 05/31/21\n125	Auditing	Comparing the Audit Standards – AICPA vs. PCAOB vs. International Standards 3.0 Basic 05/31/21\n126	Auditing	Fraud Investigation, Part 2	1.0 Basic 05/31/21\n127	Auditing	Going Concern Uncertainty	3.0 Basic 05/31/21\n128	Auditing	Healthcare Industry, Part 2 - Understanding Health Care Financial Statements 2.0 Intermediate 05/31/21\n129	Auditing	The 2017 PCAOB Auditor\'s Report 	2.0 Basic 05/31/21\n130	Auditing	Upcoming Peer Review, Part 2	1.5 Basic 05/31/21\n131	Auditing	A Guide Through Common Audit Deficiencies	2.0 Update 06/30/21\n132	Auditing	Audit Risk Assessment: The Do\'s and Don\'ts, Part 1	2.0 Basic 06/30/21\n133	Auditing	Audit Risk Assessment: The Do\'s and Don\'ts, Part 2	2.0 Basic 06/30/21\n134	Auditing	Audit Sampling, Part 4: Classical Variables Sampling (CVS) Techniques 1.5 Basic 06/30/21\n135	Auditing	Fundamental IT Auditing Concepts	2.0 Basic 06/30/21\n136	Auditing	How to Properly Prepare Audit Documentation and Workpapers, Part 2	2.0 Basic 06/30/21\n137	Auditing	Professional Judgment in Audit	2.0 Basic 06/30/21\n138	Auditing	Reporting on Subject Matters Other Than Historical Financial Statements 3.5 Basic 06/30/21\n139	Auditing	Auditing Property, Plant & Equipment	2.0 Basic 07/31/21\n140	Auditing	Employee Benefits, Part 3—Defined Benefit Pension Plans	2.5 Intermediate 07/31/21\n141	Auditing	Financial Statement Fraud	2.5 Intermediate 07/31/21\n142	Auditing	Fraud Investigation, Part I 	1.0 Basic 07/31/21\n143	Auditing	Auditing Cash and Cash Equivalents 	2.0 Basic 08/31/21\n144	Auditing	Auditing Current Liabilities	2.0 Basic 08/31/21\n145	Auditing	Auditing Employee Benefit Plans – Part 1 	2.0 Basic 08/31/21\n146	Auditing	Auditing Long Term Liabilities 	2.0 Basic 08/31/21\n147	Auditing	Compilations and Reviews – Avoiding Peer Review Deficiencies (Updated for SSARS 24)3.0 Basic 08/31/21\n148	Auditing	Fraud Investigation, Part 3	1.5 Basic 08/31/21\n149	Auditing	Group Audit (AU-C 600)	1.5 Basic 08/31/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n150	Auditing	How to Assess Internal Controls and Safeguard Assets	2.0 Basic 08/31/21\n151	Auditing	Preparing Compilations and Reviews: 2018-2019, Part 1	1.5 Basic 08/31/21\n152	Auditing	Preparing Compilations and Reviews: 2018-2019, Part 2	2.0 Basic 08/31/21\n153	Auditing	Required Communications in an Audit	2.0 Basic 08/31/21\n154	Auditing	Audit Quality: How to Prevent Audit Failure	2.0 Update 09/30/21\n155	Auditing	Auditing Contingencies	2.0 Basic 09/30/21\n156	Auditing	Auditing Employee Benefit Plans – Part 2	2.0 Basic 09/30/21\n157	Auditing	How to Properly Prepare Audit Documentation and Workpapers, Part 1	2.0 Basic 09/30/21\n158	Auditing	How to Properly Review Audit Documentation and Workpapers, Part 1	2.0 Intermediate 09/30/21\n159	Auditing	How to Properly Review Audit Documentation and Workpapers, Part 2	2.0 Intermediate 09/30/21\n160	Auditing	The Impact of Sarbanes-Oxley on Internal Controls	2.0 Basic 09/30/21\n161	Auditing	Use of Specialists and Auditing Estimates	2.0 Basic 09/30/21\n162	Auditing	AICPA Documentation Requirements	2.0 Basic 10/31/21\n163	Auditing	Use of Internal Auditors and Initial Audits	2.0 Basic 10/31/21\n164	Auditing	Fraud in the Construction Industry, Part 1	1.0 Intermediate 11/30/21\n165	Auditing	Fraud in the Construction Industry, Part 2	1.0 Intermediate 11/30/21\n166	Auditing	Fraud in the Construction Industry, Part 3	1.0 Intermediate 11/30/21\n167	Auditing	Fraud in the Construction Industry, Part 4	1.0 Intermediate 11/30/21\n168	Auditing	Leveraging Internal Control Frameworks for Success	2.0 Basic 11/30/21\n169	Auditing	The Life Cycle of the Internal Audit	1.0 Basic 11/30/21\n170	Auditing	The New Employee Benefit Plan (EBP) Auditor’s Report	2.0 Basic 11/30/21\n171	Auditing	Updated COSO Internal Control Framework	2.0 Basic 11/30/21\n172 Auditing (Governmental) Federal Government Contracting—Unallowable Costs	2.5 Intermediate 03/31/21\n173 Auditing (Governmental) Government/Fund Accounting: Introduction to Government Accounting, Financial Reporting an 4.0 Basic 03/31/21\n174 Auditing (Governmental) Yellow Book 2018 Revision	2.0 Update 03/31/21\n175 Auditing (Governmental) Yellow Book Performance Audits (Updated for Government Auditing Standards 2018 Revision)3.5 Intermediate 03/31/21\n176 Auditing (Governmental) 2019 Single Audit Update	2.0 Update 05/31/21\n177 Auditing (Governmental) Evaluating the Effectiveness of Not-for-Profit Entities	2.0 Intermediate 05/31/21\n178 Auditing (Governmental) Not-for-Profit Accounting and Reporting: An Introduction	3.0 Basic 05/31/21\n179 Auditing (Governmental) Risk Management in the Public Sector	2.5 Intermediate 05/31/21\n180 Auditing (Governmental) Housing and Urban Development: An Auditing Introduction	3.0 Basic 06/30/21\n181 Auditing (Governmental) Internal Control Considerations—Focus on Non-profits and Governmental Entities3.0 Basic 07/31/21\n182 Auditing (Governmental) Federal Government Contracting—Contract Fraud	1.5 Intermediate 08/31/21\n183 Auditing (Governmental) GAO Green Book - Government Internal Control Standards	3.0 Basic 09/30/21\n184 Auditing (Governmental) Introduction to Federal Accounting and Reporting	3.5 Basic 09/30/21\n185 Auditing (Governmental)\nYellow Book Financial Audits and Attest Engagements (Updated for Government Auditing \nStandards 2018 Revision)\n3.0 Intermediate 09/30/21\n186 Auditing (Governmental) Single Audit Quality – Focus on Designing and Performing Tests of Control and Compliance 3.0 Basic 10/31/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n187 Auditing (Governmental) Single Audit Quality – Focus on Risk Assessment, Evaluating Results, and Reporting 3.0 Basic 10/31/21\n188 Auditing (Governmental) Yellow Book Independence & Quality Control	2.0 Intermediate 10/31/21\n189 Auditing (Governmental) Common audit deficiencies: Governmental audits	2.0 Update 12/31/21\n190 Auditing (Governmental) Not-for-profit accounting and reporting: an introduction	3.0 Basic 02/28/22\n191	Behavioral Ethics Ethics—Integrity, A Foundation for Success	1.5 Basic 03/31/21\n192	Behavioral Ethics Expectations of Corporate Governance and Social Responsibility in Today\'s World1.5 Basic 03/31/21\n193	Behavioral Ethics Maintaining Professional Ethics in a Culture of Oversharing	1.0 Basic 03/31/21\n194	Behavioral Ethics Leadership and Balance in Turbulent Times	2.0 Basic/Intermediate05/31/21\n195	Behavioral Ethics Strengthening Your Workforce: Diversity and Inclusion	1.5 Basic 07/31/21\n196	Business Law Contract Law for Accountants	2.5 Basic 08/31/21\n197 Business Management and OrganizationClient Management	1.5 Basic 06/30/21\n198 Business Management and OrganizationSeeking Millennials and Generation Z	1.5 Basic 06/30/21\n199 Business Management and OrganizationNew Leader Assimilation	1.0 Basic 09/30/21\n200 Business Management and OrganizationAccounting Firm’s System of Quality Control, Part 2 	1.0 Basic 10/31/21\n201 Business Management and OrganizationDelegation in a small firm	1.5 Intermediate 01/31/22\n202 Communications & Marketing Data Visualization	1.0 Basic 03/31/21\n203 Communications & Marketing Executive Leadership Tools and Tactics, Part 1	2.0 Basic 03/31/21\n204 Communications & Marketing Executive Leadership Tools and Tactics, Part 2	1.5 Basic 03/31/21\n205 Communications & Marketing Leadership – Others	2.5 Basic 04/30/21\n206 Communications & Marketing Developing Client Rapport: Making Everyone a Match!	2.0 Basic 05/31/21\n207 Communications & Marketing Networking Skills	1.5 Basic 06/30/21\n208 Communications & Marketing Coaching Employees for Better Results	1.0 Basic 07/31/21\n209 Communications & Marketing Collaborating to Build Your Personal and Professional Networks	1.0 Basic 07/31/21\n210 Communications & Marketing Creating Effective Presentations	2.0 Basic 07/31/21\n211 Communications & Marketing Effective Business Writing for CPAs, Part 1	2.0 Basic 07/31/21\n212 Communications & Marketing Role of the Leader in Talent Development	1.5 Basic 07/31/21\n213 Communications & Marketing The Art of High-Impact Conversations	1.5 Basic 07/31/21\n214 Communications & Marketing Developing ideal working relationships	1.0 Basic 08/31/21\n215 Communications & Marketing Leadership skills	2.0 Basic 08/31/21\n216 Communications & Marketing Leveraging Social Media for Firm Success 	1.0 Basic 09/30/21\n217 Communications & Marketing The Ultimate Endorsement: Unleash the Power of You	2.0 Basic 09/30/21\n218 Communications & Marketing Executive Presence	2.0 Basic 10/31/21\n219 Communications & Marketing Leadership-Self	2.0 Basic 10/31/21\n220 Communications & Marketing Creating effective presentations 	1.0 Basic 11/30/21\n221 Communications & Marketing Public Speaking and Presentation Skills	1.5 Basic 11/30/21\n222 Communications & Marketing Developing a growth mindset	1.0 Basic 02/28/22\n223 Computer Software & ApplicationsArtificial intelligence and machine learning—What’s the buzz?	1.0 Basic 02/28/22\n224	Economics	Macroeconomic Analysis – Bringing the Big Picture into Focus, Part 1 2.0 Intermediate 03/31/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n225	Economics	Macroeconomic Analysis – Bringing the Big Picture into Focus, Part 2 2.0 Intermediate 03/31/21\n226	Economics	Microeconomic Analysis – Understanding the Effect on Supply and Demand, Part 12.0 Intermediate 03/31/21\n227	Finance	Capital Budgeting: The Tools to Enhance Shareholder Value Part 1	2.5 Basic 03/31/21\n228	Finance	Capital Budgeting: The Tools to Enhance Shareholder Value Part 2	3.0 Basic 03/31/21\n229	Finance	Identity Theft Prevention	1.0 Basic 03/31/21\n230	Finance	Personal Financial Planning	2.0 Basic 03/31/21\n231	Finance	Financial Therapy and the CPA	2.0 Basic 05/31/21\n232	Finance	Coronavirus (COVID-19): What should a controller do?	1.5 Intermediate 07/31/21\n233	Finance	Financial Statement Analysis, Part 1	2.0 Basic 08/31/21\n234	Finance	Financial Statement Analysis, Part 2	2.5 Basic 08/31/21\n235 Information Technology Big Data, Part 1	1.5 Basic 03/30/21\n236 Information Technology Identity Theft Today	1.0 Basic 03/31/21\n237 Information Technology Introduction to IT Security	3.5 Basic 03/31/21\n238 Information Technology California Consumer Privacy Act and what it means for accountants	2.0 Basic 04/30/21\n239 Information Technology Identity Theft: How to Detect, Prevent, and Recover in the Digital Age 2.0 Basic 04/30/21\n240 Information Technology Understanding Blockchain Technology	1.5 Basic 05/31/21\n241 Information Technology Cybersecurity Preparedness for Industry CPAs	3.0 Basic 08/31/21\n242 Information Technology Big Data, Part 2	1.5 Intermediate 09/30/21\n243 Information Technology Global telework - The future of work is here	1.0 Basic 09/30/21\n244 Information Technology Global telework - The future of work is here	1.0 Basic 09/30/21\n245 Information Technology Going digital, staying human: Retooling your digital dexterity for success 1.0 Intermediate 11/30/21\n246 Management Services Developing Business in the Professional Services Industry	2.0 Basic 03/31/21\n247 Management Services Management Keys to Success: Culture and Leadership 	2.0 Intermediate 03/31/21\n248 Management Services Management Keys to Success: Hiring the Best Personalities	1.0 Intermediate 03/31/21\n249 Management Services Enterprise Risk Management - Integrating with Strategy & Performance, Part 1 2.0 Basic 04/30/21\n250 Management Services Management Keys to Success, Leadership, Continued Improvement and Self-Management2.5 Basic 04/30/21\n251 Management Services Considering an ESOP	2.0 Basic 05/31/21\n252 Management Services Enterprise Risk Management—Integrating with Strategy & Performance, Part 2 2.0 Intermediate 05/31/21\n253 Management Services\nPractical Project Management Series #1: Overview of Practical Project Management (for Any \nEffort)\n1.5 Basic 05/31/21\n254 Management Services\nPractical Project Management Series #2: Kicking Off a New Project Right with Strong Goals \nand Support \n1.5 Basic 07/31/21\n255 Management Services\nPractical Project Management Series #3: Building a Strong Team and Communicating \nThroughout\n1.5 Basic 07/31/21\n256 Management Services Change Management	1.0 Basic 08/31/21\n257 Management Services Emotional Intelligence	3.0 Intermediate 08/31/21\n258 Management Services Strategic Management: The Tools to Compete in the Era of Rapid Change, Part 13.0 Basic 09/30/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n259 Management Services Strategic Management: The Tools to Compete in the Era of Rapid Change, Part 23.0 Basic 09/30/21\n260 Management Services Techniques for Project Scope and Time Management	2.0 Basic 09/30/21\n261 Management Services Leading Through Emotional Intelligence	2.0 Intermediate 11/30/21\n262 Personal Development Becoming a Highly Effective Performer! Making the Best of YOUR Time! 1.5 Basic 03/31/21\n263 Personal Development Being a Trusted Advisor	2.0 Basic 03/31/21\n264 Personal Development Managing Your Career Using the 70-20-10 Rule	1.5 Basic 03/31/21\n265 Personal Development Serving on a Board: What a CPA Needs to Know	2.0 Basic 03/31/21\n266 Personal Development Transitioning from Buddy to Boss	1.5 Basic 04/30/21\n267 Personal Development\nMOTIVATORS: Understanding What Drives Your Behaviors, and What Drives Others’ \nBehaviors\n1.0 Basic 05/31/21\n268 Personal Development Careers in accounting	2.0 Basic 06/30/21\n269 Personal Development Finding a Better Work-Life Balance	2.5 Basic 06/30/21\n270 Personal Development Getting “UP!” Supercharging Your Energy	2.0 Basic 06/30/21\n271 Personal Development How To Support And Engage Your Team During A Crisis	2.0 Basic 06/30/21\n272 Personal Development Power of Positivity	2.0 Basic 06/30/21\n273 Personal Development AICPA and the Future of the Profession	2.0 Basic 07/31/21\n274 Personal Development Behavioral Interviewing	1.5 Basic 07/31/21\n275 Personal Development It\'s worse than you thought: A leadership mindset to flourish in tough times 2.0 Basic 07/31/21\n276 Personal Development No easy choices: Decision-making under stress	2.0 Basic 07/31/21\n277 Personal Development Problem Solving	1.0 Basic 07/31/21\n278 Personal Development Time Management for Professionals	2.0 Basic 08/31/21\n279 Personal Development The millennial leaders	1.0 Basic 09/30/21\n280 Personal Development The millennial leaders	1.0 Basic 09/30/21\n281 Personal Development Managing with courage	1.0 Intermediate 12/31/21\n282 Personal Development 10 Habits of highly successful careers	2.0 Basic 01/31/22\n283 Personnel/Human Resources Performance Management Essentials	1.0 Basic 03/31/21\n284 Personnel/Human Resources Talent Management and Succession Planning	1.5 Intermediate 04/30/21\n285 Personnel/Human Resources Training and Developing Employees Using a Competency Framework	1.5 Basic 04/30/21\n286 Personnel/Human Resources Lessons learned working from home	1.0 Basic 05/31/21\n287 Personnel/Human Resources The power of using compelling questions	1.0 Basic 06/30/21\n288 Personnel/Human Resources Establishing a Successful Mentoring Program	2.0 Basic 07/31/21\n289 Personnel/Human Resources Launching a Winning Team	1.0 Basic 07/31/21\n290 Personnel/Human Resources Motivating remote workers	1.0 Basic 07/31/21\n291 Personnel/Human Resources Stimulating innovative thinking in your team 	1.0 Basic 07/31/21\n292 Personnel/Human Resources Myths and realities of working from home	1.0 Basic/Intermediate08/31/21\n293 Personnel/Human Resources Leading vs. managing: What to do when and with whom	1.0 Basic 09/30/21\n294 Personnel/Human Resources Leading vs. managing: What to do when and with whom	1.0 Basic 09/30/21\n295 Personnel/Human Resources Fostering a Culture of Ownership 	1.0 Basic 11/30/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n296 Personnel/Human Resources Fostering a Culture of Premier Client Experience (CX)	2.0 Basic 11/30/21\n297	Regulatory Ethics AICPA\'s Ethics Codification Project	2.0 Basic 03/31/21\n298	Regulatory Ethics Alaska Ethics	4.0 Basic 03/31/21\n299	Regulatory Ethics Arizona Ethics	4.0 Basic 03/31/21\n300	Regulatory Ethics Colorado Ethics	4.0 Basic 03/31/21\n301	Regulatory Ethics Kansas Ethics	2.0 Basic 03/31/21\n302	Regulatory Ethics Kentucky Ethics	2.0 Basic 03/31/21\n303	Regulatory Ethics Mississippi Ethics 	3.5 Basic 03/31/21\n304	Regulatory Ethics Montana Ethics	2.0 Basic 03/31/21\n305	Regulatory Ethics North Carolina Ethics	2.0 Basic 03/31/21\n306	Regulatory Ethics Colorado Rules and Regulations	2.0 Basic 05/31/21\n307	Regulatory Ethics Delaware Ethics	4.0 Basic 05/31/21\n308	Regulatory Ethics Ethical Foundations	1.0 Basic 05/31/21\n309	Regulatory Ethics Independence – Historical Insights and Today’s Rules	3.0 Basic 05/31/21\n310	Regulatory Ethics Independence Overview	1.5 Update 05/31/21\n311	Regulatory Ethics California Ethics	4.0 Basic 06/30/21\n312	Regulatory Ethics Connecticut Ethics	4.0 Basic 06/30/21\n313	Regulatory Ethics D.C. Ethics	4.0 Basic 06/30/21\n314	Regulatory Ethics Ethics, Integrity and the AICPA Code of Professional Conduct (expanded version)4.0 Basic 06/30/21\n315	Regulatory Ethics Ethics, Integrity, and the AICPA Code of Professional Conduct	3.0 Basic 06/30/21\n316	Regulatory Ethics Illinois Ethics	4.0 Basic 06/30/21\n317	Regulatory Ethics Indiana Ethics	4.0 Basic 06/30/21\n318	Regulatory Ethics Iowa Ethics	4.0 Basic 06/30/21\n319	Regulatory Ethics Maine Ethics	3.0 Basic 06/30/21\n320	Regulatory Ethics Maryland Ethics	4.0 Basic 06/30/21\n321	Regulatory Ethics Massachusetts Ethics	4.0 Basic 06/30/21\n322	Regulatory Ethics Michigan Ethics	3.0 Basic 06/30/21\n323	Regulatory Ethics Minnesota Ethics	4.0 Basic 06/30/21\n324	Regulatory Ethics Missouri Ethics	3.0 Basic 06/30/21\n325	Regulatory Ethics Nebraska Ethics	4.0 Basic 06/30/21\n326	Regulatory Ethics New Hampshire Ethics	4.0 Basic 06/30/21\n327	Regulatory Ethics New York Ethics	4.0 Basic 06/30/21\n328	Regulatory Ethics North Dakota Ethics	4.0 Basic 06/30/21\n329	Regulatory Ethics Ohio Ethics	3.0 Basic 06/30/21\n330	Regulatory Ethics Oklahoma Ethics	4.0 Basic 06/30/21\n331	Regulatory Ethics Pennsylvania Ethics	4.0 Basic 06/30/21\n332	Regulatory Ethics Puerto Rico Ethics	3.0 Basic 06/30/21\n333	Regulatory Ethics Utah Ethics	3.0 Basic 06/30/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n334	Regulatory Ethics Vermont Ethics	4.0 Basic 06/30/21\n335	Regulatory Ethics Virginia Ethics 2020	2.0 Basic 06/30/21\n336	Regulatory Ethics Wisconsin Ethics	3.0 Basic 06/30/21\n337	Regulatory Ethics Government Ethics and Independence	2.0 Intermediate 07/31/21\n338	Regulatory Ethics The Role of the Whistleblower	1.0 Basic 07/31/21\n339	Regulatory Ethics Creating a Culture of Integrity	2.0 Basic 08/31/21\n340	Regulatory Ethics Independence Technical Update	1.0 Update 08/31/21\n341	Regulatory Ethics Tennessee Ethics	2.0 Basic 08/31/21\n342	Regulatory Ethics OH-PSR Ethics Course	3.0 Basic/Intermediate09/30/21\n343	Regulatory Ethics OH-PSR Ethics Course	3.0 Basic/Intermediate09/30/21\n344	Regulatory Ethics Wyoming Ethics 	4.0 Basic/Intermediate09/30/21\n345	Regulatory Ethics Wyoming Ethics 	4.0 Basic/Intermediate09/30/21\n346	Regulatory Ethics Independence Update – AICPA, GAGAS & PCAOB	2.0 Update 12/31/21\n347 Specialized Knowledge Business Valuation Discounts and Premiums	4.0 Basic 03/31/21\n348 Specialized Knowledge Business Owners’ Response to Coronavirus (COVID-19)	2.0 Basic 04/30/21\n349 Specialized Knowledge Canada\'s COVID-19 Economic Response Plan	1.0 Basic 04/30/21\n350 Specialized Knowledge A Sampling of Industries and Sectors, Part 1	2.5 Basic 05/31/21\n351 Specialized Knowledge A Sampling of Industries and Sectors, Part 2	2.0 Basic 05/31/21\n352 Specialized Knowledge Business Valuation: Valuing Intangibles: Part 1	2.0 Basic 05/31/21\n353 Specialized Knowledge Business Valuation: Valuing Intangibles: Part 2	1.5 Basic 05/31/21\n354 Specialized Knowledge Business Development	1.5 Basic 06/30/21\n355 Specialized Knowledge Overview of the Business Valuation Profession and Current Events	3.0 Basic 07/31/21\n356 Specialized Knowledge Business Valuation: Introduction to Fair Value Measurement: Part 1	1.5 Basic 10/31/21\n357 Specialized Knowledge Business Valuation: Introduction to Fair Value Measurement: Part 2	2.0 Basic 10/31/21\n358 Specialized Knowledge Leadership: Based on the Book \"LEAD… for God\'s Sake!\"	1.5 Basic 10/31/21\n359 Specialized Knowledge Understanding the Fundamentals of Investments – A Guide for Accountants 3.0 Basic 11/30/21\n360	Taxes	Estate and gift taxes, part 1	1.0 Intermediate 03/31/21\n361	Taxes	Estate and gift taxes, part 2 	1.0 Intermediate 03/31/21\n362	Taxes	Ethics Checklist for Tax Practitioners	1.0 Basic 03/31/21\n363	Taxes\nFederal Tax Accounting, Part 4—Transactional Issues and Net Operating Loss Concepts and \nComputations (Updated for TCJA 2017)\n1.5 Intermediate 03/31/21\n364	Taxes	Federal tax implications for the trucking industry	1.5 Basic 03/31/21\n365	Taxes	Foreign Currency Transactions	2.0 Intermediate 03/31/21\n366	Taxes	Individual Taxes - Income & Deductions (2019 Returns)	3.0 Basic 03/31/21\n367	Taxes	Internal Revenue Service Practices and Procedures – Gain a Fundamental Edge, Part 12.0 Basic 03/31/21\n368	Taxes	Internal Revenue Service Practices and Procedures – Gain a Fundamental Edge, Part 22.0 Basic 03/31/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n369	Taxes	Partnerships and Multiple-Member LLCs: Taxation and Other Considerations 2.0 Intermediate 03/31/21\n370	Taxes\nPartnerships: Understanding the New Audit Rules and the Implications of the Repeal of the \nTechnical Termination Rules \n2.0 Basic 03/31/21\n371	Taxes	Planning for Foreign Operations	3.0 Basic 03/31/21\n372	Taxes	Preparing New York Income Tax Returns for Businesses	2.0 Basic 03/31/21\n373	Taxes	Real Estate Taxation, Part 2: Involuntary Conversions, Character of Dispositions, and Rentals1.0 Intermediate 03/31/21\n374	Taxes	State Tax Update Q4 2019	2.0 Basic 03/31/21\n375	Taxes	Tax Compliance and Planning for Gig Workers	3.0 Basic 03/31/21\n376	Taxes	Tax Consequences of Changing Business Entities	1.5 Basic 03/31/21\n377	Taxes	Tax Implications for Charities and Charitable Contributions	2.0 Basic 03/31/21\n378	Taxes	The New International Provisions (TCJA Changes)	3.0 Basic 03/31/21\n379	Taxes	Understanding the New Excess Business Loss Limitation	2.0 Basic 03/31/21\n380	Taxes	2018 Reviewing Tax Returns for Accuracy and Efficiency	1.5 Basic 04/30/21\n381	Taxes	Individual Alternative Minimum Tax: What Every Practitioner Needs to Know 2.0 Basic 04/30/21\n382	Taxes	Individual taxes— itemized deductions (2019 returns) 	3.0 Basic 04/30/21\n383	Taxes	Organizing, Operating, and Closing a Partnership	2.0 Basic 04/30/21\n384	Taxes	Quarterly Estimates for Corporations	1.5 Basic 04/30/21\n385	Taxes	Tackling Tax Depreciation Issues	2.0 Basic 04/30/21\n386	Taxes	Tax implications for members of the clergy and religious workers	2.0 Basic 04/30/21\n387	Taxes	Tax implications for the oil and gas industry	1.5 Basic 04/30/21\n388	Taxes	Taxation implications for controlled foreign corporations	2.0 Advanced 04/30/21\n389	Taxes\nThe Building Blocks of Taxation: Interest and Debt Investments (Updated for Tax Cuts and \nJobs Act 2017)\n2.0 Update 04/30/21\n390	Taxes	Understanding federal excise taxes	1.5 Basic 04/30/21\n391	Taxes	Understanding tax-favored health plans	1.0 Basic 04/30/21\n392	Taxes\nCorporate Distributions: The Tax Treatment of Dividends, Redemptions and Liquidations \n(Updated for Tax Cuts and Jobs Act 2017)\n1.5 Intermediate 05/31/21\n393	Taxes	Federal Tax Accounting, Part 3 – Inventory (Updated for Tax Cuts and Jobs Act 2017)1.5 Intermediate 05/31/21\n394	Taxes	Fundamentals of Equity-Based Compensation	2.0 Basic 05/31/21\n395	Taxes	How the CARES Act impacts retirement planning	1.0 Basic 05/31/21\n396	Taxes	Key Tips on Reading Brokerage Statements	2.0 Basic 05/31/21\n397	Taxes	Maximizing Higher Education Tax Credits	2.0 Basic 05/31/21\n398	Taxes	Real Estate Taxation, Part 1: Like-Kind Exchanges (Updated for Tax Cuts and Jobs Act 2017)1.5 Intermediate 05/31/21\n399	Taxes	Understanding Like-Kind Exchanges	2.0 Basic 05/31/21\n400	Taxes	Understanding the Foreign Tax Credit Rules and Calculations	3.0 Basic 05/31/21\n401	Taxes	Capital Assets and Preparing Schedule D	1.5 Basic 06/30/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n402	Taxes	Corporate Earnings and Profits:  An Overview 	1.0 Advanced 06/30/21\n403	Taxes	Debt and Capital Basis for S-Corporations	2.0 Basic 06/30/21\n404	Taxes	Earned income tax credit – Rules and common pitfalls	1.0 Basic 06/30/21\n405	Taxes	Expatriation-Individuals and Corporations	1.5 Basic 06/30/21\n406	Taxes	Fundamentals of International Tax	4.5 Intermediate 06/30/21\n407	Taxes	Fundamentals of Sales and Use Tax in the United States	3.0 Basic 06/30/21\n408	Taxes	IRC Sections - 465 and 469	2.0 Basic 06/30/21\n409	Taxes	Non-Qualified Deferred Compensation: The Impact of Section 409A	1.5 Intermediate 06/30/21\n410	Taxes\nOrganization for Economic Co-operation and Development (OECD) Base Erosion and Profit \nShifting (BEPS) Initiative\n3.0 Basic 06/30/21\n411	Taxes	Outbound Transactions	3.0 Intermediate 06/30/21\n412	Taxes	Repair Regulations	2.5 Intermediate 06/30/21\n413	Taxes	Section 754 Elections: Tax Implications of Partnership Step-ups	2.0 Advanced 06/30/21\n414	Taxes	Special Valuation Rules, Part 1: See Clearly Through the Clutter	1.5 Intermediate 06/30/21\n415	Taxes	Tax Implications for Farming: Farm Income and Expenses	3.0 Basic 06/30/21\n416	Taxes	Tax Issues of Foreign Professionals Entering the U.S. Workforce	1.0 Basic 06/30/21\n417	Taxes	The At Risk and Passive Activity Loss Rules – How they Impact You	2.0 Intermediate 06/30/21\n418	Taxes	Transfer of an Interest in Trust: Section 2702	1.5 Intermediate 06/30/21\n419	Taxes	Understanding the \"Other\" Taxes: Consumption, Retail, Value Added, and Destination-Based2.5 Advanced 06/30/21\n420	Taxes	An introduction to the R&D tax credit	2.0 Basic 07/31/21\n421	Taxes	Compliance Issues of Sales and Use Tax in the United States – Part 2 2.5 Basic 07/31/21\n422	Taxes	Coronavirus (COVID-19): Maximizing the use of net operating losses	1.5 Intermediate 07/31/21\n423	Taxes	Corporate Taxation and Preparation Strategies, Part 1	1.5 Basic 07/31/21\n424	Taxes	Corporate Taxation and Preparation Strategies, Part 2	2.0 Intermediate 07/31/21\n425	Taxes	Dissecting the Internal Revenue Code	1.5 Basic 07/31/21\n426	Taxes	Estate and gift taxes, part 3	2.0 Intermediate 07/31/21\n427	Taxes	Filing Form 709 – How to report federal gift tax	1.5 Basic 07/31/21\n428	Taxes	Fundamentals of state and local taxation	2.0 Basic 07/31/21\n429	Taxes	How to Understand Subchapter K	2.0 Basic 07/31/21\n430	Taxes	Partnerships: Legal and Taxation Considerations (Updated for Tax Cuts and Jobs Act 2017)2.0 Intermediate 07/31/21\n431	Taxes	Preparing Texas Franchise Tax Returns for Businesses	1.5 Basic 07/31/21\n432	Taxes	Social Security and Medicare Tax Issues	2.0 Basic 07/31/21\n433	Taxes	Working in the Legal Marijuana Industry - Taxation Updates and Other Tips 1.0 Basic 07/31/21\n434	Taxes	Annual Tax Update for Individuals	4.0 Intermediate 08/31/21\n435	Taxes	Filing Form 706 – How to report federal estate tax	2.0 Basic 08/31/21\n436	Taxes	Form 1040 Walkthrough	1.5 Basic 08/31/21\n\nUPDATED: 03/23/21\nCourse # Field of Study	Title	CPE Credit Course LevelExpiration Date\nBecker Select - Courses\n437	Taxes	Fundamentals of Consolidated Returns	2.0 Intermediate 08/31/21\n438	Taxes	Preparing for a Tax Audit	1.5 Basic 08/31/21\n439	Taxes	Preparing Passthrough Entity Returns Under the New Law	2.0 Basic 08/31/21\n440	Taxes	Reform and Development of Sales and Use Tax in the United States – Part 1 2.5 Basic 08/31/21\n441	Taxes	Reform and Development of Sales and Use Tax in the United States – Part 2 3.0 Basic 08/31/21\n442	Taxes	Tax issues for churches and religious organizations	1.5 Basic 08/31/21\n443	Taxes	Tax Reform and Multinational Entities	3.0 Intermediate 08/31/21\n444	Taxes	Tax Research Basics	1.5 Basic 08/31/21\n445	Taxes	Corporate Taxation Strategies: Fundamentals of Reorganizations	2.0 Intermediate 09/30/21\n446	Taxes	Federal Tax Accounting, Part 2 – Deferred Payment Sales	1.5 Intermediate 09/30/21\n447	Taxes	Overview of the Federal Tax System	3.5 Basic 09/30/21\n448	Taxes	Section 382:  An Overview	1.5 Advanced 09/30/21\n449	Taxes	Tax Fraud Due to Identity Theft	1.5 Basic 09/30/21\n450	Taxes	Your go-to guide to S Corporations	2.0 Intermediate 09/30/21\n451	Taxes	Your go-to guide to S Corporations	2.0 Intermediate 09/30/21\n452	Taxes	Quarterly Estimates for Individual	2.0 Basic 10/31/21\n453	Taxes	Close Examination of Personal Casualty Losses After the Tax Cuts and Jobs Act2.0 Basic 11/30/21\n454	Taxes	Federal tax accounting, part 4 - Transactional issues and net operating loss 1.5 Intermediate 11/30/21\n455	Taxes	Global Intangible Low-Taxed Income (GILTI) Rules	2.0 Basic 11/30/21\n456	Taxes	Maximizing Higher Education Tax Credits 	1.5 Basic/Intermediate11/30/21\n457	Taxes	Navigating M&A Transaction Costs From a Tax Perspective	2.0 Intermediate 11/30/21\n458	Taxes	Reporting Book-Tax Differences—Understanding Schedules M-1 and M-3	1.0 Basic 11/30/21\n459	Taxes	The Taxation of Corporate Liquidations	1.5 Intermediate 11/30/21\n460	Taxes	The Transition Tax-Section 965 and Final Treasury Regulations	2.0 Intermediate 11/30/21\n461	Taxes Tax Treaties 	2.0 Intermediate 12/31/21\n462	Taxes	S corporations from A to Z—Getting ready to prepare the S corp return 2.0 Basic 12/31/21\n463	Taxes	Advanced Individual Tax - Schedule C	2.5 Intermediate 12/31/21\n464	Taxes	Base Erosion and Anti-Abuse Tax	2.0 Intermediate 12/31/21\n465	Taxes	Filing Form 990: Implications for higher education institutions	2.0 Intermediate 01/31/22\n466	Taxes	2021 Tax Update for Individuals	3.5 Intermediate 02/28/22\n467	Taxes	A focus on special topics for S corporations - revocation, termination, & reorganization2.0 Intermediate 02/28/22\n468	Taxes	Preparing Form 1040 for Tax Year 2020	3.5 Basic 02/28/22', 271, 1, '2025-06-04 10:18:46', '2025-06-04 10:18:46');
INSERT INTO `documents` (`id`, `title`, `filename`, `file_hash`, `filepath`, `content`, `size`, `category_id`, `created_at`, `updated_at`) VALUES
(70, 'مستند تجريبي', 'Chapter 4 (1).pdf', NULL, 'documents/ZnVw7AdhskDSvqonKidnXCj17vujbYjhwtMNfVrm.pdf', 'Data Mining\nClassifications II\nProf. Alaa El-Halees\n1\nChapter  4\n\nOutline\n2\nDecision Trees\nRandom Forest\nRule Based classifier\n\nDecision Trees: Definition\n3\nDecisiontreelearningisacommonmethodusedindata\nmining.\nADecisionTreeisatree-structuredplanofasetof\nattributestotestinordertopredicttheoutput.\nItisatypeoftree-diagramusedindeterminingthe\noptimumcourseofaction,insituationshavingseveral\npossiblealternativeswithuncertainoutcomes.\n\n4\nDecision Trees: Definition\n\nDecision Trees: Definition\n5\nDecision tree consist of:\nAninternalnodeisatestonanattribute,e.g.Body\ntemperature.\nAbranchrepresentsanoutcomeofthetest,e.g.,\nWarm\nAleafnoderepresentsaclasslabele.g.Mammals\nAteachnode,oneattributeischosentosplittraining\nexamplesintodistinctclassesasmuchaspossible\nAnewcaseisclassifiedbyfollowingamatchingpath\ntoaleafnode.\n\nExample\n6\n\nExample of a Decision TreeTidRefundMarital\nStatus\nTaxable\nIncomeCheat\n1Yes Single125K No\n2No Married100K No\n3No Single70K No\n4Yes Married120K No\n5No Divorced95K Yes\n6No Married60K No\n7Yes Divorced220K No\n8No Single85K Yes\n9No Married75K No\n10No Single90K Yes\n10 \nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarriedSingle, Divorced\n< 80K > 80K\nSplitting Attributes\nTraining Data	Model:  Decision Tree\n\nAnother Example of Decision TreeTidRefundMarital\nStatus\nTaxable\nIncomeCheat\n1Yes Single125K No\n2No Married100K No\n3No Single70K No\n4Yes Married120K No\n5No Divorced95K Yes\n6No Married60K No\n7Yes Divorced220K No\n8No Single85K Yes\n9No Married75K No\n10No Single90K Yes\n10 \nMarSt\nRefund\nTaxInc\nYESNO\nNO\nNO\nYes\nNo\nMarried\nSingle, \nDivorced\n< 80K > 80K\nThere could be more than one tree that \nfits the same data!\n\nDecision Tree Classification TaskApply \nModel Induction\nDeductionLearn \nModel \nModelTid Attrib1 Attrib2 Attrib3 Class \n1 Yes Large 125K No \n2 No Medium 100K No \n3 No Small 70K No \n4 Yes Medium 120K No \n5 No Large 95K Yes \n6 No Medium 60K No \n7 Yes Large 220K No \n8 No Small 85K Yes \n9 No Medium 75K No \n10 No Small 90K Yes \n10 \n  Tid Attrib1 Attrib2 Attrib3 Class \n11 No Small 55K ? \n12 Yes Medium 80K ? \n13 Yes Large 110K ? \n14 No Small 95K ? \n15 No Large 67K ? \n10 \n  \nTest Set\nTree\nInduction\nalgorithm\nTraining Set \nDecision \nTree\n\nApply Model to Test Data\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarriedSingle, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\nStart from the root of tree.\n\nApply Model to Test Data\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarriedSingle, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\n\nApply Model to Test Data\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarriedSingle, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\n\nApply Model to Test Data\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarriedSingle, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\n\nApply Model to Test Data\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarried Single, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\n\nApply Model to Test Data\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarried Single, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\nAssign Cheat to “No”\n\nDecision Tree Classification TaskApply \nModel Induction\nDeductionLearn \nModel \nModelTid Attrib1 Attrib2 Attrib3 Class \n1 Yes Large 125K No \n2 No Medium 100K No \n3 No Small 70K No \n4 Yes Medium 120K No \n5 No Large 95K Yes \n6 No Medium 60K No \n7 Yes Large 220K No \n8 No Small 85K Yes \n9 No Medium 75K No \n10 No Small 90K Yes \n10 \n  Tid Attrib1 Attrib2 Attrib3 Class \n11 No Small 55K ? \n12 Yes Medium 80K ? \n13 Yes Large 110K ? \n14 No Small 95K ? \n15 No Large 67K ? \n10 \n  \nTest Set\nTree\nInduction\nalgorithm\nTraining Set \nDecision \nTree\n\nDecision Trees: Building Tree\n17\nThereisalargenumberofdecision-treeinduction\nalgorithmsdescribedprimarilyinthemachine-learning\nandapplied-statisticsliterature.Theyaresupervised\nlearningmethodsthatconstructdecisiontreesfromaset\nofinput-outputsamples.\nToBuildatree:\nTop-downtreeconstruction\nAtstart,alltrainingexamplesareattheroot.\nPartitiontheexamplesrecursivelybychoosingoneattributeeach\ntime.\n\nDecision Trees: Building Tree\n18\nTDIDT (Top-Down Induction of Decision Trees)\nGreedy Tree Growing\nRecursive Partitioning\nfind “best” attribute test to install at root\nsplit data on root test\nfind “best” attribute test to install at each new node\nsplit data on new test\nrepeat until:\nall nodes are pure\nall nodes contain fewer than k cases\ntree reaches predetermined max depth\nno more attributes to test\n\nDecision Trees: A criterion for \nattribute selection\n19\nWhich is the best attribute?\nThe one which will result in the smallest tree\nHeuristic: choose the attribute that produces the “purest” \nnodes\nPopular impurity criterion:information gain\nInformation gain increases with the average purity of the \nsubsets that an attribute produces\nStrategy: choose attribute that results in greatest \ninformation gain\n\n20\nSelect the attribute with the highest information gain\nScontains s\nirows of class C\nifor i = {1, …, m}\nInformation gain= \ninformation before split (Parent) –information after split \n(Child)\nDecision Trees: A criterion for attribute \nselection\n\n21\nInformationmeasures info required to classify any \narbitrary tuple\nInformation gained by branching on attribute A as \nInformations\ns\nlog\ns\ns\n),...,s,ssI(\ni\nm\ni\ni\nm21	2\n1\n\n\n )s,...,s(I\ns\ns...s\nE(A)	mjj\nv\nj\nmjj\n1\n1\n1\n\n\n\n \nDecision Trees: A criterion for attribute \nselection\n\n22\n\nTriangles and Squares: Example#	Shape\nColor Outline Dot\n1 green dashed no triange\n2 green dashed yes triange\n3 yellow dashed no square\n4 red dashed no square\n5 red solid no square\n6 red solid yes triange\n7 green solid no square\n8 green dashed no triange\n9 yellow solid yes square\n10 red solid no square\n11 green solid yes square\n12 yellow dashed yes square\n13 yellow solid no square\n14 red dashed yes triange\nAttribute\n\nTriangles and Squares\n.\n.\n.\n.\n.\n.\nData Set:\nA set of classified objects\n\nExample\n•5 triangles\n•9 squares\n•class probabilities\n•entropy\n.\n.\n.\n.\n.\n.s\ns\nlog\ns\ns\n),...,s,ssI(\ni\nm\ni\ni\nm21	2\n1\n\n\n\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nColor?\nred\nyellow\ngreen\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nColor?\nred\nyellow\ngreen)s,...,s(I\ns\ns...s\nE(A)	mjj\nv\nj\nmjj\n1\n1\n1\n\n\n\n\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nColor?\nred\nyellow\ngreen\nInformation gain=  (information before split) –(information after split)\n\nInformation Gain of The Attribute\nAttributes\nGain(Color) = 0.246\nGain(Outline) = 0.151\nGain(Dot) = 0.048\nAttribute with the highest gain is chosen COLOR\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nColor?\nred\nyellow\ngreen\nGain(Outline) = 0.971 –0 = 0.971 bits\nGain(Dot) = 0.971 –0.951 = 0.020 bits\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nColor?\nred\nyellow\ngreen\n.\n.\nOutline?\ndashed\nsolid\nGain(Outline) = 0.971 –0.951 = 0.020 bits\nGain(Dot) = 0.971 –0 = 0.971 bits\n\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\nColor?\nred\nyellow\ngreen\n.\n.\ndashed\nsolid\nDot?\nno\nyes\n.\n.\nOutline?\n.\n\nFind “best” attribute test to install at root\n33\nOutlookTemperatureHumidityWindyPlay?\nsunny hot high falseNo\nsunny hot high true No\novercasthot high falseYes\nrain mild high falseYes\nrain cool normal falseYes\nrain cool normal true No\novercastcool normal true Yes\nsunny mild high falseNo\nsunny cool normal falseYes\nrain mild normal falseYes\nsunny mild normal true Yes\novercastmild high true Yes\novercasthot normal falseYes\nrain mild high true No\nDecision Trees: Example\n\nDecision Trees: Example\n34\n\nDecision Trees: Example \n35\n“Outlook” = “Sunny”: (2 yes,3 no)\n“Outlook” = “Overcast”: (4 yes, 0 no)\n“Outlook” = “Rainy”: (3 yes,2 no)\nExpected information for attribute:bits 971.0)5/3log(5/3)5/2log(5/2I([2,3])	 bits 0)0log(0)1log(1I([4,0])	 bits 971.0)5/2log(5/2)5/3log(5/3I([3,2])	 \nNote: log(0) is not \ndefined, but we \nevaluate 0*log(0) as \nzero971.0)14/5(0)14/4(971.0)14/5([3,2])[4,0],E([3,2],	 bits 693.0\n\nDecision Trees: Example\n36\nInformation gain: \n(information before split) –(information after split)\nInformation gain for attributes from weather data:0.693-0.940[3,2])[4,0],E([2,3],-I([9,5]))Outlook\"gain(\"	 bits 247.0 bits 247.0)Outlook\"gain(\"  bits 029.0)e\"Temperaturgain(\"  bits 152.0)Humidity\"gain(\"  bits 048.0)Windy\"gain(\"  \nOutlook has the highest information gain, then it will be the root\n\nContinuing to split\n37bits 571.0)e\"Temperaturgain(\"  bits 971.0)Humidity\"gain(\"  bits 020.0)Windy\"gain(\"  \nDecision Trees: Example\n\nDecision Trees: Example\n38\nNote: not all leaves need to be pure; \nSplitting stops when data can’t be split any further\n\nComplicated Tree (Temperature as root)\nTemperature\nOutlook	Windy\ncold\nMild\nhot\nP\nsunny rainy\nN\nyes no\nP\novercast\nOutlook\nsunny rainy\nP\novercast\nWindy\nPN\nyesno\nWindy\nNP\nyesno\nHumidity\nP\nhigh normal\nWindy\nPN\nyesno\nHumidity\nP\nhigh normal\nOutlook\nN\nsunny rainy\nP\novercast\nnull\n\n40\nRepresent the knowledge in the form of IF-THEN\nrules\nOne rule is created for each path from the root to a \nleaf\nEach attribute-value pair along a path forms a \nconjunction\nThe leaf node holds the class prediction\nRules are easier for humans to understand\nDecision Trees: Extracting Classification Rules\n\nOutput: A Decision Tree for “buys_computer”\n41\nage?\novercast\nstudent?	credit rating?\nno yes	fairexcellent\n<=30\n>40\nno	noyes	yes\nyes\n30..40\nDecision Trees\n\nDecision Trees: Extracting Classification Rules\n42\nExample\nIF age= “<=30” AND student= “no”   THEN buys_computer= “no”\nIF age= “<=30” AND student= “yes”  THEN buys_computer= “yes”\nIF age= “31…40” 	THEN buys_computer= “yes”\nIF age= “>40”   AND credit_rating= “excellent”   THEN buys_computer = \n“yes”\nIF age= “<=30” AND credit_rating= “fair”  THEN buys_computer= “no”\n\nOverfitting\n43\nOverfitting is a condition that occurs when \namachine learningmodel performs significantly \nbetter for training data than it does for new data.\nExample:\nLet’s say we want to predict if a student will get  a job in a \njob interview based on his/herresume.\nNow, assume we train a model from a dataset of 10,000 \nresumes and their outcomes.\nNext, we try the model out on the original dataset, and it \npredicts outcomes with 99% accuracy.\nBut, When we run the model on a new (“unseen”) dataset \nof resumes, we only get 50% accuracy.\n\nDecision Trees: Avoid Overfitting\n44\nOverfitting:  An induced tree may overfit the training data\nToo many branches, some may reflect anomalies due to noise or \noutliers\nPoor accuracy for unseen samples\nTwo approaches to avoid overfitting \nPrepruning: Halt tree construction early—do not split a node if this \nwould result in the goodness measure falling below a threshold\nDifficult to choose an appropriate threshold\nPostpruning: Remove branches from a “fully grown” tree—get a \nsequence of progressively pruned trees\nUse a set of data different from the training data to decide which is the \n“best pruned tree”\n\nPre-Pruning \n45\n\nPost-pruning\n46\n\nPost-pruning\n47\n\nOutline\n48\nDecision Trees\nRandom Forest\nRule Based classifier\n\nRandom Forest\n49\nRandomforest,likeitsnameimplies,consistsofa\nlargenumberofindividualdecisiontrees.\nRandomforestisasupervisedlearningalgorithm.\nThe\"forest\"itbuilds,isanensembleofdecision\ntrees,usuallytrainedwiththe“bagging”method.\nEnsemblelearning,ingeneral,isamodelthatmakes\npredictionsbasedonanumberofdifferentmodels.\nBagging(Bootstrapping):Trainingabunchof\nindividualmodelsinaparallelway.Eachmodelis\ntrainedbyarandomsubsetofthedata\n\nRandom Forest\n50\n\nRandom Forest\n51\nStepsinvolvedinrandomforestalgorithm:\nStep1:InRandomforestnnumberofrandomrecords\nUsingbaggingorbootstrapping\nBootstrappingindicatesthatseveral\nindividualdecisiontreesaretrainedin\nparallelonvarioussubsetsofthetraining\ndatasetusingdifferentsubsetsofavailable\nfeatures.\n\nRandom Forest\n52\n\nRandom Forest\n53\nStep 2: Individual decision trees are constructed for each \nsample.\n\nRandom Forest\n54\nStep 3: In each testing case (Instance), Each decision tree \nwill generate an output.\n\nRandom Forest\n55\nStep 4: Final output is considered based onMajority Voting\n\nRandom Forest: Example\n56\nGiven the instance:\nClassParking \nSpace\nFacilities \nAvailable\n# of \nBedrooms\nLocalityPrice of \nHouse\n?NoNo2yes5 ,000\n\nRandom Forest: Example\n57\nIn the Example:\nThe first tree Buy\nThe second tree Don’t Buy\nThe Third tree Don’t Buy\nThe results\nClassParking \nSpace\nFacilities \nAvailable\n# of \nBedrooms\nLocalityPrice of \nHouse\nDon’t BuyNoNo2yes5 ,000\n\nOutline\n58\nDecision Trees\nRandom Forest\nRule Based Classifier\n\nRule-Based Classifier\nArule-basedclassifierextractsasetof\nrulesthatshowtherelationshipsbetween\ntheattributesofadatasetandtheclass\nlabel\nArule-basedclassifierusesasetofIF-\nTHENrulesforclassification\n\nRule-Based Classifier\n60\nRule:    (Condition) y\nwhere \nConditionis a conjunctions of attributes \nyis the class label\nLHS: rule antecedent or condition\nRHS: rule consequent\n\nRule-Based Classifier\n61\nExamples of classification rules:\n(Refund=yes) and (income > 125) cheat =No\n(Marital status = married) and  (Refund=Yes) cheat=Yes\n\nRule-Based Classifier\nRule Coverage and Accuracy\nCoverage of a rule:\nFraction of records that satisfy \nthe condition of a rule\nAccuracy of a rule:\nFraction of records that satisfy \nboth the condition and \nconsequent of a ruleTid Refund Marital \nStatus \nTaxable \nIncome Class \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n  \n(Status=Single) No\nCoverage = 40%,  Accuracy = 50%\n\nRule-Based Classifier\n63\nGiven the following dataset:\n\nRule-Based ClassifierName Blood Type Give Birth Can FlyLive in Water Class\nlemur warm	yes no no ?\nturtle cold	no no sometimes ?\ndogfish sharkcold	yes no yes ? \nWe got the following rules:\nR1: (Give Birth = no) and  (Can Fly = yes) Birds\nR2: (Give Birth = no) and  (Live in Water = yes) Fishes\nR3: (Give Birth = yes) and  (Blood Type = warm) Mammals\nR4: (Give Birth = no) and  (Can Fly = no) Reptiles\nR5: (Live in Water= sometimes) Amphibians \nA lemur triggers rule R3, so it is classified as a mammal\nA turtle triggers both R4 and R5\nA dogfish shark triggers none of the rules\n\nRule-Based Classifier: Ordered Rule SetName Blood Type Give Birth Can FlyLive in Water Class\nturtle cold	no no sometimes ? \nRules are rank ordered according to their priority\nAn ordered rule set is known as a decision list\nWhen a test record is presented to the classifier \nIt is assigned to the class label of the highest ranked rule it has triggered\nIf none of the rules fired, it is assigned to the default class\nR1: (Give Birth = no) (Can Fly = yes) Birds\nR2: (Give Birth = no) (Live in Water = yes) Fishes\nR3: (Give Birth = yes) (Blood Type = warm) Mammals\nR4: (Give Birth = no) (Can Fly = no) Reptiles\nR5: (Live in Water= sometimes) Amphibians\n\nRule-Based Classifier\nBuilding Classification Rules\nDirect Method: \nExtract rules directly from data\ne.g.: Sequential Covering algorithms\nIndirect Method:\nExtract rules from other classification models (e.g. \ndecision trees).\n\nRule-Based Classifier: Covering algorithms \n67\nCoveringalgorithmsisaStrategyforgeneratingaruleset\ndirectly:foreachclassinturnfindrulesetthatcoversall\ninstancesinit(excludinginstancesnotintheclass)\nThisiscalledacoveringapproachbecauseateachstageyou\nidentifyarulethat“covers”someoftheinstances.Byits\nverynature,thiscoveringapproachleadstoasetofrules\n\nRule-Based Classifier: Covering algorithms \n1.Start from an empty rule\n2.Grow a rule using the Learn-One-Rule function\n3.Remove training records covered by the rule\n4.Repeat Step (2) and (3) until stopping criterion is met\n\nExample of Sequential Covering(i) Original Data (ii) Step 1 \nRule-Based Classifier: Covering \nalgorithms\n\n(iii) Step 2\nR1 (iv) Step 3\nR1\nR2 Rule-Based Classifier: Covering \nalgorithms\n\nRule-Based Classifier: RIPPER \n71\nRIPPER(RepeatedIncrementalPruningtoProduceError\nReduction)isarule-basedlearnerthatbuildsasetof\nrulesthatidentifytheclasseswhileminimizingthe\namountoferror.Theerrorisdefinedbythenumberof\ntrainingexamplesmisclassifiedbytherules.\n\nRule-Based Classifier: RIPPER \nAspects of RIPPER\nRule Growing\nInstance Elimination\nWe need to eliminate instances, Otherwise, the next rule is identical to \nprevious rule\nRule Evaluation\nusing Accuracy, coverage and information gain\nStopping Criterion\nRule Pruning\n\nRule Growing\nThere are two kinds of loop in Ripper algorithm:\nOuter loop : adding one rule at a time to the rule \nbase\nConditions are added to the rule until it covers no \nnegative example.\nInner loop : adding one condition at a time to the \ncurrent rule\nConditions are added to the rule to maximize an \ninformation gain or accuracy measure.\n73\nRule-Based Classifier: RIPPER\n\nRule-Based Classifier: RIPPER \nFor 2-class problem, choose one of the classes as positive \nclass, and the other as negative class\nLearn rules for positive class\nNegative class will be default class\nFor multi-class problem\nOrder the classes according to increasing class prevalence \n(fraction of instances that belong to a particular class)\nLearn the rule set for smallest class first, treat the rest as \nnegative class\nRepeat with next smallest class as positive class\n\nExample: generating a rule\nThe covering method can readily be visualized in a two-\ndimensional space of instances as follows:\n75y\nx\na\nbb\nb\nb\nb\nb\nb\nb\nbb\nb\nb\nb\nb\na\na\na\na\na \nIf true then class = a\nRule-Based Classifier: RIPPER \nWe first make a rule covering the a’s\nStart from empty rule\n\nRule-Based Classifier: RIPPER \n76y\na\nbb\nb\nb\nb\nb\nb\nb\nbb\nb\nb\nb\nb\na\na\na\na\na\nx\n1·2 \nIf x > 1.2 then class = a\nThe first test in the rule, split the space vertically as shown \nin the center picture. This gives the beginnings of a rule:\nIf x > 1.2 then class = a\n\n77y\na\nbb\nb\nb\nb\nb\nb\nb\nbb\nb\nb\nb\nb\na\na\na\na\na\nx\n1·2\n2·6 \nIf x > 1.2 and y > 2.6 then class = a\nHowever, the rule covers many b’s as well as a’s, so a \nnew test is added to the rule by further splitting the space \nhorizontally as shown in the third diagram:\nIf x > 1.2 and y > 2.6 then class = a\nRule-Based Classifier: RIPPER\n\n78\nPossible rule set for class “b”:\nIf x 1.2 then class = b\nIf x > 1.2 and y 2.6 then class = b\nThis gives a rule covering all but one of the a’s. It’s \nprobably appropriate to leave it at that, but if it were \nfelt necessary to cover the final a, another rule \nwould be necessary\nRule-Based Classifier: RIPPER\n\nSequential Covering Algorithm\n79\nwhile (enough target instances left)\ngenerate a rule\nremove positive target instances satisfying this rule\nExamples covered\nby Rule 3\nExamples covered\nby Rule 2\nExamples covered\nby Rule 1\nPositive \nexamples\n\nRule-Based Classifier: Example\n80\nOutlook Temperature Humidity Windy Play?\nsunny Hot high false No\nsunny Hot high true No\novercast Hot high false Yes\nrain Mild high false Yes\nrain Cool normal false Yes\nrain Cool normal true No\novercast Cool normal true Yes\nsunny Mild high false No\nsunny Cool normal false Yes\nrain Mild normal false Yes\nsunny Mild normal true Yes\novercast Mild high true Yes\novercast Hot normal false Yes\nrain Mild high true No\n\nRule-Based Classifier: Example\n81\nOutlookTemperature Humidity Windy Play?\nsunny Hot high false No\nsunny Hot high true No\novercastHot high false Yes\nrain Mild high false Yes\nrain Cool normal false Yes\nrain Cool normal true No\novercastCool normal true Yes\nsunny Mild high false No\nsunny Cool normal false Yes\nrain Mild normal false Yes\nsunny Mild normal true Yes\novercastMild high true Yes\novercastHot normal false Yes\nrain Mild high true No\n\nRule-Based Classifier: Example\n82\nChoose class play=no:\nPossible rules:\nOutlook = sunny 3/5\nOutlook = Overcast  0/4\nOutlook = Rain 2/5\nTemp= Hot	2/4\nTemp = Mild 2/6\nTemp= Cool	1/4\nHumidity= high 4/7\nHumidity = normal 1/7\nWindy = False 2/8\nWindy = True 3/6\n\nRule-Based Classifier: Example\n83\nIf outlook = sunny then play =no   3/5\nTemp= high  2/2\nTemp =mild 1/2\nTemp = cool 0/1\nHumidity = high 3/3\nHumidity = normal 0/2\nWindy =false 2/3\nWindy =  true 1/2\nIf outlook =sunny and humidity = high then play = no   3/3\nOutlook Temperature Humidity Windy Play?\nsunny Hot high false No\nsunny Hot high true No\nsunny Mild high false No\nsunny Cool normal false Yes\nsunny Mild normal true Yes\n\nRule-Based Classifier: Example\n84\nOutlookTemperature Humidity Windy Play?\novercastHot	high false Yes\nrain Mild high false Yes\nrain Cool normal false Yes\nrain Cool normal true No\novercastCool normal true Yes\nsunny Cool normal false Yes\nrain Mild normal false Yes\nsunny Mild normal true Yes\novercastMild high true Yes\novercastHot	normal false Yes\nrain Mild high true No\n\nRule-Based Classifier: Example\n85\nPossible rules:\nOutlook = sunny 0/2\nOutlook = Overcast  0/4\nOutlook = Rain 2/5\nTemp= Hot 0/2\nTemp = Mild 1/5\nTemp= Cool 1/4\nHumidity= high 1/4\nHumidity = Normal 1/7\nWindy = False 0/6\nWindy = True 2/5\n\nRule-Based Classifier: Example\n86\nIf outlook =rain  then play = no   2/5\nTemp = mild 1/3\nTemp =cool 1/2\nHumidity= high 1/2\nHumidity= Normal 2/3\nWindy = false 0/3\nWindy =true 2/2\nIf outlook =rain and windy = true then play = no 2/2 \nOutlook Temperature Humidity WindyPlay?\nrain Mild high falseYes\nrain Cool normal falseYes\nrain Cool normal trueNo\nrain Mild normal falseYes\nrain Mild high trueNo\n\nRule-Based Classifier: Example\n87\nOtherwise play = Yes\nOutlook Temperature Humidity Windy Play?\novercast Hot	high false Yes\nrain Mild	high false Yes\nrain Cool	normal false Yes\novercast Cool	normal true Yes\nsunny Cool	normal false Yes\nrain Mild	normal false Yes\nsunny Mild	normal true Yes\novercast Mild	high true Yes\novercast Hot	normal false Yes\n\nRule-Based Classifier: Example\n88\nFull Rules:\nIf outlook =sunny and humidity = high then play = no 3/3\nIf outlook =rain and windy = true  then play = no 2/2\nOtherwise play = Yes  9/9\n\nRule-Based Classifier: Contact lens Example\n89\n\nRule-Based Classifier: contact lens Example\n90\nSelect recommendation = hard\nRule we seek:\nPossible tests:\nAge = Young	2/8\nAge = Pre-presbyopic	1/8\nAge = Presbyopic	1/8\nSpectacle prescription = Myope	3/12\nSpectacle prescription = Hypermetrope 1/12\nAstigmatism = no	0/12\nAstigmatism = yes	4/12\nTear production rate = Reduced	0/12\nTear production rate = Normal	4/12\nIf ?\nthen recommendation = hard\n\nRule-Based Classifier: contact lens Example\n91\nRule with best test added:\nInstances covered by modified rule:\nAge	Spectacle \nprescription\nAstigmatism Tear production \nrate\nRecommended \nlenses\nYoung Myope	Yes	Reduced	None\nYoung Myope	Yes	Normal	Hard\nYoung Hypermetrope Yes	Reduced	None\nYoung Hypermetrope Yes	Normal	hard\nPre-presbyopicMyope	Yes	Reduced	None\nPre-presbyopicMyope	Yes	Normal	Hard\nPre-presbyopic Hypermetrope Yes	Reduced	None\nPre-presbyopicHypermetrope Yes	Normal	None\nPresbyopic Myope	Yes	Reduced	None\nPresbyopic Myope	Yes	Normal	Hard\nPresbyopic Hypermetrope Yes	Reduced	None\nPresbyopic Hypermetrope Yes	Normal	None\nIf astigmatism = yes \nthen recommendation = hard\n\nRule-Based Classifier: contact lens Example\n92\nCurrent state:\nPossible tests:\nAge = Young	2/4\nAge = Pre-presbyopic	1/4\nAge = Presbyopic	1/4\nSpectacle prescription = Myope	3/6\nSpectacle prescription = Hypermetrope 1/6\nTear production rate = Reduced	0/6\nTear production rate = Normal	4/6\nIf astigmatism = yes   and ? \nthen recommendation = hard\n\nRule-Based Classifier: contact lens Example\n93\nRule with best test added:\nInstances covered by modified rule:\nAge Spectacle \nprescription\nAstigmatism Tear production \nrate\nRecommended \nlenses\nYoung Myope	Yes Normal Hard\nYoung Hypermetrope Yes Normal hard\nPre-\npresbyopic\nMyope	Yes Normal Hard\nPre-\npresbyopic\nHypermetrope Yes Normal None\nPresbyopic Myope	Yes Normal Hard\nPresbyopic Hypermetrope Yes Normal None\nIf astigmatism = yes\nand tear production rate = normal \nthen recommendation = hard\n\nRule-Based Classifier: contact lens Example\n94\nCurrent state:\nPossible tests:\nAge = Young	2/2\nAge = Pre-presbyopic	1/2\nAge = Presbyopic	1/2\nSpectacle prescription = Myope	3/3\nSpectacle prescription = \nHypermetrope\n1/3\nIf astigmatism = yes \nand tear production rate = normal\nand ?\nthen recommendation = hard\nTie between the first and the fourth test\nWe choose the one with greater coverage\n\nRule-Based Classifier: contact lens Example\n95\nFinal rule:\nSecond rule for recommending “hard lenses”: (built from instances \nnot covered by first rule)\nThese two rules cover all “hard lenses”:\nProcess is repeated with other two classes\nIf astigmatism = yes\nand tear production rate = normal\nand spectacle prescription = myope\nthen recommendation = hard\nIf age = young and astigmatism = yes\nand tear production rate = normal\nthen recommendation = hard\n\nReference \n96\nDATA MINING: Concepts, Models, Methods, and \nAlgorithms Ch6\nintroduction to data mining and analytics Ch11\nWhat is a Decision Tree Diagram\nhttps://www.lucidchart.com/pages/decision-tree\nA Complete Guide to the Random Forest Algorithm\nhttps://builtin.com/data-science/random-forest-\nalgorithm\nRule-Based Classifier –Machine Learning\nhttps://www.geeksforgeeks.org/rule-based-classifier-\nmachine-learning/\n\nData Mining\nClassifications 1\nProf. Alaa El-Halees\n97\nChapter  3\n\nOutline\n98\nIntroduction\nk-Nearest Neighbors\nEvaluation\nNaïve Bayesian Classifiers\n\nIntroduction\n99\nClassification is a process of categorizing a given set of data into \nclasses.\n\nIntroduction\n100\nClassificationis“Techniquesusedtopredictgroup\nmembershipfordatainstances”.\nForexample,youmaywishtouseclassificationto\npredictwhethertheweatheronaparticulardaywill\nbe“sunny”,“rainy”or“cloudy”.\n\nIntroduction\n101\nClassificationisaclassicdataminingtask,withrootsin\nmachinelearning.Atypicalapplicationis:\n\"Givenpastrecordsofcustomerswhoswitchedtoanother\nsupplier,predictwhichcurrentcustomersarelikelytodothe\nsame.\"\n\nIntroduction\n102\nThis type of learning called supervised learning\nExample: Classification model may be built to categorize \nbank loan applications as either safeor risky. \nTo build the model, wee need old data from bank about \ncustomers who take loan and whether they repaid (safe) \nit or not (risky) (Training data)\n\nBasic principles of classification\nWant to classify objects as boats and houses\n\nBasic principles of classification\n104\nAll objects before the coast line are boats and all objects after the coast line \nare houses. \n•Coast line serves as a decision surface that separates two classes.\n\nBasic principles of classification\n105\n\nBasic principles of classification\n106\nThe methods that build classification models (i.e., “classification algorithms”) \noperate very similarly to the previous example.\n•First all objects are represented geometrically.\n\nBasic principles of classification\n107\nThen the algorithm seeks to find a decision surface that \nseparates classes of objects\n\nBasic principles of classification\n108\nUnseen (new) objects are classified as “boats” if they fall below the decision \nsurface and as “houses” if the fall above it\n\nIntroduction \n109\nTypical applications of classification:\nFinancialinstitutionsuseclassificationalgorithmstofind\ndefaulterstodeterminewhosecardsandloanstheyshould\napprove.\nMeteorologistsuseclassificationtopredicttheweather\nconditionsaccordingtovariousparameterssuchashumidity,\ntemperature,etc.\nPublichealthexpertsuseclassifiersforpredictingtheriskof\nvariousdiseases.\nInEducationSystemtopredicateifstudentisapplicableforcertainfield.\n\nIntroduction\nClassification Basic Approach:\nGivena collection of records (training set )\nEach record contains a set of attributes, one of the attributes is \nthe class.\nFinda modelfor class attribute as a function of the \nvalues of other attributes (Model Construction).\nGoal: previously unseenrecords should be assigned a \nclass as accurately as possible (Evaluation).\n\nIntroduction\n111\nAtestsetisusedtodeterminetheaccuracyof\nthemodel.\nUsually,thegivendatasetisdividedinto\ntrainingandtestsets,withtrainingsetusedto\nbuildthemodelandtestsetusedtovalidateit.\n\nIntroduction \nTraining\nDataNAMERANK YEARSoccupation\nMikeAssistant Prof3 no\nMaryAssistant Prof7 yes\nBill Professor 2 yes\nJimAssociate Prof7 yes\nDaveAssistant Prof6 no\nAnneAssociate Prof3 no \nClassification\nAlgorithms\nIF rank = ‘professor’\nOR years > 6\nTHEN occupation= ‘yes’  \nClassifier\n(Model)\nProcess 1 Model Construction: Example\n\nIntroduction \nClassifier \nTesting\nDataNAMERANK YEARSoccupation\nTom Assistant Prof2 no\nMerlisaAssociate Prof7 no\nGeorgeProfessor 5 yes\nJosephAssistant Prof7 yes  \nUnseen Data\n(Jeff, Professor, 4) \noccupation?\nProcess 2 Using the Model in Evaluation\n\nMachine learning techniques \n114\nExamples of machine learning techniques:\nk-Nearest Neighbors\nNaïve Bayesian Classifiers\nDecision Trees\nRule Induction\n\nOutline\n115\nIntroduction\nk-Nearest Neighbors\nEvaluation\nNaïve Bayesian Classifiers\n\nK-Nearest Neighbor\n116\nThek-nearestneighbors(KNN)algorithm\nisasimple,easy-to-implementsupervised\nmachinelearningalgorithmthatcanbe\nusedtosolveclassificationproblems\nTheKNNalgorithmassumesthatsimilar\nthingsexistincloseproximity.Inother\nwords,similarthingsareneartoeach\nother.\nAlsocalledinstancebasedlearning\n\nK-Nearest Neighbor\n117\nAlgorithm\nGiven a new instance x, \nFind its nearest neighbor <x’,y’>\nReturn y’as the class of x\n\n1-nn: q1 is positive\n5-nn: q1 is classified as negative\nK-Nearest Neighbor\n+\n+\n+\n+\n-\n-\n-\n-\n-\n-\nq1\nDealing with noise –k-\nnearest neighbor\nUse more than 1 neighbor\n\nK-Nearest Neighbor\nTheclassifiersdonotuseanymodeltofitandonly\nbasedonmemory.Givenaquerypoint,\nWefindKnumberofobjectsor(trainingpoints)closest\ntothequerypoint.Theclassificationisusingmajority\nvoteamongtheclassificationoftheKobjects.\nAnytiescanbebrokenatrandom\n119\n\n120\n\nK-Nearest Neighbor\n121\nLazy vs. eager learning\nLazylearning(e.g.nearestNeighbor):\nSimplystorestrainingdata(oronlyminorprocessing)andwaits\nuntilitisgivenatestaninstance.\nlesstimeintrainingbutmoretimeinpredicting\nEagerlearning(e.g.Decisiontreeandneuralnetwork):\nGivenasetoftrainingset,constructsaclassificationmodelbefore\nreceivingnew(e.g.,test)datatoclassify\n\nk-Nearest Neighbor\nMeasureDistance:\nAllinstancescorrespondtopointsinthen-Dspace\nThenearestneighboraredefinedintermsofEuclidean\ndistance.\nEuclideandistancebetweentwopoints,X=(x\n1,x\n2,…,x\nn)\nandY=(y\n1,y\n2,…y\nn)is:\n\n\nn\ni\niiyxYXd\n1\n2\n)(),(\n\nk-Nearest Neighbor: Example\nThe distance between points 1 and 2, (user 1 and user \n2) can be calculated as below:\n123\n\nNearest Neighbor:  Algorithm\n124\nHereisstepbysteponhowtocomputeK-nearest\nneighborsKNNalgorithm:\n1)DetermineparameterK=numberofnearest\nneighbors\n2)Calculatethedistancebetweenthequery-instance\nandallthetrainingsamples\n3)Sortthedistanceanddeterminenearestneighbors\nbasedontheK-thminimumdistance\n4)Gatherthecategoryofthenearestneighbors\n5)Usesimplemajorityofthecategoryofnearest\nneighborsasthepredictionvalueofthequery\ninstance\n\nNearest Neighbor:  Example\nWe have data from the \nquestionnaires survey here \nis four training samples :\ntest with X\n1= 3 and X\n2= 7 \nX\n1 X\n2 Y\n7\n7 Bad\n7\n4 Bad\n3\n4 Good\n1\n4 Good\n\nNearest Neighbor:  Example\n1. Determine parameter K = \nnumber of nearest \nneighbors \nSuppose use K = 3 \n2. Calculate the distance \nbetween the query-instance \nand all the training samples\nX1X2 Distance\n7\n7(7-3)\n2\n+(7-7)\n2\n=16\n7\n4(7-3)\n2\n+(4-7)\n2\n=25\n3\n4(3-3)\n2\n+(4-7)\n2\n=9\n1\n4(1-3)\n2\n+(4-7)\n2\n=13\n126\n\nNearest Neighbor:  Example\n3. Sort the distance and determine nearest neighbors based \non the K-th minimum distance \nX1X2 Distance Rank\n7\n7 (7-3)\n2\n+(7-7)\n2\n=16 3\n7\n4 (7-3)\n2\n+(4-7)\n2\n=25 4\n3\n4 (3-3)\n2\n+(4-7)\n2\n=9 1\n1\n4 (1-4)\n2\n+(4-7)\n2\n=13 2\n127\n\nNearest Neighbor:  Example\n4. Gather the category  of the nearest neighbors. Notice in the second row \nlast column that the category of nearest neighbor (Y) is not included \nbecause the rank of this data is more than 3 (=K). \nX1X2 Distance Rank Y\n7\n7 (7-3)\n2\n-(7-7)\n2\n=16 3 Bad\n7\n4 (7-3)\n2\n-(4-7)\n2\n=25 4 -\n3\n4 (3-3)\n2\n-(4-7)\n2\n=9 1 Good\n1\n4 (1-4)\n2\n-(4-7)\n2\n=13 2 Good\n128\n\nNearest Neighbor:  Example\n129\n5)Usesimplemajorityofthecategoryofnearestneighborsas\nthepredictionvalueofthequeryinstance\nWehave2goodand1bad,since2>1thenweconclude\nthatanewtestwithX1=3andX2=7isincludedin\nGoodcategory.\n\nNearest Neighbor\nScalingissues\nAttributesmayhavetobescaledtoprevent\ndistancemeasuresfrombeingdominatedby\noneoftheattributes\nSolution:Normalizethevectorstounitlength\n(makeallvaluesbetween0and1).\n\nNearest Neighbor\n131\nExample:\nAttribute Call Duration dominate the others.\n\nNearest Neighbor\n132\nUse Min-Max\nMatrix Becomes:AAA\nAA\nA\nminnewminnewmaxnew\nminmax\nminv\nv	_)__(\'	\n\n\n\n\nNearest Neighbor: Categorical variable\nIf attribute is categorical\nUse 0/1 distance:\nFor each attribute, add 1 if the instances differ in that \nattribute, else 0\n133\n\nNearest Neighbor: Categorical variable\n1341\n0\n\n\nDyx\nDyx \nX Y Distance\nMale Male 0\nMale Female1\nExample\n\nNearest Neighbor: Example\nCustomer ID Debt Income Marital StatusRisk\nAbel High High Married Good\nBen Low High Married Doubtful\nCandy Medium Very low Unmarried Poor\nDale Very highLow Married Poor\nEllen High Low Unmarried Poor\nFred High Very low Married Poor\nGeorge Low High Unmarried Doubtful\nHarry Low Medium Married Doubtful\nIgor Very Low Very HighMarried Good\nJack Very HighMedium Married Poor\n\nNearest Neighbor: Example\nK = 3\nDistance \nScore for an attribute is 1 for a match and 0 otherwise\nDistance is sum of scores for each attribute\nCustomer \nID\nDebt Income Marital StatusRisk\nZeb High Medium Married ?\n\nNearest Neighbor: Example\nCustomer \nID\nDebt Income Marital \nStatus\nRisk Distance\nAbel High High MarriedGood 1\nBen Low High MarriedDoubtful2\nCandy Medium Very lowUnmarriedPoor 3\nDale Very highLow MarriedPoor 2\nEllen High Low UnmarriedPoor 2\nFred High Very lowMarriedPoor 1\nGeorge Low High UnmarriedDoubtful3\nHarry Low Medium MarriedDoubtful1\nIgor Very LowVery HighMarriedGood 2\nJack Very HighMedium MarriedPoor 1\n\nNearest Neighbor: Example\nCustomer \nID\nDebt Income Marital \nStatus\nRisk Distance\nAbel High High MarriedGood 1\nBen Low High MarriedDoubtful\nCandy Medium Very lowUnmarriedPoor\nDale Very highLow MarriedPoor\nEllen High Low UnmarriedPoor\nFred High Very lowMarriedPoor 1\nGeorge Low High UnmarriedDoubtful\nHarry Low Medium MarriedDoubtful1\nIgor Very LowVery HighMarriedGood\nJack Very HighMedium MarriedPoor 1\n\nNearest Neighbor: Example\nCustomer \nID\nDebt Income Marital StatusRisk\nZeb High Medium Married Poor\nTwo poor one Good and one doubtful\n\nOutline\n140\nIntroduction\nk-Nearest Neighbors\nEvaluation\nNaïve Bayesian Classifiers\n\nEvaluation\nEvaluationon“LARGE”data\nIfmanyofexamplesareavailable,\nincludingseveralexamplesfromeach\nclass,thenasimpleevaluationissufficient\nRandomlysplitdataintotrainingandtest\nsets(usually2/3fortrain,1/3fortest)\nBuildaclassifierusingthetrainsetand\nevaluateitusingthetestset.\n\nClassification Step 1: \nSplit data into train and test sets\nResults Known\n+\n+\n-\n-\n+\nTHEPAST\nData\nTraining set\nTesting set\nEvaluation\n\nClassification Step 2: \nBuild a model on a training set\nTraining set\nResultsKnown\n+\n+\n-\n-\n+\nTHE PAST\nData\nModel Builder\nTesting set\nEvaluation\n\nClassification Step 3:\nEvaluate on test set\nData\nPredictions\nYN\nResultsKnown\nTraining set\nTesting set\n+\n+\n-\n-\n+\nModel Builder\nEvaluate\n+\n-\n+\n-\nEvaluation\n\n145 \nExample\nEvaluation\n\nEvaluation\nGenerally,thelargerthetrainingdatathebetterthe\nclassifier.\nThelargerthetestdatathemoreaccuratetheerror\nestimate.\n\nEvaluation : Confusion Matrix\nIntheclassificationproblem,theprimarysourceof\nperformancemeasurementisconfusionmatrix.\nTheconfusionmatrixisausefultoolforanalyzinghow\nwellyourclassifiercanrecognizedataofdifferentclasses.\nConfusionmatrixfortwo–classclassificationproblemis:\n\nConfusion Matrix\nTrue Class\nNegativePositive\nFalse \nPositive \n(FP)\nTrue \nPositive \n(TP)\nPositive\nPredi\ncted\nClass\nTrue \nNegative \n(TN)\nFalse \nNegative \n(FN)\nNegative\nConfusion Matrix-Table\n\nEvaluation : Confusion Matrix\n149\nd no\nTemperatureHumidity\nPlay? (True \nResult)\nPlay? (Predicate \nResult)\n1 hot high Yes No\n2 hot high No No\n3 hot high No Yes\n4 mild high Yes Yes\n5 cool normal Yes Yes\n6 cool normal No No\n7 cool normal No No\n8 mild high Yes No\n9 cool normal Yes Yes\n10 mild normal Yes Yes\n11 mild normal Yes No\n12 mild high No Yes\n13 hot normal No No\n14 mild high Yes Yes\n\nEvaluation : Confusion Matrix\nTrue Class\nNegativePositive\nFalse \nPositive \n(FP)\nTrue \nPositive \n(TP)\nPositive\nPredi\ncted\nClass\nTrue \nNegative \n(TN)\nFalse \nNegative \n(FN)\nNegative\nTrue Class\nnoyes\n45yes\nPredi\ncted\nClass\n23no\nConfusion Matrix-Table\n\nEvaluation : Confusion Matrix\nTrue Class\nnoyes\n466954yes\nPredi\ncted\nClass\n2588412no\nConfusion matrix for\nclass buys computer\nAnother Example\n\nConfusion Matrix-Accuracy\nInthetable:\nTruepositive(TP)refertopositiveinstancesthatcorrectlylabeled\ntheclassifier(numberofcustomerspredictedtobuycomputers\nandactuallyboughtcomputer).\nTruenegatives(TN)refertonegativeinstancesthatcorrectly\nlabeledtheclassifier(numberofcustomerspredictedtonotbuy\ncomputerandactuallydidnotbuycomputer).\nFalsePositive(FP)arethenegativeinstancesthatwereincorrectly\nlabeled(numberofcustomerswhopredictedtobuycomputersand\ntheyactuallydidnotbuyscomputers)\nFalseNegative(FN)arethepositiveinstancesthatwereincorrectly\nlabeled(numberofcustomerswhopredictedtonotbuycomputers\nandtheyactuallyboughtcomputers)\n\nConfusion Matrix-Accuracy\nFrom the matrix we can find the following formulas:\n1) Accuracy= (TP + TN) / (TP + TN + FP +FN)\n2) Precision= TP/(TP + FP)\n3) Recall= TP/(TP +FN)\n4) F-measure= 2*Precision*Recall /(Precision+Recall)\n5) Error Rate  = (FN+FP) / (TP + TN + FP +FN)\n\nConfusion Matrix-Accuracy\nFrom the example matrix we can calculate the \nfollowing formulas:\n1) Accuracy= (TP + TN) / (TP + TN + FP +FN)\n(6854 + 2588)/10000= 0.9442=95.42%\n\nConfusion Matrix-Accuracy\n2) Precision = TP/(TP + FP) = 6954/(6954+46)= 99.34%\n3) Recall = TP/(TP +FN) = 94.41%\n4) F-measure = 2*Precision*Recall /(Precision+Recall)\n= 2 * 0.99 * 0.94/ (0.99+0.94)\n= 96.43%\n5) Error Rate = (FP +FN)/ (TP + TN + FP +FN)\n=(46 +412)/10000\n= 4.58%\n\nConfusion Matrix-Accuracy\nWhenclassificationproblemnotbinary,confusion\nmatrixgetmorecomplicated.Wecancompute\nclassifieraccuracyas:\nWhere iis the class number and nis total number of the classescases ofnumber  Total\non)clasifcati rue(\n1\ni\n\n\nn\ni\nT\nAccuacy\n\nConfusion Matrix-Accuracy\nTrue Class\nItem 3item2Item 1\ne\n13e\n12tp\n1\nItem 1\nPredicted\nClass\ne\n23tp\n2e\n21\nitem2\ntp\n3e\n32e\n31\nitem3\n\nConfusion Matrix-Accuracy\nPrecision item1 = tp\n1/(tp\n1+e\n21+e\n31) \nRecall item 1= tp\n1/(tp\n1+e\n12+e\n13) \nF-measure item 1 = 2 * Precision * Recall / (Precision + Recall)\n\nConfusion Matrix-Accuracy\nTrue Class\nColdMildHot\n2722Hot\nPredicted\nClass\n7185Mild\n2153Cold\nAccuracy = (22+18+21)/90 = 67.77%\nExample\n\nConfusion Matrix-Accuracy\nPrecision Hot  = tp\n1/(tp\n1+e\n21+e\n31) =\n22/(22+5+3)=73.33%\nRecall Hot= tp\n1/(tp\n1+e\n12+e\n13) =\n22/(22+7+2)=70.96%\nF-measure Hot  =2 * Precision * Recall / (Precision + Recall)\n= 2 *0.63*0.74/(0.63+0.71)\n= 69.58%\n\nOutline\n161\nIntroduction\nk-Nearest Neighbors\nEvaluation\nNaïve Bayesian Classifiers\n\nBayes Classification\n162\nBayesianclassificationisbasedonBayes\'\nTheorem.\nBayesianclassifiersarethestatistical\nclassifiers.\nBayesianclassifierscanpredictclass\nmembershipprobabilitiessuchasthe\nprobabilitythatagiveninstancebelongsto\naparticularclass.\n\nBayes Classification\n163\nBayesClassificationisaterminBayesianstatisticsdealing\nwithasimpleprobabilisticclassifierbasedonapplying\nBayes\'theoremwithstrong(naive)independence\nassumptions\n\nBayes Classification\n164\nThis is a direct application of Bayes’ rule\nx -a vector of x\n1,x\n2,…,x\nn\nc -is class label\n\nBayes Classification: Theorem\n165\nLetXbeadatasamplewhoseclasslabelis\nunknown\nLethbeahypothesisthatXbelongstoclassc\nForclassificationproblems,determine:\nP(h=c|X):theprobabilitythatthehypothesisholds\ngiventheobserveddatasampleX\nP(h=c):priorprobabilityofhypothesish(i.e.the\ninitialprobabilitybeforeweobserveanydata,\nreflectsthebackgroundknowledge)\nP(X):probabilitythatsampledataisobserved\nP(X|h=c):probabilityofobservingthesampleX,\ngiventhatthehypothesisholds\n\n166\nBayes Classification: An Example \nX : 35 year old customer with an income of $50,000 PA\nh : Hypothesis that our customer will buy our computer \n(h=yes) \nI am 35 \nyears old\nI earn \n$50,000 PA\nWill he buy a \ncomputer?\n\n167\nP(h=yes|X):ProbabilitythatcustomerXwillbuyour\ncomputergiventhatweknowhisageandincome\nP(h=yes):Probabilitythatanycustomerwillbuyour\ncomputerregardlessofageandincome(Prior\nProbability)\nP(X):Probabilitythatapersonfromoursetof\ncustomersis35yrsoldandearns$50,000\nP(X|h=yes):Probabilitythatthecustomeris35yrs\noldandearns$50,000,giventhathehasboughtour\ncomputer(PosteriorProbability)\nBayes Classification: An Example\n\n168\nOutlook Play?\nsunny No\nsunny No\novercast Yes\nrain Yes\nrain Yes\nrain No\novercast Yes\nsunny No\nsunny Yes\nrain Yes\nsunny Yes\novercast Yes\novercast Yes\nrain No\nWe want to see if we need to play or not given only one \nattribute (outlook)  Using Bays Rule\nP(C|x) = P(x|C)P(C)/P(x)\nFirst we want calculate the probability of play=yes\nP(play=Yes | outlook=Sunny) =\nP( outlook= Sunny | play=Yes) \n* P(play=Yes) / P (outlook=Sunny)\nP (outlook=Sunny |play=Yes) = 2/9 = 0.22\nP( play=Yes)= 9/14 = 0.64\nP(outlook=Sunny) = 5/14 = 0.36\nP (play=Yes | outlook=Sunny) = 0.22 * 0.64 / 0.36 = 0.36\nBayes Classification: An Example\n\n169\nOutlook Play?\nsunny No\nsunny No\novercast Yes\nrain Yes\nrain Yes\nrain No\novercast Yes\nsunny No\nsunny Yes\nrain Yes\nsunny Yes\novercast Yes\novercast Yes\nrain No\nSecond  we want calculate the probability of play=No\nP(play=No | outlook=Sunny) =\nP( outlook= Sunny | play=No) \n* P(play=No)/ P(outlook=Sunny) \nP (outlook=Sunny |play=No) = 3/5 = 0.6\nP( play=No)= 5/14 = 0.36\nP(outlook=Sunny) = 5/14 = 0.36\nP (play=No | outlook=Sunny) = 0.6 * 0.36 / 0.36 = 0.6\nSo, \nP (play=Yes | outlook=Sunny) = 0.36\nP (play=No | outlook=Sunny) = 0.6\nWe can conclude that for outlook Sunny \nwe can NOT play\nBayes Classification: An Example\n\n170\nIn addition, if we look at the two equations: \nP(play=Yes | outlook=Sunny) =  P( outlook= Sunny | \nplay=Yes)  * P(play=Yes) / P (outlook=Sunny)\nand\nP(play=No | outlook=Sunny) = P( outlook= Sunny | \nplay=No)  * P(play=No)/ P(outlook=Sunny) \nWe can notice that\n(outlook=Sunny) is common for both equations\nFor comparison we do not need it in the equation\nThe equations will be:\nP (play=Yes | outlook=Sunny) = 0.22 * 0.64= 0.14\nP (play=No | outlook=Sunny) = 0.6 * 0.36  = 0.22\nBayes Classification: An Example\n\n171\nFor more than one attribute\nA simplified assumption: attributes are conditionally \nindependent:\nThe  product of occurrence of say 2 elements x\n1and x\n2, given the \ncurrent class is C, is the product of the probabilities of each \nelement taken separately, given the same class P([y\n1,y\n2],C) = \nP(y\n1,C) * P(y\n2,C)\nNo dependence relation between attributes \n\n\nn\nk\nCixk\nPCi\nXP\n1\n)|()|( \nBayes Classification\n\nBayes Classification\n172\nFor complete example:\nhttps://www.javatpoint.com/machine-learning-naive-bayes-\nclassifier\n\nBayes Classification: Example  \n173Sample\nA1 A2 A3 Class\n11 2	1 A\n20 0	1 B\n32 1	2 B\n41 2	1 B\n50 1	2 A\n62 2	2 B\n71 0	1 A \nIt is necessary to predict classification of the new sample\nX = {1, 2, 2, class = ?}.\n\nBayes Classification: Example\n174\nIn our example, we need to maximize the product \nP(X|C\ni) * P(C\ni) for i = A,B because there are only two \nclasses. First, we compute prior probabilities P(C\ni) of the \nclass:\nP(C=A)=4/7=0.57\nP(C=B)=3/7=0.43\n\n175\nSecond, we compute conditional probabilities P(X|C\ni) for \nevery attribute value given in the new sample X = {1, 2, 2, \nC = ?}, \nor more precisely, X = {A1 = 1, A2 = 2, A3 = 2, C = ?}) \nusing training data sets:\nP(A\n1=1|C=A)=2/4=0.5\nP(A\n1=1|C=B)=1/3=0.3333\nP(A\n2=2|C=A)=1/4=0.25\nP(A\n2=2|C=B)=2/3=0.666\nP(A\n3=2|C=A)=1/4=0.25\nP(A\n3=2|C=B)=2/3=0.666\nBayes Classification: Example\n\nBayes Classification: Example \n176\nUnder the assumption of conditional independence of \nattributes, the conditional probabilities P(X|C\ni) will be:\nP(X|C=A)=P(A1=1|C=A)* P(A2=2|C=B)* P(A3=2|C=A)= \n0.5*0.25*0.25=0.03\nP(X|C=B)=P(A1=1|C=B)* P(A2=2/C=B)* P(A3=2/C=B) \n=0.33*0.66*0.66=0.14\n\nBayes Classification: Example \n177\nUnder the assumption of conditional independence of \nattributes, the conditional probabilities P(X|C\ni) will be:\nP(C=A|X)=P(X|C=A).P(C=A)=0.03 * 0.57 = 0.017\nP(C=B|X)=P(X/C=B).P(C=B)=0.14* 0.43 = 0.060\nBased on the previous two values that are the final results \nof the Naive Bayesian Classifier, we can predict that the \nnew sample X belongs to the   Class = B.\n\nBayes Classification: Another \nExample\n178ageincomestudentcredit_ratingbuys_computer\n<=30high nofair	no\n<=30high noexcellent no\n30…40high nofair	yes\n>40 medium nofair	yes\n>40 low yesfair	yes\n>40 low yesexcellent no\n31…40low yesexcellent yes\n<=30medium nofair	no\n<=30low yesfair	yes\n>40 medium yesfair	yes\n<=30medium yesexcellent yes\n31…40medium noexcellent yes\n31…40high yesfair	yes\n>40 medium noexcellent no \nClass:\nC1:buys_computer=\n‘yes’\nC2:buys_computer=\n‘no’\nData sample \nX =(age<=30,\nIncome=medium,\nStudent=yes\nCredit_rating=\nFair)\n\nBayes Classification: Another \nExample\n179\nCompute P(X/Ci) for each class\nP(age=“<30” | buys_computer=“yes”)  = 2/9=0.222\nP(age=“<30” | buys_computer=“no”) = 3/5 =0.6\nP(income=“medium” | buys_computer=“yes”)= 4/9 =0.444\nP(income=“medium” | buys_computer=“no”) = 2/5 = 0.4\nP(student=“yes” | buys_computer=“yes)= 6/9 =0.667\nP(student=“yes” | buys_computer=“no”)= 1/5=0.2\nP(credit_rating=“fair” | buys_computer=“yes”)=6/9=0.667\nP(credit_rating=“fair” | buys_computer=“no”)=2/5=0.4\nX=(age<=30 ,income =medium, student=yes,credit_rating=fair)\nP(X|Ci) : P(X|buys_computer=“yes”)= 0.222 x 0.444 x 0.667 x 0.0.667 \n=0.044\nP(X|buys_computer=“no”)= 0.6 x 0.4 x 0.2 x 0.4 =0.019\nP(X|Ci)*P(Ci ) : P(X|buys_computer=“yes”) * \nP(buys_computer=“yes”)=0.028\nP(X|buys_computer=“yes”) * \nP(buys_computer=“yes”)=0.007\nX belongs to  class “buys_computer=yes”\n\nReference\n180\nDATA MINING AND MACHINE LEARNING Ch. 18,22\nDATA MINING Concepts, Models, Methods, and \nAlgorithms Ch. 4,5\nSimple guide to confusion matrix terminology\nhttps://www.dataschool.io/simple-guide-to-confusion-\nmatrix-terminology/\nStatQuest: K-nearest neighbors, Clearly Explained\nhttps://www.youtube.com/watch?v=HVXime0nQeI\nNaive Bayes, Clearly Explained!!!\nhttps://www.youtube.com/watch?v=O2L2Uv9pdDA\n\nData Mining\nProf. Alaa El-Halees\n181\nData Preparation and Pre-processing\nChapter  2\n\nOutline\n182\nIntroduction\nBusiness and   Data Understanding\nData Cleaning\nMissing values\nNoisy Data\nInconsistent Data \nData Integration\nData Transformation\nData Reduction\nFeature Selection\nSampling\n\nIntroduction \n183\nDataPreparationandpreprocessingdescribesanytypeof\nprocessingperformedonrawdatatoprepareit\nforanotherprocessingprocedure\nThereal–worlddatatypicallyusedindataminingmay\nhavemillionsofrecordsandthousandsofvariables.Itis\nnoisyandhasmissingandinconsistentvalues.\nOneoftheprimarypurposesofdatapreparationisto\nensurethatrawdatabeingreadiedfordata\nprocessingandanalysisisaccurateandconsistent\nsotheresultsofdataminingwillbevalid.\n\nIntroduction\n184\nDataqualityisakeyissuewithdatamining\nToincreasetheaccuracyofthemining,hastoperform\ndatapreprocessing.\nOtherwise,\ngarbagein=>garbageout\nDataPreparationestimatedtotake70-80%ofthetime\nandeffort\n\nIntroduction\n185\nBusiness and  \nData \nUnderstanding\nData Cleaning Data Integration\nData \nTransformation\nData Reduction\nForms of data preprocessing\n\nOutline\n186\nIntroduction\nBusiness and   Data Understanding\nData Cleaning\nMissing values\nNoisy Data\nInconsistent Data \nData Integration\nData Transformation\nData Reduction\nFeature Selection\nSampling\n\nBusiness and Data Understanding\n187\nThere are two main tasks addressed in this stage:\n•Defineobjectives:Workwithyourcustomerand\notherstakeholderstounderstandandidentifythe\nbusinessproblems.Formulatequestionsthatdefine\nthebusinessgoalsthatthedataminingtechniques\ncantarget.\n•Identifydatasources:Findtherelevantdatathat\nhelpsyouanswerthequestionsthatdefinethe\nobjectivesoftheproject.\n\nBusiness Understanding\n188\n\nBusiness Understanding\n189\nToobtainthehighestbenefitfromdatamining,theremust\nbeaclearBusinessUnderstanding\nBusinessUnderstanding:Understandtheproject\nobjectivesandrequirementsfromabusinessperspective.\n“Business Goals Law”:\nBusiness objectives are the origin of every data mining \nsolution\n\nBusiness Understanding\n190\nDataminingisnotprimarilyatechnology;itisaprocess,which\nhasoneormorebusinessobjectivesatitsheart.Withouta\nbusinessobjective,thereisnodatamining.\n\nBusiness Understanding\n191\nExample of Goal for business company are: \nYou want to attract new customers\nYou want to avoid high -risk customers\nYou want to understand the characteristics of \nyour current customers?\nYou want to make your unprofitable customers \nmore profitable?\nYou want to retain your profitable customers?\nYou want to win back your lost customers?\nYou want to improve customer satisfaction?\nYou want to increase sales?\nYou want to reduce expenses\n\nData Understanding\n192\nData:Collectionofobjectsandtheirattributes\nObjectisalsoknownasrecord,point,case,\nentity,orinstance.\nAnattributeisapropertyorcharacteristicofan\nobject.\nExamples:eyecolorofaperson,temperatureofaday,etc\nAttributeisalsoknownasvariable,field,\ncharacteristic,orfeature\nCollectionofattributesdescribeanoneobject\n–\n\nData Understanding\n193\n\nAttribute Types:\n194\nBinominal\nExactly two values (for example true/false or yes/no).\nDate\nDate without time (for example 23.12.2021).\nDate_time\nBoth date and time (for example 23.12.2021 17:59).\nInteger\nA whole number (for example, 23, -5, or 11,024,768).\n\nAttribute Types:\n195\nPolynomial\nMany different string values (for example red, green, blue, \nyellow).\nReal\nA fractional number (for example 11.23 or -0.0001).\nTime\nTime without date (for example 17:59).\n\nData Understanding\n196\nData Collection: Relevance:\nWhat data is available for the task?\nIs this data relevant? \nIs additional relevant data available?\nHow much historical data is available?\nWho is the data expert ?\n\nData Understanding\n197\nDomainExpertiseAdomainexpertisapersonwith\nspecialknowledgeorskillsinaparticularareaof\nenterprise.\nAnaccountantisanexpertinthedomainofaccountancy,\nforexample.\nThedevelopmentofdataminingprojectrequires\nknowledgeintwodifferentdomains,namelyaccounting\nanddatamining.\n\nData Understanding\n198\nSimpletoolsthatshowhistogramsofthe\ndistributionofvaluesofnominalattributes,\nandgraphsofthevaluesofnumericattributes\nareveryhelpful.\n\nData Understanding\n199\nThishistogramshowsthenumberofcustomerswithvarious\neyecolors.Thissummarycanquicklyshowimportant\ninformationaboutthedatabasesuchasthatblueeyesare\nthemostfrequent\n\nData Understanding\n200\nThisgraphshowsthenumberofcustomersofdifferentages\nandquicklytellstheviewerthatthemajorityofcustomersare\novertheageof50\n\nOutline\n201\nIntroduction\nBusiness and   Data Understanding\nData Cleaning\nMissing values\nNoisy Data\nInconsistent Data \nData Integration\nData Transformation\nData Reduction\nFeature Selection\nSampling\n\nData Cleaning\n202\nBusiness and  \nData \nUnderstanding\nData Cleaning Data Integration\nData \nTransformation\nData Reduction\nForms of data preprocessing\n\nData Cleaning\n203\nReal-worlddatatendstobeincomplete,noisyand\ninconsistent.\n\nData Cleaning\n204\nData Cleaning Steps\nMissing values\nNoisy Data\nInconsistent Data\n\nData Cleaning :Missing values\n205\nA missing value is an empty(?)cell in the table that \nrepresents a dataset\n\nData Cleaning :Missing values\n206\nDealing with missing values: \n1) Ignore records with missing values\nThismethodisbasedonignoringcaseswithmissingattribute\nvalues.Thismethodisnoteffective,unlesstherecord\ncontainsseveralattributeswithmissingvalues.\nThisisusuallydonewhentheclasslabelismissing.\n\nData Cleaning: Missing values\n207\n2) Fill in the missing value manually:\nIngeneral,thisapproachistime-consumingandmaybenotgood\ngivenalargedatasetwithmanymissingvalues.\nFluNauseaHeadacheTemperatureCase\nYesNoYesHigh1\nYesYesYesHigh2\nNoNoNoNormal3\nYesYesYesHigh4\nNoYesNoHigh5\nNoNoYesNormal6\nNoYesNoNormal7\nYesNoYesHigh8\n\nData Cleaning: Missing values\n208\nFluNauseaHeadacheTemperatureCase\nYesNoUnknownHigh1\nYesYesYesHigh2\nNoNoNoUnknown3\nYesYesYesHigh4\nNoYesUnknownHigh5\nNoNoYesNormal6\nNoYesNoNormal7\nYesUnknownYesUnknown8\n3)UseGlobalconstanttofillinmissingvalue:Replaceall\nmissingvaluesbysameconstantsuchas“unknown”.\nAlthoughthismethodissimplebutitisnotrecommended\nbecauseresultswith“unknownvaluesarenot\n“interesting”.\n\nData Cleaning: Missing values\n209\n4)Usetheattributemeantofillmissingvalues.Iftheattributeis\nnumbertakethemeanvaluesofthecolumn,ifcategorical\ntakethemostfrequent.\n\nData Cleaning: Missing values\n210\n\nData Cleaning\n211\nData Cleaning Steps\nMissing values\nNoisy Data\nInconsistent Data\n\nData Cleaning: Noisy Data\n212\nNoiseis a random error in measured variable.\nSource of Noisy data:\n-Data entry problem\n-Faulty data collection instruments\n-Data transformation\n\nData Cleaning: Noisy Data\n213\nHow to handle noisy data:\n1) Binning method:\nBinningmethodisusedtosmoothingdataorto\nhandlenoisydata.Inthismethod,thedataisfirst\nsortedandthenthesortedvaluesaredistributedinto\nanumberofbucketsorbins.\n\nData Cleaning: Noisy Data\n214\nExample: \nSorted data for price (in dollars): 4, 8, 9, 15, 21, 22, 24, 25, 26, 28, \n29, 3500\nPartition into Equal frequency (Bin =3):\n-Bin 1: 4, 8, 9, 15    bin 1: [1-15]\n-Bin 2: 21, 22, 24, 25 bin2: [21-25]\n-Bin 3: 26, 28, 29, 3500 bin3 [26 –3500]\nSo, the price will be:\n[1-15], [1-15], [1-15], [1-15], [21-25], [21-25], [21-25], [21-25], \n[26 –3500], [26 –3500], [26 –3500], [26 –3500]\n\nData Cleaning: Noisy Data\n215\n2)Outlier:Outliersmay\nbedetectedbyclustering,\nwheresimilarvalues\nareorganizedinto\ngroups,valuesthatfall\noutsidethesetofclusters\nmay be considered\noutliers.\nSeeoutlierchapter.\n\nData Cleaning: Noisy Data\n216\n\nData Cleaning: Noisy Data\nAfter detecting the outlier, the outlier will be consider as \nmissing value.\n217\n\nData Cleaning\n218\nData Cleaning Steps\nMissing values\nNoisy Data\nInconsistent Data\n\nData Cleaning: Inconsistent Data\n219\nInconsistencyoccurswhentwodata\nitemsinthedatasetcontradicteach\nother:\nForexample,theZIPcodeissavedinone\ntableas0000-000numericdataformat;\nwhileinanothertableitmaybe\nrepresentedin0000000.\n\nData Cleaning: Inconsistent Data\n220\nForexample:customerisrecordedintwodifferent\nsystemsashavingtwodifferentcurrentaddresses,\nandonlyoneofthemcanbecorrect.\nInthiscasewecanconsiderthenewone.\nDatawhichisinconsistentwithourmodels,should\nbedealtwith.\nCommonsensecanalsobeusedtodetectsuchkind\nofinconsistency.\n\nData Cleaning: Inconsistent Data\n221\nForexample:\nThesamenameoccurringdifferentlyinan\napplication\nDifferentnamescanappeartobethesame(Dennis\nVsDenis)\nWasrating“1,2,3”,nowrating“A,B,C.\nYesinsomevaluesandYinotherand\nTrueinother.\n\nData Cleaning: Inconsistent Data\n222\n\nData Cleaning: Inconsistent Data\n223\nFixinginconsistencyisnotalwayspossible:itrequires\navarietyofstrategies.\nForexample:decidingwhichdatawererecorded\nmorerecently\nChangevaluestoonestandard(Yes,YandTrueto\nyes).\n\nOutline\n224\nIntroduction\nBusiness and   Data Understanding\nData Cleaning\nMissing values\nNoisy Data\nInconsistent Data \nData Integration\nData Transformation\nData Reduction\nFeature Selection\nSampling\n\nData Integration\n225\nGoal \nidentification \nand  Data \nUnderstanding\nData Cleaning Data Integration\nData \nTransformation\nData Reduction\nForms of data preprocessing\n\nData Integration\n226\nData Integration\nCombinesdatafrommultiplesourcesintoa\ncoherentstore\nDataminingusuallyrequireddata(whichiscalled\ndataset)inonetable.\nIncreasinglydataaminingprojectsrequiredata\nfrommorethanonedatasource\nSuchasmultipledatabases,datawarehouse,flatfiles\nandhistoricaldata.\n\nData Integration\n227\n\nData Integration\n228\n1) Merge by Example:\nMerginginnewcases,sometimesknown\nasappendingdata(orinSQL,“unions”)oradding\ndatabyrows(i.e.you’readdingnewrowsofdatato\neachcolumn),assumesthatthevariablesinthetwo\nfilesyou’remergingarethesameinnature.\n\nData Integration\n229\n2) Merging by Variables\nContrary to when you merge new cases, merging in \nnew variables requires the IDs for each case in the \ntwo files to be the same, but thevariable \nnamesshould be different.\nIn this scenario, which is sometimes referred to (in \nSQL, “joins”) or merging databy columns(i.e. you’re \nadding new columns of data to each row).\n\nData Integration\n230\n\nData Integration\n231\n\nOutline\n232\nIntroduction\nBusiness and   Data Understanding\nData Cleaning\nMissing values\nNoisy Data\nInconsistent Data \nData Integration\nData Transformation\nData Reduction\nFeature Selection\nSampling\n\nData Transformation\n233\nGoal \nidentification \nand  Data \nUnderstanding\nData Cleaning Data Integration\nData \nTransformation\nData Reduction\nForms of data preprocessing\n\nData Transformation\n234\nData transformation\nTransform the data into a form appropriate for given data \nmining method\nData is transformed or consolidated into forms appropriate \nfor mining.\nData transformation is the process of changing the \nformat, structure, or values of data.\nMethods include:\nType Conversations\nNormalization\n\nData Transformation\n235\n1) Type Conversion: \nFor example:\n-Numerical to Binominal\n-If value < 100 True else False\n-Numerical to Date\n-2102212 to 11/12/2021\n-Date to Numerical\n-11/12/2021 to 2102212\n-Nominal to Numerical\n-11 to range1[10-20]\n\nData Transformation\n2) Normalization: \nWhere the attributes are scaled so as to fall within a \nsmall specified ranges such as -1.0 to 1.0:\nRange normalization: Perform a liner transformation \non the original data.AAA\nAA\nA\nminnewminnewmaxnew\nminmax\nminv\nv	_)__(\'	\n\n\n \n236\n\nData Transformation\nExample:Supposetheminimumandmaximumvaluesforthe\nattributeincomeare12000$and98000$.Wewouldliketo\nmapincometorange0.0to1.0.Bymin-maxnormalization,\navalueof73600forincomeistransformedto.\n73600−12000\n98000−12000\n(1.0−0)+0=0.84\n237\n\nOutline\n238\nIntroduction\nBusiness and   Data Understanding\nData Cleaning\nMissing values\nNoisy Data\nInconsistent Data \nData Integration\nData Transformation\nData Reduction\nFeature Selection\nSampling\n\nData Reduction\n239\nGoal \nidentification \nand  Data \nUnderstanding\nData Cleaning Data Integration\nData \nTransformation\nData Reduction\nForms of data preprocessing\n\nData Reduction\n240\nWarehouse may store terabytes of data: Complex data \nanalysis/mining may take a very long time to run on the \ncomplete data set\nData reduction \nObtainsreducedrepresentationofthedatasetthat\nismuchsmallerinvolumebutyetproducesthesame\n(oralmostthesame)analyticalresults\n\nData Reduction\n241\nThechoiceofdatarepresentation,andselection,\nreduction,ortransformationoffeaturesisprobably\nthemostimportantissuethatdeterminesthequality\nofadata-miningsolution.\nTherefore,thethreebasicoperationsinadata-\nreductionprocessaredeleteacolumn(feature\nselection)anddeletearow(sampling).\n\nData Reduction\n242\nFeature Selection:\nWewanttochoosefeatures(attributes)\nthatarerelevanttoourdata-mining\napplicationinordertoachievemaximum\nperformancewiththeminimum\nmeasurementandprocessingeffort.\n\nData Reduction: Feature Selection\n243\n1) Redundant features \nDuplicate much or all of the information contained in one or \nmore other attributes\nE.g., purchase price of a product and the amount of sales tax \npaid.\n\nData Reduction: Feature Selection\n244\n2) Irrelevant features\nContain no information that is useful for the data mining task \nat hand\nE.g., students’ ID, name, mobile number  are   irrelevant to the \ntask of predicting students\' GPA\n\nData Reduction : Feature Selection\n245\n3) Remove a field with no or almost no values\ne.g. most of the values are 0.\n4) Examine the number of distinct field values\nRule of thumb: remove a field where almost all values are the same \n(e.g. male).\n\nData Reduction: Feature Selection\n246\n5) Selecting Most Relevant Fields\nIftherearetoomanyfields,selectasubsetthatis\nmostrelevant.\nCanselecttopNfieldsusingsomecomputations.\n\nCorrelation Analysis (Nominal Data)\nFor categorical data, correlation can be \ndiscovered by \n2\n(Chi-Square)test\nThe larger the Χ\n2\nvalue, the more likely the variables are related\n\n\nExpected\nExpectedObserved\n2\n2	)(\n \n247\n\nCorrelation Analysis: Example\n248\n\nCorrelation Analysis: Example\nCalculate if there is a correlation between sex and survival. \n249\n\nCorrelation Analysis: Example\n250\n\nCorrelation Analysis: Example\nComputeExpectedvalue:\nTheexpectednumbers(underthenullhypothesis)\nineachcellareequalto\nThusforthefemalesshouldsurvivecellthe\nexpectednumberis\n463*449/1313=158.13\n251\n\nCorrelation Analysis: Example\n252\n\nData Reduction: Feature Selection\n253\nSomeredundancycanbedetectedbycorrelation\nanalysis(howstronglyoneattributeimpliesthe\nother).\n\nData Reduction: Feature Selection\n254\nUsing Correlation Analysis, we can delete a column if \ntheir is low correlation between the Colum and the class.\nOR\nUsing Correlation Analysis, we can delete a column if \ntheir is high correlation between the Colum and another \ncolumn.\n\nData Reduction: Sampling\n255\nSampling:Obtainingasmallsamplestorepresentthe\nwholedatasetN\nAllowaminingalgorithmtorunincomplexitythatis\npotentiallysub-lineartothesizeofthedata\nKeyprinciple:Choosearepresentativesubsetofthe\ndata.\n\nData Reduction: Sampling \n256\nThekeyprincipleforeffectivesamplingisthefollowing:\nusing a sample will work almost as well as using the entire \ndata sets, if the sample is representative\nAsampleisrepresentativeifithasapproximatelythesame\nproperty(ofinterest)astheoriginalsetofdata\n\nRaw Data\nTypes of Sampling:\n1-Random Sampling without replacement\nOnce an object is selected, it is removed from the population :\nthis better because no replication\nData Reduction: Sampling\n257\n\nData Reduction: Sampling\n258\nRaw Data 	Cluster/Stratified Sample\n2-Stratified sampling: \nPartition the data set, and draw samples from each partition \n(proportionally, i.e., approximately the same percentage \nof the data)\n\nData Reduction: Sampling\n259\n\nReferences\n260\nDATA MINING, Concepts, Models, Methods, and Algorithms \nCh.2,3,4\nIntroduction to data mining and analysis ch.9\nDATA MINING AND MACHINE LEARNING Part 1\nData Preprocessing in Data Mining -A Hands On Guide\nhttps://www.analyticsvidhya.com/blog/2021/08/data-\npreprocessing-in-data-mining-a-hands-on-guide/\nWhat Is Data Preprocessing & What Are The Steps Involved?\nhttps://monkeylearn.com/blog/data-preprocessing/\nData Mining : Topic 3 (Data Preprocessing)\nhttps://www.youtube.com/watch?v=HJPVU6vfct8\n\nData Mining \nChapter 1\nIntroduction\nProf. Alaa El-Halees\n\nOutline\n262\nDIKW Pyramid\nDefinition of Data Mining\nData Mining as an Interdisciplinary field\nData Mining vs. Data Science\nProcess of Data Mining\nData Mining Tasks\nData Mining Applications\nChallenges of Data Mining\nIntroduction to RapidMiner\n\nDIKW Pyramid\n263\nData, Information Knowledge and \nWisdom (DIKW Pyramid)\n\nDIKW Pyramid\n264\nData(RAW)\nDataisarawandunorganizedfactthatrequiredtobeprocessed\ntomakeitmeaningful.\nDataisgivenbysimplesequencesofsignsandsymbolsthathave\nnofurthermeaningbesidestheirsimplepresence.\nDatacomprisesfacts,observations,perceptionsnumbers,\ncharacters,symbols,image,etc.\nEx:Red,123,15.\nImaginethatyouaredrivinginaplaceyouhavenever\nbeen.Ifsomeonesimplysaystoyoutheword“red,”\nthisdatahasnomeaning(data).\n\nDIKW Pyramid\n265\nInformation(Meaning)\nDataorganizedandpresentedinaparticularmanner.\nInformationisasetofdatawhichisprocessedinameaningful\nwayaccordingtothegivenrequirement.Informationis\nprocessed,structured,orpresentedinagivencontexttomake\nitmeaningfulanduseful.\nIncomputer,adatasetmakesinformationfromthedatastored\nwithinit.\nEx:.youarenowwarnedthatatrafficlightisred.Now\nyouhaveapieceofinformation(information).\nEx:Ahmed’sSalary20,000\n\nDIKW Pyramid\n266\nKnowledge(Context,pattern)\nKnowledgeisinformationwithmeaningandcontexttoit.\nKnowledgerepresentsapatternthatconnectsand\ngenerallyprovidesahighlevelofpredictabilityastowhat\nisdescribedorwhatwillhappennext.\nEx:Thenyougetanalertthat,rightinfrontofyou,there\nisatrafficlight,anditisred.\nEx:Ifthehumidityisveryhighandthetemperaturedropssoit\nrains.\n\nDIKW Pyramid\n267\nWisdom(Applied)\nItdemonstrativeofhigh-level“understanding”.\nWisdomusesknowledgefordecisionmaking.\nEx:Nowyouunderstandthatyouneedtobrakeandstopthe\ncarwhenyouseetheredlight.(wisdom).\nEx.Weshouldreducecarbonemissions.\n\nDIKW Pyramid\n268\n\nDIKW Pyramid\n269\nExample\nData:I have one item (picture for example)\nInformation:It’s a tomato.Now, we understand the item and its \ncharacteristics.\nKnowledge:A tomato is a fruit.We can identify patterns in the \ninformation and apply them to the item.\nWisdom:Tomato is never added to a fruit salad.There is an \nunderlying, commonly understood principle that governs the item’s \npurpose.\n\nOutline\n270\nDIKW Pyramid\nDefinition of Data Mining\nData Mining as an Interdisciplinary field\nData Mining vs. Data Science\nProcess of Data Mining\nData Mining Tasks\nData Mining Applications\nChallenges of Data Mining\nIntroduction to RapidMiner\n\nDefinition of Data Mining\n271\nGoal of data mining to go from data and information to \nknowledge.\n\"Computers have promised us a source of wisdom but \ndelivered a flood of data.“\n\"It has been estimated that the amount of information in \nthe world doubles every 20 months.”\n\nDefinition of Data Mining\n272\n\nDefinition of Data Mining\n273\nKnowledgerepresentedaspattern\nPatternisanarrangementofrepeatedparts.\nInadatatable,apatternisdefinedasasetofrowsthat\nsharethesamevaluesintwoormorecolumns.\nConsiderforexample,thefollowingtablethatcontains\ndataaboutobjects;shape,color,andweight.\n\nDefinition of Data Mining\nWeightColorShapeRow #\n100RedBox1->\n200RedBox2->\n300RedBox3->\n400BlueBox4\n400BlueCone5\n274\nInthistable,wehave3rows(row1,2and3)thatsharethesamevalues\nintwocolumns(ShapeandColor).Fromthistable,wecanobservethe\nfollowingpatterns:\nMostBoxesareRed.\nWecanrepresentPatternasrule:\nIfShape=BoxthenColor=Red.\n\nDefinition of Data Mining\n275\nKnowledgediscoveryindatabases(data\nmining)is\nData mining is a collection of techniques for efficient \nautomated discovery of valid, novel, and understandable \npatternsin large databases. \nThe patternsmust be actionableso they may be used in \nan enterprise’s decision making.’\n\nDefinition of Data Mining\n276\nValid:Discovered patterns should be true on new data \nwith some degree of certainty.\nGeneralize to the future (other data). \nNovel:Patterns must be novel (should not be previously \nknown).\n\nDefinition of Data Mining\n277\nUnderstandable:Patternsmustbemadeunderstandablein\nordertofacilitateabetterunderstandingofthe\nunderlyingdata.\nActionable:patternsshouldpotentiallyleadtosomeuseful\nactions.\n\nDefinition of Data Mining\nExample:Credit Risk\n278\n\nDefinition of Data Mining\n279\nIsitvalid?\nThepatternhastobevalidwithrespecttoacertainty\nlevel(ruletrueforthe86%)\nIsitnovel?\nThevaluekshouldbepreviouslyunknownorobvious\nIsituseful?\nThepatternshouldprovideinformationusefultothe\nbankforassessingcreditrisk\nIsitunderstandable?\n\nDefinition of Data Mining \n280\nManypeopletreatdataminingasasynonymforanother\npopularlyusedterm,KnowledgeDiscoveryinDatabases,\norKDD.\nAlternatively,otherviewdataminingassimplyanessential\nstepintheprocessofknowledgediscoveryindatabases.\n\nOutline\n281\nDIKW Pyramid\nDefinition of Data Mining\nData Mining as an Interdisciplinary field\nData Mining vs. Data Science\nProcess of Data Mining\nData Mining Tasks\nData Mining Applications\nChallenges of Data Mining\nIntroduction to RapidMiner\n\nData Mining as an Interdisciplinary \nfield\n282\n“Dataminingisaninterdisciplinaryfieldbringing\ntogethertechniquesfrommachinelearning,\nstatistics,databases,andvisualizationtoaddress\ntheissueofinformationextractionfromlarge\ndatabases”\n\nData Mining as an Interdisciplinary field \n283\nData Mining\nDatabase \nTechnology\nStatistics\nOther\nDisciplines\nMachine\nLearning\nVisualization\n\nData Mining Vs. Statistics\n284\nStatisticsisthecollection,analysis,modeling,\nandpresentationofdata.\nStatisticsisonecomponentofdatamining,\nmeaningitisonetoolinthedatamining’stoolkit.\nHavingknowledgeandunderstandingof\nstatisticswillhelpadataminerbetterunderstand\nthebehind-the-scenesprocessingofmanyofthe\ndata-mining.\n\nData Mining Vs. Statistics\nSomedataminingmethodsusesstatistics\nsuchasnaïvebaysandmaximumentropy\nandalsousedforestimatingprobabilities\nofpredictions.\nDataminingisdifferthanstatisticsinkind\nofdata(notonlynumerical),kindsof\nmethods(mostlyusemachinelearning\nmethods),morethanonehypotheses,\namountofdata(statisticsusessamples)\n285\n\nData Mining vs. Machine Learning\n286\nMachinelearningisdefinedastheuseof\ndatapattern-recognitionalgorithmsthat\nallowaprogramtosolveproblems,suchas\nclustering,categorization,andpredictive\nanalysis,withouttheneedforexplicitstep-\nby-stepprogramminginstructionstotellthe\nalgorithmhowtoperformtasks.\nDataMiningusesmethodsfromMachine\nLearningsuchasdecisiontreeandneuralnets.\n\nData Mining vs. Machine Learning\n287\nMachineLearningusessamplesandDataMininguses\nwholedata.DataMiningcanaccessdatafromdatabase.\nMachineLearningsometimesusedtoreplacehuman\nwhereDataMiningtohelphuman.\nBecausedataminingoften(butnotalways)makes\nextensiveuseofmachinelearning,thetwoterms\noftenappeartogether.\n\nData Mining vs. Databases\n288\nDatabasesPartofDataMiningthatprovidethefastand\nreliableaccesstodata.\nBefore data-mining tools can perform analytic \noperations, the data must reside within an accessible \nstorage location. As the volume of data applications \nmust process continues to grow,so, too, does the \ndemands on the underlying database\nDatabasesusedfordataoperation(Storingandretrieving\ndata),DataMiningforDecisionmaking.\n\nData Mining vs. Visualization \n289\nDatavisualizationisthegraphicalrepresentationof\nthedataandinformationextractedfromdata\nminingusingthevisualelementslikegraph,chart,\nandmaps,datavisualizationtool,andtechniques\nhelpsinanalyzingmassiveamountofinformation\nandmakedecisionontopofit.\n\nData Mining vs. Visualization \n290\n\nOutline\n291\nDIKW Pyramid\nDefinition of Data Mining\nData Mining as an Interdisciplinary field\nData Mining vs. Data Science\nProcess of Data Mining\nData Mining Tasks\nData Mining Applications\nChallenges of Data Mining\nIntroduction to RapidMiner\n\nData Mining vs. Data Science\n292\nDatascienceisabroadfieldthatincludesthe\nprocessesofcapturingofdata,analyzing,andderiving\ninsightsfromit.Ontheotherhand,dataminingis\nmainlyaboutfindingusefulinformationinadataset\nandutilizingthatinformationtouncoverhidden\npatterns.\nDataScienceisafieldofstudywhichincludes\neverythingfromBigDataAnalytics,DataMining,\nPredictiveModeling,DataVisualization,Mathematics,\nandStatistics.\n\nData Mining vs. Data Science\n293\nAnothermaindifferencebetweendatascienceanddata\nminingliesinthetypeofdatausedbytheseprofessionals.\nUsually,datasciencedealswitheverytypeofdata\nwhetherstructured,semi-structured,orunstructured.On\ntheotherhand,dataminingmostlydealswithstructured\ndata.\n\nOutline\n294\nDIKW Pyramid\nDefinition of Data Mining\nData Mining as an Interdisciplinary field\nData Mining vs. Data Science\nProcess of Data Mining\nData Mining Tasks\nData Mining Applications\nChallenges of Data Mining\nIntroduction to RapidMiner\n\nProcess of Data Mining\n295\nDataMiningisessentiallyaprocessofdata\ndriveextractionofnotsoobviousbutuseful\ninformationfromlargedatabases.\nTheentireprocessisinteractiveanditerative.\nNextFiguredepictedtheprocessofKDD.\n\n__\n__\n__\n__\n__\n__\n__\n__\n__\nTransformed \nData \nPatterns\nand \nRules\nTarget \nData\nRaw \nData  \nKnowledgeInterpretation\n& Evaluation\nIntegration\nUnderstanding\nData Mining Process\nDATA\nWare\nhouse\nKnowledge\n296\n\nProcess of Data Mining\n297\n1-Data cleaning:\nReal-worlddatatendstobeincomplete,noisyand\ninconsistent.\nincomplete:lackingattributevalues,lacking\ncertainattributesofinterest,\ne.g.,job=“”(missingdata)\nnoisy:containingnoise,errors,oroutliers\ne.g.,Salary=“−10”(anerror)\ninconsistent:containingdifferenceincodesor\nnames,e.g.,\ne.g.,Age=“10”Birthday=“2007”\n\nProcess of Data Mining\n298\n2-DataIntegration:\nDataintegrationisthemergingofdatafrommultiple\nsources.Thesesourcesmayincludemultipledatabases,\ndatacubes,orflatfiles.\n\nProcess of Data Mining\n299\n3-DataSelection:\nwheredatarelevanttotheanalysistaskareretrievedfrom\nthedatabase.Therefore,irrelevant,weaklyrelevantor\nredundantattributesmaybedetectedandremoved.\n\nProcess of Data Mining \n300\n4-DataTransformation\nwheredataaretransformedormergedintoforms\nappropriateforminingbyperformingsummaryor\naggregationoperation(forexampledailysalesmaybe\naggregatedtomonthlysalesorannualsales).\n\nProcess of Data Mining \n301\n5-Data Mining: \nAnessentialprocesswhereintelligentmethodsareapplied\nondatatocovertittoknowledgeinfordecisionmaking.\nWiderangeofmethodscanbeusedindataminingsuch\nneuralnets,decisiontreeandAssociation.\n\nProcess of Data Mining \n302\n6-pattern (knowledge) evaluation :\nToidentifythetrulyinterestingpatternbasedon\nsomeinterestingnessmeasures.\n\nProcess of Data Mining \n303\n7)KnowledgeRepresentation\nKnowledgecanbeinruleformsuchas:\nIfAandBarepurchasedthenCispurchased\nOrTree\n\nOutline\n304\nDIKW Pyramid\nDefinition of Data Mining\nData Mining as an Interdisciplinary field\nData Mining vs. Data Science\nProcess of Data Mining\nData Mining Tasks\nData Mining Applications\nChallenges of Data Mining\nIntroduction to RapidMiner\n\nData Mining Tasks\n305\nDataminingtasksarethekindofdatapatternsor\nknowledgethatcanbemined.\nDataMiningfunctionalitiesareusedtospecifythekindof\npatternstobefoundinthedataminingtasks.\n\nData Mining Tasks\n306\nIngeneraldataminingtaskscanbeclassifiedintotwo\ncategories:\nDescriptiveminingtaskscharacterizethegeneral\npropertiesofthedata.\nPredictiveminingtasksperformdeductiononthecurrent\ndatainordertomakepredictions.\n\nData Mining Tasks\n307\nMost famous data mining tasks:\n\nData Mining Tasks :Classification\n308\nTheinputdataforpredictivemodelingconsists\noftwotypesofvariables:Firstexplanatory\nvariables,whichdefinetheessentialproperties\nofthedata,andthesecondisonetarget\nvariables,whosevaluesaretobepredicted.\nClassificationisusedtopredicatethevalueof\ndiscretetargetvariable.\n\n\nData Mining Tasks :Classification\n309\n\nData Mining Tasks :Classification\n310\n•Meteorologists use these algorithms to predict \nthe weather conditions according to various \nparameters such as humidity, temperature, etc.\n•Financial institutions use classification \nalgorithms to find defaulters to determine whose \ncards and loans they should approve.\n•A marketing manager at a company needs to \nanalyze a customer with a given profile, who will \nbuy a new computer.\n\nData Mining Tasks :Prediction\n311\nSimilartoclassification,exceptwearetryingto\npredictthevalueofavariable(e.g.amountof\npurchase),ratherthanaclass(e.g.purchaseror\nnon-purchaser(\n\nData Mining Tasks :Prediction \n312\nExamples:\nPredictingsalesamountsofnewproductbasedonadvertising\nspending.\nPredictingwindvelocitiesasafunctionoftemperature,\nhumidity,airpressure,etc.\n\nTime Series Analysis (Forecasting )\n313\nA time-seriesis a set of observations on a quantitativevariable \ncollected over time.\nForecasting: Primary Function is to Predict the Future using (time \nseries related) data we have in hand\nBusinesses are often very interested in forecasting time series \nvariables.\nIn time series analysis, we analyze the past behavior of a \nvariable in order to predict its future behavior.\n\nDemand for Mercedes E Class\nTime\nJan Fe\nb \nMarAprMayJunJulAug\nActual demand (past sales)\nPredicted demand\nWe try to predict the \nfuture by looking back \nat the past\nPredicted \ndemand \nlooking \nback six \nmonths\nTIME SERIES ANALYSIS (FORECASTING )\n314\n\nTime Series Analysis (Forecasting )\n315\nExamples:\nDow Jones Industrial Averages\nHistorical data on sales, inventory, customer \ncounts, interest rates, costs, etc\nMeteorology: weather variables, like temperature, \npressure, wind. \nMarketing: activity of business, sales. \nIndustry: electric load, power consumption, voltage, \nsensors.\n\nData Mining Tasks :Association\nAssociationrulelearningisamethodfor\ndiscoveringinterestingrelationsbetweenvariablesin\nlargedatabases.Itisintendedtoidentifystrongrules\ndiscoveredindatabasesusingdifferentmeasuresof\ninterestingness.\nSeektoproduceasetofrulesdescribingthesetof\nfeaturesthatarestronglyrelatedtoeachothers.\n316\n\n317\nExample\nAssociation rulesfrom the itemset:\nClothes Milk, Chicken[sup = 3/7, conf = 3/3]\n…	…\nClothes, ChickenMilk, [sup = 3/7, conf = 3/3]\nt1:Beef, Chicken, Milk\nt2:Beef, Cheese\nt3:Cheese, Boots\nt4:Beef, Chicken, Cheese\nt5:Beef, Chicken, Clothes, Cheese, Milk\nt6:Chicken, Clothes, Milk\nt7:Chicken, Milk, Clothes\nDATA MINING TASKS\n:ASSOCIATION\n\nData Mining Tasks :Clustering\n318\nFindsgroupsofdatapointes(clusters)sothatdatapoints\nthatbelongtooneclusteraremoresimilartoeachother\nthantodatapointsbelongingtodifferentcluster.\n\nData Mining Tasks :Clustering\n319\nFollowing figure is an example of finding clusters of US \npopulation based on their income and debt .\n\nData Mining Tasks :Clustering\n320\nExample:\nClustering can also help marketers discover distinct \ngroups in their customer base. And they can \ncharacterize their customer groups based on the \npurchasing patterns.\nInsurance: Identifying groups of motor insurance \npolicy holders with a high average claim cost.\nClustering in Data Mining helps in the classification \nof animals and plants are done using similar \nfunctions or genes in the field of biology.\n\nData Mining Tasks: Outlier Analysis\nDiscovers data points that are significantly different than \nthe rest of the data. Such points are known as anomalies\nor outliers.\nApplications:\nCredit Card Fraud Detection\n321\n\nOutline\n322\nDIKW Pyramid\nDefinition of Data Mining\nData Mining as an Interdisciplinary field\nData Mining vs. Data Science\nProcess of Data Mining\nData Mining Tasks\nData Mining Applications\nChallenges of Data Mining\nIntroduction to RapidMiner\n\nData Mining Applications\nsome of the major applications of data mining:\n1) Financial Analysis\nThebankingandfinanceindustryreliesonhigh-\nquality,reliabledata.Inloanmarkets,financialand\nuserdatacanbeusedforavarietyofpurposes,like\npredictingloanpaymentsanddeterminingcredit\nratings.\n.\n323\n\nData Mining Applications\n2)TelecommunicationIndustry\nDataminingcanenablekeyindustryplayersto\nimprovetheirservicequalitytostayaheadofthe\nothers.\nTechniqueslikeoutlieranalysiscandetect\nfraudulentusers.Also,canhelpcompare\ninformation,suchasusergroupbehavior,profit,\ndatatraffic,systemoverloads,etc.\n324\n\nData Mining Applications\n3)IntrusionDetection\nNetworkresourcescanfacethreatsandactionsthatintrude\nontheirconfidentialityorintegrity.\nTherefore,detectionofintrusionhasemergedasacrucial\ndataminingpractice.\n4)RetailIndustry\nDataanalysishelpsdealwithdatarelatedtodifferenttypes\nofcustomers,products,regions,andtimezones.\nOnlineretailerscanalsorecommendproductstodrivemore\nsalesrevenueandanalyzetheeffectivenessoftheir\npromotionalcampaigns.\nSupermarkets,usejointpurchasingpatternstoidentify\nproductassociationsanddecidehowtoplacetheminthe\naislesandontheshelves.\n325\n\nData Mining Applications\n5) Higher Education\nInstitutionscanusedataminingtopredictwhich\nstudentswouldenrollinaparticularprogram,who\nwouldrequireadditionalassistancetograduate,refining\nenrollmentmanagementoverall.\n6) Energy Industry\ndata mining can also achieve productive gains by \npredicting power outputs and the clearing price of \nelectricity.\n326\n\nData Mining Applications\n7) Medical Data Mining\nHavingallofthepatient\'sinformation,suchas\nmedicalrecords,physicalexaminations,and\ntreatmentpatterns,allowsmoreeffectivetreatments\ntobeprescribed\nItenablesmoreeffective,efficientandcost-\neffectivemanagementofhealthresourcesby\nidentifyingrisks,predictingillnessesincertain\nsegmentsofthepopulationorforecastingthelength\nofhospitaladmission.\n327\n\nData Mining Applications\n8)CriminalInvestigation\nTheidentificationandcrime-machiningprocess\nwouldtakeplacebydiscoveringpatternsinmassive\nstoresofdata.\n9)E-Commerce\nE-commercecompaniesarelikeAmazon,Flipkart,\nMyntra,etc.Theyusedataminingtechniquestoseethe\nfunctionalityofeveryproductinsuchawaythat“which\nproductisviewedmostbythecustomeralsowhatthey\nalsolikedother”.\n328\n\nData Mining Applications\n10) Insurance\nData mining methods help in forecasting the \ncustomers who buy the policies, analyze the \nmedical claims that are used together, find out \nfraudulent behaviors and risky customers\n329\n\nOutline\n330\nDIKW Pyramid\nDefinition of Data Mining\nData Mining as an Interdisciplinary field\nData Mining vs. Data Science\nProcess of Data Mining\nData Mining Tasks\nData Mining Applications\nChallenges of Data Mining\nIntroduction to RapidMiner\n\nChallenges of Data Mining \n331\nScalability:Toeffectivelyextractinformation\nfromahugeamountofdata,datamining\nalgorithmsmustbeefficientandscalable.\nDimensionality:Manyapplicationsmayinvolves\nalargenumberofdimensions(e.g.featuresor\nattributesofdata)\n\nChallenges of Data Mining\n332\nHeterogeneousandComplexData:Inrecentyears\ncomplicateddatatypessuchasgraph-based,text-free,\nmultimediaandstructureddatatypesareintroduced.\nTechniquesdevelopedfordataminingmustbeableto\nhandletheheterogeneityofthedata.\nDataQuality:Manydatasetsareimperfectdueto\npresentofmissingvaluesandnoiseunthedata.To\nhandletheimperfection,robustdatamining\nalgorithmsmustbedeveloped.\n\nChallenges of Data Mining\n333\nDataDistribution:Asthevolumeofdataincreases,itisno\nlongerpossibleorsafetokeepallthedatainthesameplace.\nAsaresult,theneedfordistributeddataminingtechniques\nhasincreasedovertheyears.\nPrivacyPreservation:Whileprivacyintendstopreventthe\ndisclosureofinformation,dataminingattemptstorevel\ninterestingknowledgeaboutdata.Asaresult,thereisgrowing\ninterestindevelopingprivacy-preservingdatamining\nalgorithms.\n\nOutline\n334\nDIKW Pyramid\nDefinition of Data Mining\nData Mining as an Interdisciplinary field\nData Mining vs. Data Science\nProcess of Data Mining\nData Mining Tasks\nData Mining Applications\nChallenges of Data Mining\nIntroduction to RapidMiner\n\nIntroduction to RapidMiner\n335\nDownload\nhttps://rapidminer.com/get-started/\nIntroduction\nhttps://www.youtube.com/watch?v=Gg01mmR3j-g\n\nReferences\n336\nDATA MINING, Concepts, Models, Methods, and \nAlgorithms Ch. 1\nPrinciples of Data Mining Ch. 1\nData Mining and Data Warehousing Ch. 2\nDifference between Information and Data\nhttps://www.guru99.com/difference-information-\ndata.html.\nWhat is Data Mining\nhttps://www.youtube.com/watch?v=81bm2OsEzbg\n12 Most Useful Data Mining Applications of 2022\nhttps://www.upgrad.com/blog/12-most-useful-data-\nmining-applications-of-2020/', 7375, 1, '2025-06-04 10:22:24', '2025-06-04 10:22:24');
INSERT INTO `documents` (`id`, `title`, `filename`, `file_hash`, `filepath`, `content`, `size`, `category_id`, `created_at`, `updated_at`) VALUES
(71, 'مستند 80 تجريبي', 'Lecture4-BasicClassification.pdf', NULL, 'documents/ee81bzrG5ThjxfE22e2OdhuYcsr3oUTHCRxGZFLz.pdf', 'Given a collection of records (training set )\n›Each record contains a set of attributes, one of the \nattributes is (always) the class.\nFind a modelfor class attribute as a \nfunction of the values of other attributes.\nGoal: previously unseenrecords should be \nassigned a class as accurately as possible.\n›A test setis used to determine the accuracy of the \nmodel. Usually, the given data set is divided into \ntraining and test sets, with training set used to build \nthe model and test set used to validate it.\n\nTidRefundMarital\nStatus\nTaxable\nIncomeCheat\n1Yes Single125K No\n2No Married100K No\n3No Single70K No\n4Yes Married120K No\n5No Divorced95K Yes\n6No Married60K No\n7Yes Divorced220K No\n8No Single85K Yes\n9No Married75K No\n10No Single90K Yes\n10 Goal: Try to guess value of \ncheat (the “class”) based on \nthe other values of that record\nOne record\nGoal: Find function f() such that: \nf (Refund, Marital Status, Taxable Income) = Cheat\nHere, class = cheat \nattribute of records.\nIt’s called classification \nbecause we try to put each \nrecord in one class/category \n(“yes”, “no”)\nExample of classification\n\nPrerequisites for classification\n›Must haveclass attribute\n›Class attribute MUST BE discrete\nI.e. set of values of class attribute countable, \nfixed and known beforehand.\n›Other attributes of records (except class \nattribute) can be anything: discrete and/or \ncontinuous.\n\nTidRefundMarital\nStatus\nTaxable\nIncomeCheat\n1Yes Single125K No\n2No Married100K No\n3No Single70K No\n4Yes Married120K No\n5No Divorced95K Yes\n6No Married60K No\n7Yes Divorced220K No\n8No Single85K Yes\n9No Married75K No\n10No Single90K Yes\n10 One record\nNOTE: \nWhen class is a continuous \nattribute,then such tasks are called \nregression (which you probably \nknow but we won’t deal with this). \nBasic difference between \nclassification and regression .\nAgain, here we’re interested only in \nsituations where class is a discrete \nattribute (=classification).\n“Class” also mentioned as: \ncategories, category labels, \nlabels.\n\nFormal definition of the classification \nproblem\n“Classificationisthetaskof\nlearningatarget-functionf,that\nmapseachattributesetxtoone\nofthepredefinedclasslabelsy.”\nTarget-function falso known as \nclassification model or simply model.\n\nDescriptive modeling\n›Used as an explanatory tool for distinguishing \nobjects in different categories\nE.g. for biologists to explain how a mammal, bird, \nfish is defined based on some characteristics\nPredictive modeling\n›Used as a tool to predict the label of a class of \nunknown objects\nE.g. predict whether or not customers will be \ndefaulting on loans or not\n\nBest\n›When class attribute is binary(i.e. only has only \ntwo distinct values) or is nominal\nNot so good\n›When class attribute is ordinal(=values can be \nordered e.g. Small, Medium, Large, XLarge)\nClassification does not take into consideration the \nordering that ordinals imply –be careful \n›When class attribute resembles hierarchy \n(category/subcategory)\nOur focus: class attribute is binary or \nnominal\n\nPredicting tumor cells as benignor \nmalignant\nClassifying credit card transactions \nas legitimateor fraudulent\nClassifying secondary structures of \nprotein \nas alpha-helix, beta-sheet, or \nrandom\ncoil\nCategorizing news stories as \nfinance, \nweather, entertainment, sports, etc\n\nDecision Tree based Methods\nk-nearest neighbors\nRule-based Methods\nMemory based reasoning\nNeural Networks\nNaïve Bayesand Bayesian Belief \nNetworks\nSupport Vector Machines (SVD)\n\nApply \nModel Induction\nDeductionLearn \nModel \nModelTid Attrib1 Attrib2 Attrib3 Class \n1 Yes Large 125K No \n2 No Medium 100K No \n3 No Small 70K No \n4 Yes Medium 120K No \n5 No Large 95K Yes \n6 No Medium 60K No \n7 Yes Large 220K No \n8 No Small 85K Yes \n9 No Medium 75K No \n10 No Small 90K Yes \n10 \n  Tid Attrib1 Attrib2 Attrib3 Class \n11 No Small 55K ? \n12 Yes Medium 80K ? \n13 Yes Large 110K ? \n14 No Small 95K ? \n15 No Large 67K ? \n10 \n  \nTest Set\nLearning\nalgorithm\nTraining Set In training set: \nclass of each \nrecord already \nknown and \ncorrect and this \ncreates the \nmodel!\nIn testing set: \nclass of each \nrecord unknown \nand the goal is to \nfind it by applying \nthe model.\n\nThe goal is to find a model that assigns to \neach record the correct class\nHowever, errors may occur\n›What is an “Error”?Model puts record in class j\nwhen it in reality, it belongs to class k.\nConfusion matrix tells us the performance\nPredicted classby model\nClass 1 Class 0\nTrue class it \nbelongsto\nClass 1 f\n11 f\n10\nClass 0 f\n01 f\n00\nConfusion matrix: \nf11 tells us the \nnumber of class 1 \nitems that have \nbeen put by \nalgorithm in class \n1 . \nIn general: fij= \nnumber of items \nin class i\npredicted by \nmodel as class j.\n\nMetrics based on confusion matrix\n›Accuracy\nPct of correctly identified classes\n›Error rate\nPct of incorrectly identified classes\n\nWhat is a decision tree?\n›A hierarchical structure, with nodes and edges, \nwhich recursively partitions the data (records) \ninto classes, by examining the values of its \nattributes. \n›Build classification of regression models in the \nform of a tree (hierarchical structure)\n›Decision trees are models\nBuilt by the training set to determine the structure\nUsed by the testing set to assign records into a class\n›The basic idea: perform a series of \nsteps/comparisonsin order to reach a \nconclusion (=i.e. class). The decision tree tells you \nwhich comparisons to make.\n\nTidRefundMarital\nStatus\nTaxable\nIncomeCheat\n1Yes Single125K No\n2No Married100K No\n3No Single70K No\n4Yes Married120K No\n5No Divorced95K Yes\n6No Married60K No\n7Yes Divorced220K No\n8No Single85K Yes\n9No Married75K No\n10No Single90K Yes\n10 Refund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarried\nSingle, Divorced\n< 80K > 80K\nSplitting Attributes\nTraining Data	Model:  Decision Tree\nBuilds\nNOTE: There could be more than one tree that fits the same data!\n\nTypes of nodes in decision trees\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarried\nSingle, Divorced\n< 80K	> 80K\nRoot node: Has no \nincoming edges and 0 or \nmore outgoing edges\nInternal node: Has exactly \none incoming edge and 2  \nor more outgoing edges\nLeafs or terminal nodes: Has \nexactly one incoming edge \nand 0 outgoing edges\n\nIn decision trees, each leaf/terminal node is \nassigned a class label (i.e. one value of the \nclass attribute)\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarried\nSingle, Divorced\n< 80K	> 80K\nLeaf or terminal node: \nAssigned one value of class \nattribute (in each leaf one \nof the 2 values of cheat: \n{YES, NO} )\n\nNon-terminal nodes (i.e. root and \ninternal nodes) contain test conditions \non the record’s attributes.\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarried\nSingle, Divorced\n< 80K	> 80K\nRoot/internal node: Test \ncondition on some attribute \nof data. \nRoot applies test condition \non attribute Refund\nApplies test condition on \nattribute Marital Status\nApplies test \ncondition on \nattribute \nTaxable \nincome\n\nEdges have labels, indicating the values \nof the test condition\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarried\nSingle, Divorced\n< 80K	> 80K\nPossible value of attribute \n“Refund”.  Here, value of \nRefund is “No”.\nPossible value of \nattribute “Refund”.  \nHere, value of \nRefund is “Yes”.\nPossible value of \nattribute “Marital \nStatus”.  Here, value of \nMarital status is \n“Married”.\nPossible value of \nattribute “Marital \nStatus”.  Here, value of \nMarital status is Single \nor Divorced.\nPossible value of \nattribute “Taxable \nIncome”.  Here, value \nof taxable income is > \n80K.\n\nDecision trees (=the model) are built by \nusing the training data\n›Where the class is already known and \ncorrect \nDecision trees (=the model) are used by \ntesting and unknown sets to classify the \ndata\n\nApply \nModel Induction\nDeductionLearn \nModel \nModelTid Attrib1 Attrib2 Attrib3 Class \n1 Yes Large 125K No \n2 No Medium 100K No \n3 No Small 70K No \n4 Yes Medium 120K No \n5 No Large 95K Yes \n6 No Medium 60K No \n7 Yes Large 220K No \n8 No Small 85K Yes \n9 No Medium 75K No \n10 No Small 90K Yes \n10 \n  Tid Attrib1 Attrib2 Attrib3 Class \n11 No Small 55K ? \n12 Yes Medium 80K ? \n13 Yes Large 110K ? \n14 No Small 95K ? \n15 No Large 67K ? \n10 \n  \nTest Set\nTree\nInduction\nalgorithm\nTraining Set Decision \nTree\n\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarriedSingle, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\nStart from the root of tree and \napply conditions to record of \ntest data.\nWe’ve built this \ndecision tree (model) \nfrom the training \ndata.\nOur goal: Try to \npredict value of \nCheat for this \nparticular record.\n\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarriedSingle, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\n\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarriedSingle, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\n\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarriedSingle, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\n\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarried Single, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\n\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarried Single, Divorced\n< 80K	> 80KRefund Marital \nStatus \nTaxable \nIncome Cheat \nNo Married 80K ? \n10 \n  \nTest Data\nReached leaf node \nwhich contains a \nclass.\nAssign value of leaf \nnode to cheat i.e. \nassign cheat to \n“No” . Since record \nhas been assigned \nto class (“No”), \nterminate.\nNo\n\nApply \nModel Induction\nDeductionLearn \nModel \nModelTid Attrib1 Attrib2 Attrib3 Class \n1 Yes Large 125K No \n2 No Medium 100K No \n3 No Small 70K No \n4 Yes Medium 120K No \n5 No Large 95K Yes \n6 No Medium 60K No \n7 Yes Large 220K No \n8 No Small 85K Yes \n9 No Medium 75K No \n10 No Small 90K Yes \n10 \n  Tid Attrib1 Attrib2 Attrib3 Class \n11 No Small 55K ? \n12 Yes Medium 80K ? \n13 Yes Large 110K ? \n14 No Small 95K ? \n15 No Large 67K ? \n10 \n  \nTest Set\nTree\nInduction\nalgorithm\nTraining Set Decision \nTree\nEach step/test \ncondition is \ndetermined from \ntraining set\nInterested in \nthis!\n\nThe problem again!TidRefundMarital\nStatus\nTaxable\nIncomeCheat\n1Yes Single125K No\n2No Married100K No\n3No Single70K No\n4Yes Married120K No\n5No Divorced95K Yes\n6No Married60K No\n7Yes Divorced220K No\n8No Single85K Yes\n9No Married75K No\n10No Single90K Yes\n10 \nTraining Data\nYou start with this, AND NO \nDECISION TREE AT ALL\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes	No\nMarried\nSingle, Divorced\n< 80K > 80K\nSplitting Attributes\nModel:  Decision Tree\nNeed to \nconstruct THIS\n\nBuilding DecisionTree based on training \nset using Tree Induction\nMany Algorithms :\n›Hunt’s algorithm (one of the earliest)\n›CART\n›ID3, C4.5\n›SLIQ,SPRINT\n\nLet D\ntbe the set of training \nrecords that reach a node t\nGeneral Procedure:\n›If D\ntcontains records that \nbelong the same class y\nt, \nthen t is a leaf node labeled \nas y\nt\n›If D\ntis an empty set, then t is \na leaf node labeled by the \ndefault class, y\nd\n›If D\ntcontains records that \nbelong to more than one \nclass (note: this is the \ntraining set and we know \nthe class), use an attribute \ntest to split the data into \nsmaller subsets. Recursively\napply the procedure to \neach subset.Tid Refund Marital \nStatus \nTaxable \nIncome Cheat \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n  \nD\nt\n?\nTraining Data\n\nTidRefundMarital\nStatus\nTaxable\nIncomeCheat\n1Yes Single125K No\n2No Married100K No\n3No Single70K No\n4Yes Married120K No\n5No Divorced95K Yes\n6No Married60K No\n7Yes Divorced220K No\n8No Single85K Yes\n9No Married75K No\n10No Single90K Yes\n10 Refund\nDon’t \nCheat\nYes No\nMarital\nStatus\nDon’t \nCheat\nCheat\nSingle,\nDivorced\nMarried\nDon’t \nCheat\nRefund\nDon’t \nCheat\nDon’t \nCheat\nYes No\nRefund\nDon’t \nCheat\nYes No\nMarital\nStatus\nDon’t \nCheat\nCheat\nSingle,\nDivorced\nMarried\nTaxable\nIncome\nDon’t \nCheat\n< 80K >= 80K\nSame class? No. \nFind attribute to \nsplit. Choose \n“Refund”\nSplit on “Refund”\nAll records with \nRefund=no. Same \nclass? No. \nChoose attribute \nto split.\nAll records with \nRefund=Yes. All in \nsame class? YES! \nNo need to split. \nAssign class\n\nGreedy strategy (greedy algorithms)\n›Split the records of the training set based on \nan attribute that optimizes *now* a certain \ncriterion\nSome serious issues though\n›How to split the records?\nHow to specify attribute test condition for non-\nterminal nodes?\nWhich attribute to select, i.e. how to determine \nbest split?\n›When to stop splitting?\n\nDepends on attribute types of records. \nCan be:\n›Nominal\n›Ordinal\n›Continuous\nDepends on number of ways to split\n›2-way split\n›Multi-way split\n\nMulti-way split: Use as many partitions as \ndistinct values. E.g.\nBinary split:  Divides values into two subsets. \nNeed to find optimal partitioning.                                        \nE.g.\nCarType\nFamily\nSports\nLuxury\nCarType\n{Family, \nLuxury}\n{Sports}\nCarType\n{Sports, \nLuxury}\n{Family} OR\n\nMulti-way split: Use as many partitions as \ndistinct values. \nBinary split:  Divides values into two subsets. \nNeed to find optimal partitioning.\nWhat about this split?\nSize\nSmall\nMedium\nLarge\nSize\n{Medium, \nLarge}\n{Small}\nSize\n{Small, \nMedium}\n{Large}\nOR\nSize\n{Small, \nLarge}\n{Medium}\nNOPE! Does not \npreserve order. \nREMEMBER: \nOrdinals are \nabout order\n\nDifferent ways of handling\n›Discretizationto form an ordinal categorical \nattribute\nStatic –discretize once at the beginning\nDynamic –ranges can be found by equal\ninterval bucketing, equal frequency \nbucketing (percentiles) or clustering.\n›Binary Decision: (A < v) or (A v)\nconsider all possible splits and finds the best cut \ncan be more computational intensive\n\nExamples of continuous attributes\nTaxable\nIncome\n> 80K?\nYes No\nTaxable\nIncome?\n(i) Binary split	(ii) Multi-way split\n< 10K\n[10K,25K)[25K,50K)[50K,80K)\n> 80K\n\nHow to determine which attribute to \nselect for split i.e. determine best split?\n›Is it possible to somehow measure the \n“goodness”/qualityof a split based on some \nattribute?\nIf yes, select the split with the best quality\n(Answer: YES, there are measures.)\n\nIntuitive observations in order to come \nup with a measure\nAssume before splitting:10 records of class 0,\n10 records of class 1\nOwn\nCar?\nC0: 6\nC1: 4\nC0: 4\nC1: 6\nC0: 1\nC1: 3\nC0: 8\nC1: 0\nC0: 1\nC1: 7\nCar\nType?\nC0: 1\nC1: 0\nC0: 1\nC1: 0\nC0: 0\nC1: 1\nStudent\nID?\n...\nYes No Family\nSports\nLuxury	c\n1\nc\n10\nc\n20\nC0: 0\nC1: 1\n...\nc\n11\nThis means: If splitting based on attribute \n“Own car”, then out of all records with \nvalue “Yes” in “Own Car”, 6 belong in \nclass 0 and 4 in class 1.\nQ: Which attribute (Own Car, Car Type, \nStudent ID) is better? i.e. Which split is \nbetter? ANSWER: Car Type.\nSplit all records \non “Own Car” \nattribute\nSplit all records \non “Car Type” \nattribute\nSplit all records \non “Student ID” \nattribute\n\nAll algorithms have greedyand divide and \nconquerapproach: \n›Nodes with homogeneousclass distribution are \npreferred\nHomogeneous?Low impurity, low intermixing of records \nbelonging to different classes, records to belong mostly \nto one class  \nNeed a measure of node impurity:C0: 5\nC1: 5 C0: 9\nC1: 1 \nNon-homogeneous, \nHigh degree of impurity\nHomogeneous, Low \ndegree of impurity\nSplit records on \nattribute A\nSplit (same) records on \nattribute B\nThis is better!\n\nThree measures of node impurity\n›Gini index\n›Entropy\n›Misclassification error\nDifferent algorithms use different node \nimpurity measures. E.g.\n›ID3, C4.5 uses Entropy\n›Hunt, CART, SLIQ, SPRINT uses Gini index\n\nGini index for a given node t\n…where p(j|t)the relative frequency of class j at node t \n(Note: each node may contain records from any class)\nObservations:\n›Maximum = 1 –1/n\ncwhen records of node t are distributed \nequally among classes. Greatest impurity \n›Minimum = 0.0 when all records of node t belong to only \none class. Smallest (no) impurity.\n›Lower values are better/preferred!\n›Understanding Giniindex? Measures how often \n(=probability) a randomly chosen record would be placed \nin the incorrect class.\n\nExamples: calculating the Gini index for \nvarious nodes\nP(C1) = 0/6 = 0     P(C2) = 6/6 = 1 (rel. fr/prob.)\nGini = 1 –P(C1)\n2 \n–P(C2)\n2\n= 1 –0 –1 = 0 \nClass 1 (C1) 0\nClass 2 (C2) 6\nA node with: 0 rec in class \n0, 6 rec in class 1\nClass 1 (C1) 1\nClass 2 (C2) 5\nP(C1) = 1/6          P(C2) = 5/6\nGini = 1 –(1/6)\n2 \n–(5/6)\n2\n= 0.278\nClass 1 (C1) 2\nClass 2 (C2) 4\nP(C1) = 2/6          P(C2) = 4/6\nGini = 1 –(2/6)\n2 \n–(4/6)\n2\n= 0.444\n\nCalculate Gini Index for splits(also referred \nto as “Gini Index of attribute”)\nAssume a node pis split (based on \nattribute) into k partitions (“children”) then \nGini Index of split:\n…where n\ni= number of records at node i\nand n = number of records at node p\n\nExample: assume binary split. What is the Gini index \nof this split?\nB?\nYes	No\nNode N1 Node N2\nGini(N1) = 1 –(5/7)\n2 \n–(2/7)\n2\n= 0.408 \nGini(N2) = 1 –(1/5)\n2 \n–(4/5)\n2\n= 0.320\nGini(Split or B) = 7/12 * Gini(N1) \n+  5/12 * Gini(N2) = 7/12 * 0.408 \n+ 5/12 * 0.320 = 0.413 QED\nN1\nC1 5\nC2 2\nN2\nC1 1\nC2 4\nB? \n(parent)\nC1 6\nC2 6\nGini index of \neach of the 2 \nnodes (N1, N2)\n\nExamplesTid Refund Marital \nStatus \nTaxable \nIncome Cheat \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n  \nCompute Gini index for “Refund” \nattribute (i.e. compute Gini index of split \nbased on “Refund”). Cheat is class!\nIf we split based on “Refund” we get:\nRefund\nYes	No\nCheat=\nYes\n0\nCheat=\nNo\n3\nCheat=\nYes\n3\nCheat=\nNo\n4\n3 out of 10 rec. 7out of 10 rec.\nCheat=\nYes\n3\nCheat=\nNo\n7\nTotal 10 rec\nGini(Left node) = 1-(0/3)\n2\n-(3/3)\n2\n= 0\nGini(Right node) = 1 –(3/7)\n2\n–(4/7)\n2\n= 0.489  \nGini(Refund) = (3/10) * 0 + \n(7/10) * 0.489 = 0.3423 QED\n\nGini gain\n›The difference between parent node’s Gini \nindexand Gini index of split:\nGini\ngain= Gini\nParent-Gini\nSplit\n›Measures how Giniindex improves\n›Goal:maximize gain, i.e. this difference, which \ndetermines which attribute to select for splitting \nin this step.\n›Important:Used in algorithms to select attributes \nand build decision trees.\n\nB?\nYes	No\nNode N3 Node N4\nA?\nYes	No\nNode N1 Node N2\nBefore Splitting:	M0\nM1	M2	M3	M4\nM12	M34\nGain = M0 –M12 vs  M0 –M34 => choose attribute with biggest gain!\nC0 N00\nC1 N01\nC0 N10\nC1 N11\nC0 N20\nC1 N21\nC0 N30\nC1 N31\nC0 N40\nC1 N41\nSplit on A or B \nattribute???\n\nExampleTid Refund Marital \nStatus \nTaxable \nIncome Cheat \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n  \nCompute Gini gain when splitting on attribute \n“Refund”. Cheat is class!\nGini gain(Refund) = Gini of entire dataset –\nGini(Refund)\nWe have already calculated Gini of attribute \n“Refund” (=0.3423). \nGini of our entire data set is also called “Gini \nof overall collection of training examples” or \n“Gini of system”:\nGini(training set) = 1-(3/10)\n2\n–(7/10)\n2\n= 0.42\nGini gain(Refund) = 0.42 –0.3423 = 0.0777QED\n\nUsing Gini and Gini gain to build decision \ntrees\nID M N Q R\n1 M1 N3 Q2 R2\n2 M2 N3 Q2 R1\n3 M2 N2 Q1 R1\n4 M1 N2 Q1 R2\n5 M2 N1 Q3 R2\n6 M1 N1 Q3 R2\nTraining Data\nSTEP 1:Compute Gini index for our entire \ndataset: \nGini = 1 –(2/6)\n2\n–(4/6)\n2\n= 0.444\nSTEP 2:Compute Gini index for each attribute \n(Split):\nGini(M) = (3/6)*0 + (3/6)*0.444 = 0.222\nGini(N) =  (2/6)*0 + (2/6)*0.5+(2/6)*0.5=0.333\nGini(Q) = (2/6)*0.5 + (2/6)*0.5 + (2/6)*0 = 0.333\nSTEP 3:Calculate Ginigains for each attribute:\nGini Gain(M) = 0.444 –0.222 = 0.222 (biggest!)\nGini Gain(N) = 0.444 –0.333 = 0.111\nGini Gain(Q) = 0.444 –0.333 = 0.111\nHence, first split based on attribute M\n\nID M N Q R\n1 M1 N3 Q2 R2\n2 M2 N3 Q2 R1\n3 M2 N2 Q1 R1\n4 M1 N2 Q1 R2\n5 M2 N1 Q3 R2\n6 M1 N1 Q3 R2\nTraining Data\nM\nM1	M2\nR2	Node N2\nIf M has value M1, then R is \nalways R2. Homogeneous \nnode! Hence no further \nsplitting and make this a \nleaf. \nWe make a rule:\nM1 R2\nR1 2\nR2 1\nHere, non-homogeneous \nnode. Apply same method \nto split based on other \nattribute. Tree will grow at \nthis branch.\nRoot node of tree will be \ntest on attribute M\n\nID M N Q R\n1 M1 N3 Q2 R2\n2 M2 N3 Q2 R1\n3 M2 N2 Q1 R1\n4 M1 N2 Q1 R2\n5 M2 N1 Q3 R2\n6 M1 N1 Q3 R2\nTraining Data\nCalculate again with the same steps.\nSTEP 1:Compute Gini index for our new \n“entire” dataset (leave out red-ishrows. \nThese have already been classified): \nGini = 1 –(2/3)\n2\n–(1/3)\n2\n= 0.444\nSTEP 2:Compute Gini index for each \nattribute:\nGini(N) =  (1/3)*0 + (1/3)*0+(1/3)*0 = 0\nGini(Q) = (1/3)*0 + (1/3)*0 + (1/3)*0 = 0\nSTEP 3:Calculate Gini gains:\nGini Gain(N) = 0.444 –0.0 = 0.444\nGini Gain(Q) = 0.444 –0.0 = 0.444\nEqual Gini gains. Hence, both are equally \ngood. choose one of N, Q. Assume we \nchoose N (can also choose Q).\n\nID M N Q R\n1 M1 N3 Q2 R2\n2 M2 N3 Q2 R1\n3 M2 N2 Q1 R1\n4 M1 N2 Q1 R2\n5 M2 N1 Q3 R2\n6 M1 N1 Q3 R2\nTraining Data\nM\nM1	M2\nR2	N\nHomogeneous nodes. Hence, \nno need to split further. Make \nthem leaves with proper class \nvalues (from nodes) and devise \nrules (include incoming M2):\nM2, N1 R2\nM2, N2 R1\nM2, N3 R1\nDataset empty, hence done. \nDecision tree built.\nN1\nN2\nN3\nR2\nR1\nR1\n\nFor each distinct value, gather counts for each \nclass in the dataset\nUse the count matrix to make decisions CarType \n {Sports, \nLuxury} \n{Family} \nC1 3 1 \nC2 2 4 \nGini 0.400 \n \n   CarType \n \n{Sports}  \n{Family,\nLuxury} \nC1 2 2 \nC2 1 5 \nGini 0.419 \n \n   CarType \n Family Sports Luxury \nC1 1 2 1 \nC2 4 1 1 \nGini 0.393 \n \n  \nMulti-way split	Two-way split \n(find best partition of values)\nNote: for k categorical values you need \nto check (2\nk\n–2)/2 = 2\nk-1\n-1 partitions\nGini(CarType)\n\nUse Binary Decisions based on one \nvalue\nSeveral Choices for the splitting \nvalue\n›Number of possible splitting \nvalues  = Number of distinct \nvalues\nEach splitting value has a count \nmatrix associated with it\n›Class counts in each of the \npartitions, A < v and A v\nSimple method to choose best v\n›For each v, scan the database to \ngather count matrix and \ncompute its Giniindex\n›Computationally Inefficient! \nComplexity: O(n\n2\n) computing \nGini. \nRepetition of work.Tid Refund Marital \nStatus \nTaxable \nIncome Cheat \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n  Taxable\nIncome\n> 80K?\nYes No\n\nHow to improve speed?\nFor efficient computation: for each attribute,\n›Sort the attribute on values. Complexity O(nlogn) =>better!\n›Linearly scan these values, each time updating the count \nmatrix and computing Giniindex. Some optimization tricks:\nChoose split points at midpoint between values\nIdentify adjacent examples that differ in their target (class) labels \nand attribute values => Set of candidate splits\n›Calculate GiniIndex and choose the split position that has \nthe least giniindexCheat No No No Yes Yes Yes No No No No\nTaxable Income\n60 70 75 85 90 95 100 120 125 220\n55 65 72 80 87 92 97 110 122 172 230\n<=><=><=><=><=><=><=><=><=><=><=>\nYes0303030312213030303030\nNo 0716253434343443526170\nGini0.4200.4000.3750.3430.4170.4000.3000.3430.3750.4000.420 \nSplit Positions\nSorted Values\n\nEntropy –measures information (INFO)\nCalculate Entropy for node t : \n…where p(j | t) the relative frequency of class j at \nnode t. (note log= base-2 logarithm i.e. log\n2)\nMeasures homogeneity of a node\n›Maximum = log n\ncwhen all records of node equally \ndistributed among classes\n›Minimum = 0.0 when all records of node belong to \none class\n\nEntropy measures information in a node\n›Yes, you can measure amount of \ninformation!\n›Intuitively:when all records of node belong \nto one class, implies most information. Wen \nrecords of node belong to different classes, \nleast information\n›Try to maximize amount of information gain\n›Entropy based computations similar to Gini\nIndex computations.\n\nExamples: calculating the Entropy for \nvarious nodes\nP(C1) = 0/6 = 0     P(C2) = 6/6 = 1 (rel. fr/prob.)\nEntropy = –0log\n20 –1log\n21 = 0 ( note: 0log0 = 0 )\nClass 1 (C1) 0\nClass 2 (C2) 6\nA node with: 0 rec in class \n0, 6 rec in class 1\nClass 1 (C1) 1\nClass 2 (C2) 5\nP(C1) = 1/6          P(C2) = 5/6\nEntropy = –(1/6)log\n2 (1/6) –(5/6)log\n2 (5/6) = \n0.65 \nClass 1 (C1) 2\nClass 2 (C2) 4\nP(C1) = 2/6          P(C2) = 4/6\nEntropy = –(2/6)log\n2 (2/6) –(4/6)log\n2 (4/6) = \n0.92\n\nEntropy of split\n›Assume node pis split into k children, then\nEntropy of split (or Entropy of attribute): \n…where n\ni= number of records at node i, \nEntropy(i) = Entropy of node/child i, n = \nnumber of records at node p.\nNote: Compare to GiniIndex of Split: \nSimilar!\n\nInformation gain\n›Measuring the gain of information when \nsplitting on an attribute. Assume parent node \np split into k partitions (children):\n…where n\ninumber of records at node i, n# of records \nat parent and Entropy(i)entropy of child i(note similar \nto Gini)\nMeasures reduction in entropy because of split. \nChoose split that achieves most reduction (maximizes \nGAIN)\nDisadvantage: Prefers splits resulting in large number \nof “pure” nodes/partitions (pure=low intermixing)\n\nID M N Q R\n1 M1 N3 Q2 R2\n2 M2 N3 Q2 R1\n3 M2 N2 Q1 R1\n4 M1 N2 Q1 R2\n5 M2 N1 Q3 R2\n6 M1 N1 Q3 R2\nTraining Data\nSTEP 1:Compute Entropy for our entire training data \n(Entropy of node): \nEntropy Node = -(2/6)log2(2/6) –(4/6)log2(4/6) = \n0.9182\nSTEP 2:Compute Entropy for each attribute of node \ni.e. split node based on each attribute (split –use \nformula on slide 62):\nEntropy(M) = (3/6)*0 + (3/6)*[ -(2/3)*log2(2/3) –\n(1/3)*log2(1/3)] = 0.4591\nEntropy(N) = (2/6)*0 + (2/6)*1+ (2/6)*1 = 0.6666\nEntropy(Q) = (2/6)*1 + (2/6)*1 + (2/6)*0 = 0.6666\nSTEP 3:Calculate Entropy gains for each attribute:\nEntropy Gain(M) = 0.9182 –0.4591 = 0.4591 (biggest!)\nEntropy Gain(N) = 0.9182 –0.6666 = 0.2516\nEntropy Gain(Q) = 0.9182 –0.6666 = 0.2516\nHence, first split based on attribute M (biggest gain)\n\nID M N Q R\n1 M1 N3 Q2 R2\n2 M2 N3 Q2 R1\n3 M2 N2 Q1 R1\n4 M1 N2 Q1 R2\n5 M2 N1 Q3 R2\n6 M1 N1 Q3 R2\nTraining Data\nM\nM1	M2\nR2	Node N2\nIf M has value M1, then R is \nalways R2. Homogeneous \nnode! Hence no further \nsplitting and make this a \nleaf. \nWe make a rule:\nM1 R2\nR1 2\nR2 1\nHere, non-homogeneous \nnode. Apply same method \nto split based on other \nattribute. Tree will grow at \nthis branch.\nRoot node of tree will be \ntest on attribute M\n\nID M N Q R\n1 M1 N3 Q2 R2\n2 M2 N3 Q2 R1\n3 M2 N2 Q1 R1\n4 M1 N2 Q1 R2\n5 M2 N1 Q3 R2\n6 M1 N1 Q3 R2\nTraining Data	Calculate again following the same steps (leave out red \nrows).\nSTEP 1:Compute Entropy for our new “entire” \nnode/dataset (leave out red-ishrows. These have \nalready been classified): \nEntropy node = -(2/3)log2(2/3) –(1/3)log2(1/3) = 0.9182\nSTEP 2:Compute Entropy for each attribute (use formula \non slide 62):\nEntropy(N) =  (1/3)*0 + (1/3)*0+(1/3)*0 = 0\nEntropy(Q) = (1/3)*0 + (1/3)*0 + (1/3)*0 = 0\nSTEP 3:Calculate Entropy gains:\nGini Gain(N) = 0.9182 –0.0 = 0.9182\nGini Gain(Q) = 0.9182 –0.0 = 0.9182\nEqual Entropy gains. Hence, both attributes are equally \ngood. Choose one of N, Q. Assume we choose N (can \nalso choose Q).\n\nID M N Q R\n1 M1 N3 Q2 R2\n2 M2 N3 Q2 R1\n3 M2 N2 Q1 R1\n4 M1 N2 Q1 R2\n5 M2 N1 Q3 R2\n6 M1 N1 Q3 R2\nTraining Data\nM\nM1	M2\nR2	N\nHomogeneous nodes. Hence, \nno need to split further. Make \nthem leaves with proper class \nvalues (from nodes) and devise \nrules (include incoming M2):\nM2, N1 R2\nM2, N2 R1\nM2, N3 R1\nDataset empty, hence done. \nDecision tree built.\nN1\nN2\nN3\nR2\nR1\nR1\n\nGain ratio \n›alternative way instead of information gain to solve \ninformation gain problems. \nAssume node pis split into k partitions (children)\n… where n\niis number of records in partition i\nAdjust information gain by the entropy of the \npartition. Large number of small partitions is \npenalized (i.e. higher entropy partitions)\nOvercomes disadvantages of Information gain\nUsed in C4.5\n\nExample –Entropy split/ \nattributeTid Refund Marital \nStatus \nTaxable \nIncome Cheat \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n  \nCompute Entropy for “Refund” attribute \n(i.e. compute Entropy of split based on \n“Refund”). Cheat is class!\nIf we split based on “Refund” we get:\nRefund\nYes	No\nCheat=\nYes\n0\nCheat=\nNo\n3\nCheat=\nYes\n3\nCheat=\nNo\n4\n3 out of 10 rec. 7out of 10 rec.\nCheat=\nYes\n3\nCheat=\nNo\n7\nTotal 10 rec\nEntropy(Refund=“yes”) = -(3/3)log(3/3) –(0/3)log(0/3) =0\nEntropy(Refund=“no”) = -(3/7)log(3/7) –(4/7)log(4/7) = 0.9852 \nEntropy(Refund) = (3/10) * 0 + \n(7/10) * 0.9852 = 0.6894 QED\nEntire dataset\n\nExample –Entropy GainTid Refund Marital \nStatus \nTaxable \nIncome Cheat \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 No Single 90K Yes \n10 \n  \nCompute Entropy gain for split on \n“Refund”attribute. Cheat is class!\nRefund\nYes	No\nCheat=\nYes\n0\nCheat=\nNo\n3\nCheat=\nYes\n3\nCheat=\nNo\n4\n3 out of 10 rec. 7out of 10 rec.\nCheat=\nYes\n3\nCheat=\nNo\n7\nTotal 10 rec\nCalculate Entropy for parent node (entire dataset):\nEntropy(dataset) = -(3/10)log(3/10) –(7/10)log(7/10)= \n= 0.8812\nEntropy(Refund) = 0.6894(see previous slide)\nEntropy Gain = 0.8812 –0.6894 = 0.1918QED\nEntire dataset\n\nClassification error at node i\nMeasures misclassification error made by \na node.\n›Maximum = 1-1/n\ncwhen records are equally \ndistributed among all classes\n›Minimum = 0.0, when all records belong to \none class\n\nExamples: calculating the Classification \nerror for various nodes\nP(C1) = 0/6 = 0     P(C2) = 6/6 = 1 (rel. fr/prob.)\nError = 1 –max(0, 1) = 1-1 = 0\nClass 1 (C1) 0\nClass 2 (C2) 6\nA node with: 0 rec in class \n0, 6 rec in class 1\nClass 1 (C1) 1\nClass 2 (C2) 5\nP(C1) = 1/6          P(C2) = 5/6\nError = 1 –max( 1/6, 5/6) = 1 –5/6 = 1/6\nClass 1 (C1) 2\nClass 2 (C2) 4\nP(C1) = 2/6          P(C2) = 4/6\nError = 1 –max( 2/6, 4/6) = 1 –4/6 = 1/3\n\nUsing criteria Entropy and Classification \nerrorto build Decision Trees in the same \nway the Giniindex is used\nID M N Q R\n1 M1 N3 Q2 R2\n2 M2 N3 Q2 R1\n3 M2 N2 Q1 R1\n4 M1 N2 Q1 R2\n5 M2 N1 Q3 R2\n6 M1 N1 Q3 R2\nTraining Data\nSTEP 1:Calculate Entropy/Classification \nerror for system\nSTEP 2:For each attribute compute \nEntropy/Classification error (Entropy \nsplit/attribute etc)\nSTEP 3:Calculate gains. Choose \nattribute with biggest gain and make it \nnode….\n… etc…\n\nFor a 2-class problem (Q: why 2-class?)\n\nStop expanding when training set is empty\nStop expanding a node when all the \nrecords belong to the same class\nStop expanding a node when all the \nrecords have similar attribute values\nEarly termination\n\nAccuracy\n›Correctly identified categories (all)\nPrecision\n›Proportion of positive class identification was correct = TP / (TP + \nFP) i.e. \nRecall\n›Proportion of actual positives was correct = TP / (TP + FN) i.e. how \nmuch of actual positive were identified.\nF-measure (0 to 1) the greater the better.\n›Single number for Precision and Recall: F-measure = \n(2*Precision*Recall)/(Precision + Recall)  i.e. harmonic mean!\nPredicted classby model\nPositive Negative\nTrue class it \nbelongsto\nPositive True PositiveFalse Positive\nNegative False NegativeTrue Negative\nPrecision\nRecall\nImportant:Above is a 2 class problem. Yet, can generalize \nAccuracy/Precision/Recall to problems with more than 2 classes.\nNote on terminology: in 2 class problems \n(binary class problems), classes are \nusually referred to as positive/negative. \nCould also use Class0/Class1 .\n\nWhen to use Precision, Recall, F-measure?\n›When your classification dataset is imbalanced\nWhen the distribution of rows in known classes is \nbiased or skewed in the training set!\nMore clearly: don’t have the same amount of obsin each \nclass in your training set. Severe \n›Type of research\nE.g. In medicine, false negativemuch more important \nthan say false positive. Hence, Recallmuch more \nimportant. For YT recommendations, Precisionbetter.\n›If balanced, accuracy is fine.\nPredicted classby model\nPositive Negative\nTrue class it \nbelongsto\nPositive True PositiveFalse Positive\nNegative False NegativeTrue Negative\n\nIn R, two ways of building and using \nDecision Trees\n›Using the rpart package\nRecursive partitioning for classification\nDocumentation\nSee:https://cran.r-project.org/web/packages/rpart/rpart.pdf\n›Using the tree package\nExample in R presented in next slides\nCode in next slide(s) uses Carseatsdataset of package \nISLR, a simulated dataset containing sales of child car \nseats at 400 different stores\n\n#includes the Carseatsdataset, a simulated dataset containing sales of child car seats at 400 different stores\nlibrary(ISLR)\n#library for Classification and Regression Trees\nlibrary(tree)\n#Add Carseatsdataset to R’s path making thus Carseatsdataset available to R\nattach(Carseats)\n#Take a quick look at the data (peek at data)\nhead(Carseats)\n# The aim of this example is to predict the value for the Sales attribute in the 	Carseatsdataset (i.e. class=Sales). However, \n# Sales attribute is continuous hence we have to transform it into a discrete (=categorical) variable. \n#take a look at the values of attribute Sales in the Carseatsdataset\nrange(Sales)\n#Sales is a continuous attribute ranging from 0.00 up until 16.27. We make an assumption and say that if Sales >= 8 then Sale	s is High,\n#otherwise not.\nHigh = ifelse(Sales>=8, \"Yes\", \"No\")\n# Now High is a list containing Yes/No values, one for each record in dataset\n# We must now attach High to the Carseatsdataset, increasing its dimension by 1.\nCarseats= data.frame(Carseats, High)\n# The Sales attribute is no longer needed, since we have transformed it into a discrete value as specified by High. Sales\n# is the first attribute of the Carseatsdataset, so get rid of it (i.e. remove it from the Carseatsdataset). Dimension reduces by 1\nCarseats= Carseats[,-1]\n# We have made our class attribute a distinct value. Now, select randomly some records from 	Carseatsin order to create the training\n# dataset\nset.seed(2) # initialize random number generator. Note: if you keep 2, the same records will always be selected\n# Create training set. Get 200 random numbers from 1 to 400.\ntrain = sample( 1:nrow( Carseats), nrow(Carseats)/2)\ntest = -train #the rest will be our testing data\n# Train contains the indexes of the records in Carseatsthat will be included into the training set. Create the actual dataset\ntraining_data= Carseats[train,]\n# Create the decision tree using the training data. We want to predict High based on\n# all other attributes of the Carseatsdataset\ntree_model= tree(High~., training_data)\n# Decision tree built. Variable tree_modelholds now our Decision tree as it has been created from the training set. Now visualize it\nplot(tree_model)\n# Plot does now show labels on Decision Tree. Plot tree with labels to make it easily understandable.\ntext(tree_model, pretty=0)\n# Next, use the testing data to test the Decision tree referenced by tree_model\nBuilding/traininga decision tree in R (tree library)\n\nUsing Decision tree to classify testing data in R (tree library)\n#...continued from previous slide…\n# Now that we have built/trained our decision tree, apply it on the testing dataset\n# First, select records from Carseatsthat will comprise our testing dataset. Note:\n# the testing dataset has the High attribute but we will not remove it . This is \n# because we will need this to calculate accuracy and error rate. In addition, the \n# tree library will ignore this attribute anyway.\ntesting_data = Carseats[test,]\n# Peek at testing data \nhead(testing_data)\n# Predict the class attribute (High) for the testing dataset. Apply testing dataset\ntree_predict= predict(tree_model, testing_data, type=\"class\")\n# Prediction done. Now tree_predictis a one dimensional data structure (separate\n# from testing dataset) that holds one value “Yes”/”No” for each record in testing \n# set. I.e. the first value in tree_predictcorresponds to the first record in \n# testing set.\n# Now, try to evaluate how well our testing data was classified by calculating the \n# Confusion Matrix. There are two ways to do this:\n# Using the \'table\' command, which\n# compares tree_predictand attribute High from testing dataset like this:\ntestingDataConfusionTable = table(tree_predict, testing_data$High)\n# Ok, let\'s calculate some quality metrics of the model, in order to see how\n# our model performed. We calculate accuracy and error rate of the classifier/	mocel\n# Calculate accuracy\nmodelAccuracy = sum( diag(testingDataConfusionTable )/sum(testingDataConfusionTable ))\n# Calculate Error rate (note could also use 1 -modelAccuracy)\nmodelErrorRate = 1 -sum( diag(testingDataConfusionTable )/sum(testingDataConfusionTable ))\n# Print the result out nicely. We loooooooooove nice and clear responses.\nsprintf(\"Model accuracy: %f, model error rate:%f\", modelAccuracy,modelErrorRate )\n# You can also use a different library (instead of table) for calculating the confusion matrix\nlibrary(caret) #needed for confusion matrix. Install this package (may take a while)\n# Compare tree_predictand attribute High from testing dataset by showing the confuction\n# matrix. Look at matrix and accuracy\nconfusionMatrix(tree_predict, testing_data$High)\n\nImportant aspects of classification using \nDecision Trees\n›Underfittingand Overfitting\n›How to cope with missing values?\n›Costof classification\n\nThere are two types of errors when dealing \nwith decision trees\n›Training errors (aka resubstitutionerrors aka \napparent error)\nNumber of misclassifications of records in the \ntraining set\n›Generalization errors\nExpected errors of misclassifications when model is \napplied on unknown records (or the testing set)\nHow to measure misclassification? (see \nconfusion matrix)\n›Absolute number of records wrongly classified\n›Pctof records wrongly classified\n\nMisclassification on training set? Isn’t this \nimpossible?\n›Given consistent training set, will algorithms \nproduce zero error on training set?\n›Considering termination conditions:\nTraining set empty: zero error\nAll records have the same class: zero error\nNo attributes left to split: impossible(consistent \ntraining set)\nNo attribute with positive information gain: possible \nbur unusual\n›So, in general, under such circumstance it is \nimpossible(occasional possible)\n\nMisclassification on training set? Isn’t this \nimpossible?\n›Assume now inconsistent training set\n›Consider termination conditions\nTraining set empty: zero error\nAll instances have the same class: zero error\nNo attributes left to split on: inconsistent class. \nChoosing most common class minimizes error\nNo attribute has positive information gain: possible \nbut unusual\n›So, in such situations, it is possible. You have a \nminimum error on the classification of the \ntraining set (training error)\n\nReasons for errors on the training set\n›Training set is not a good sample\n›(consistent) Noiseon some attributes\n›Missingattribute values\n›Some class values present in small amounts\n\nAre training errors and generalization \nerrors–for a particular model-related?\nIs the following true?\n›“Assume two models (also called \nHypothesis) A and B over the same training \nset. If training_error(A) < training_error(B) then \nGeneralization_error(A) < \nGeneralization_error(B)” ?\nNo. That’s WRONG!\n\nUnderfitting\n›When both training errors and generalization \nerrors are large.\nHappens when Decision Tree is too simple (i.e. \nsmall number of nodes)\nWhen Decision tree has few nodes, it can’t \ndeduce the structure/relationships of attributes\nSome data will not fit. Hence errors.\n\nOverfitting\n›When training error is very small but \ngeneralization errors are large\nHappens when the Decision Tree adapts too \nwell on the training data. I.e. perfectly fits \ntraining data and hence works only for \ntraining data !\nHappens when Decision Tree grows large and \ncomplex (large number of nodes)\n\nOverfitting\n›Testing errors  \ndecrease at \nthe expense \nof the \ngeneralization \nerror!\nOverfittingUnderfitting\n\nOverfittingresults in decision trees that \nare more complex than necessary\n›Complex? => More nodes\nTraining error no longer provides a good \nestimateof how well the tree will perform \non previously unseen records\nNeed new ways for estimating errors\n›Important step to improve complex decision \ntrees\n\nAssume\n›e(t)= Error on training set\n›e’(t) = Error on testing/unknown set (same \nmodel)\nTwo approaches to estimating \ngeneralization errors\n›Optimistic approach\nAssumes that e(t) = e’(t)\nAs discussed, rather not good estimation\nEspecially for complex decision trees due to overfitting\n\nTwo approaches to estimating generalization \nerrors (cont.)\n›Pessimistic approach\nThe idea: give complex decision trees a penalty and \ncalculate a pessimistic/increased error due to this \ncomplexity\nAffects leaves of tree\nPessimistic error for a decision tree (or subtree!) T, e\ng(T) \n…where e(T) = generalization error of decision tree, \nN\nt= number of records in training set, n\nl= number of \nleaves in decision tree and c= penalty (usually 0.5)\n\nExample\nB\nYes	No\nNode N1 Node N2\nN1\nC1 5\nC2 2\nN2\nC1 1\nC2 4\nB\nC1 6\nC2 6\nAssume error: 3/12\nEstimated pessimistic error for decision tree:\ne\ng(T) = (3+0.5*2) / 12 = 0.333\n(Note:if there were 3 “children”/nodes (same #records and \nerror): (3+0.5*3)/12 = 0.375(increased penalty for \ncomplexity)\n\nTree pruning\n›Pruning?Get rid/remove of some sections of \nthe decision tree\n›Reduces the complexity of the tree (due to \nremoval of nodes)\n›That way improves accuracy and addresses \noverfitting\nTwo approaches to pruning\n›Pre-pruning\n›Post-pruning\n\nPre-Pruning (Early Stopping Rule)\n›Stop the algorithm before it becomes a fully-grown \ntreeduring the training phase\n›Typical stopping conditions for a node:\nStop if all instances belong to the same class\nStop if all the attribute values are the same\n›More restrictive conditions:\nStop if number of instances is less than some user-specified \nthreshold\nApply \n2\ntestto check if class distribution of instances are \nindependent of the available features. If so, stop\nStop if expanding the current node does not improve \nimpurity measures(e.g., Ginior information gain).\n\nPost-pruning\n›Growdecision tree to its entirety from the \ntraining set\n›Trim the nodesof the decision tree in a \nbottom-up fashion\n›If generalization error improves after \ntrimming, replace sub-tree by a leaf node\nUse of pessimistic error\n›Class label of leaf node is determined from \nmajority class of instances in the sub-tree\n›Can use other methods e.g. MDL for post-\npruning\n\nA?\nA1\nA2 A3\nA4 Class = Yes20\nClass = No10\nError = 10/30\nAssume fully grown tree from training set\nTraining Error (Before splitting) = 10/30\nPessimistic error = (10 + 0.5)/30 = 10.5/30\nTraining Error (After splitting) = 9/30\nPessimistic error (After splitting) = \n= (9 + 4 * 0.5) / 30 = 11/30\nPRUNE THIS NODE! Replace \nwith leaf node “Yes” (majority in subtree)\nClass = Yes8\nClass = No4\nClass = Yes3\nClass = No4\nClass = Yes4\nClass = No1\nClass = Yes5\nClass = No1\nNode exists in decision tree, \n(forms subtree) as created \nfrom dataset.\nShould we prune it?\n\nWhat does penalty of 0.5 mean?\n›Means that for binary split, a node should \nalways be grown along children if \nclassification improves at least one record\nIs in general cheaper\n\nMissing values affect decision tree \nconstruction in three different ways:\n›Affects how impurity measures are \ncomputed\n›Affects how to distribute instance with \nmissing value to child nodes\n›Affects how a test instance with missing value \nis classified\n\nTid Refund Marital \nStatus \nTaxable \nIncome Class \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 ? Single 90K Yes \n10 \n  Split on Refund:\nEntropy(Refund=Yes) = 0\nEntropy(Refund=No) =\n= -(2/6)log(2/6) –(4/6)log(4/6) = 0.9183\nEntropy(Children) \n= 0.3 (0) + 0.6 (0.9183) = 0.551\nEntropy Gain = 0.9 * (0.8813 –0.551) = 0.3303\nBefore Splitting:\nEntropy(Parent)\n= -0.3 log(0.3)-(0.7)log(0.7) = 0.8813\nClass=YesClass=No\nRefund=Yes 0 3\nRefund=No 2 4\nRefund=? 1 0\nProbability of this \ngain\nMissing value\n\nTid Refund Marital \nStatus \nTaxable \nIncome Class \n1 Yes Single 125K No \n2 No Married 100K No \n3 No Single 70K No \n4 Yes Married 120K No \n5 No Divorced 95K Yes \n6 No Married 60K No \n7 Yes Divorced 220K No \n8 No Single 85K Yes \n9 No Married 75K No \n10 \n  Refund\nYes	No\nRefund\nYesTid Refund Marital \nStatus \nTaxable \nIncome Class \n10 ? Single 90K Yes \n10 \n  	No\nProbability that Refund=Yes is 3/9\nProbability that Refund=No is 6/9\nAssign record with unknown Refund to the \nleft child with weight = 3/9 and to the \nright child with weight = 6/9\nClass=Yes 0+3/9\nClass=No 3\nClass=Yes 2+6/9\nClass=No 4\nClass=Yes 0\nClass=No 3\nClass=Yes 2\nClass=No 4\n\nRefund\nMarSt\nTaxInc\nYESNO\nNO\nNO\nYes\nNo\nMarried\nSingle, \nDivorced\n< 80K > 80K\nMarriedSingleDivorcedTotal\nClass=No 3 1 0 4\nClass=Yes 6/9 1 1 2.67\nTotal3.67 2 1 6.67Tid Refund Marital \nStatus \nTaxable \nIncome Class \n11 No ? 85K ? \n10 \n  \nNew/unknown record:\nProbability that Marital Status \n= Marriedis 3.67/6.67\nProbability that Marital Status \n={Single,Divorced} is 3/6.67\n\nClassificationis the problem of predicting the \nvalue of a categorical attribute (called the \nclass) from a set of \ncategorical/discrete/continuous attributes\nDecision Trees are one form of solving this \nproblem\n›Easy to understand\n›Easy to implement\n›Easy to use\n›Computationally cheap\nDecision Trees are created (grown) from training \nsets, where the class is known and applied to \ntesting and unknown data\nDecision Trees have also problems though\n›Most important one is Overfitting\n\nProof of the “GiniIndex of node” formula:\nAssume m classes, with i{1,2,3,…,m} denoting the class and f\nithe fraction \nof items of a node labelled as belonging to class i.\nAssume now a randomly selected record labelled as belonging to class i. \nThe probability of this random record to NOT belong to class iis (1-f\ni). \nSince all records are not drawn with equal probability, the probability of \nselecting an item labelled as belonging to class iis f\ni. Hence, drawing a \nrandom record with label i, and the selected record not belonging to class i\nhas probability  P(i) = f\ni(1-f\ni) \nwhere P(i) means the probability of  selecting record labelled as belonging \nto i, and record does not belong to class I (i.e. is error)\nThe total probability of error will hence be: \n=෍\n=1\n\n\n1−\n=෍\n=1\n\n\n−\n\n2\n=෍\n=1\n\n\n−෍\n=1\n\n\n\n2\n=&#3627409359;−෍\n=&#3627409359;\n\n\n\n&#3627409360;\n&#3627408451;1&#3627408450;2&#3627408450;3&#3627408450;…&#3627408450;=&#3627408451;1+&#3627408451;2+&#3627408451;3+…+&#3627408451;=\n\nClassification and Regression Trees. L. Breiman, \nJ. H. Friedman, R. A. Olshen, and C. J. Stone. \nWadsworth, Belmont, CA, 1984.\nC4.5: Programs for Machine Learning (Morgan \nKaufmann Series in Machine Learning) by J. \nRoss Quinlan\nLearning Classification Trees, Wray Buntine, \nStatistics and Computation (1992), Vol 2, pages \n63-73\nOn the Boosting Ability of Top-Down Decision \nTree Learning\nAlgorithms. Kearns and Mansour,, STOC: ACM \nSymposium on Theory of Computing, 1996“', 5236, 1, '2025-06-04 10:23:58', '2025-06-04 10:23:58');
INSERT INTO `documents` (`id`, `title`, `filename`, `file_hash`, `filepath`, `content`, `size`, `category_id`, `created_at`, `updated_at`) VALUES
(72, 'ملف عقاري', 'عقد ايجار شقة سكنية.docx', NULL, 'documents/30tCR6h4R5pmVgp0RQHeGrGfUC8qPXf0Ol5v7qo9.docx', 'بسم الله الرحمن الرحيم عقد ايجار   . انه بتاريخ اليوم  .................  الموافق       /     /      20  م تم تحريره بين كلا من : الفريق الاول :ـ                                               هوية رقم (  ……….…….  )       ( مؤجر) الفريق الثاني :                                              هوية رقم ( …………….... )       (مستأجر)  المقدمة : حيث أن الفريق الاول يملك ويتصرف  .................... ا لواقعة بالعمارة السكنية ..................... والمقامة على أرض القسيمة ................. القطعة رقم ..................... ويحدها من الشمال .............. ومن الجنوب .................. ومن الشرق ...................... ومن الغرب......................  في شارع ..................... حي ........................ وحيث أن الفريق الثاني  ي رغب في استئجار  .....................  الواقعة بالعمارة السكنية  المذكورة اعلاه   من الفريق الاول. لذا فقد تم الاتفاق بين الفريقين المذكورين اعلاه  بإيجاب  وقبول كل منهما بعد أن اقرا  بأهليتهما  القانونية للتعاقد والتصرف على ما يلي : أولاً : مقدمة العقد : تعتب ر مقدمة هذا العقد جزء لا يتجزأ منه وتقرأ معه. ثانياً: موضوع  الإجا ر : بموجب هذا العقد أجر الفريق الاول للفريق الثاني القابل  لمحل الايجار المذكور والموصوف أعلاه  ثالثاً: مدة  الإجار : م دة هذه  الإيجار ........................   تبدأ من تاريخ    ........./......../..........م  وتنتهي بتاريخ  ....../......../........م    قابلة للتجديد بموافقة الفريقين. رابعاً: بدل  الإيجار  :  اتفق الفريقان على أن تكون القيمة الايجارية ( ..... ........ .....  )  وبالحروف ......................... تكون ( شهري/ سنوي) . خامساً: كيفية استعمال المأجور : اتفق الفريقان  على ان يستعمل الفريق ا لثاني العين المؤجرة  المذكورة والموصوفة أعلاه لغرض.................... . يقر الفريق الثاني بانه عاين العين المؤجرة موضوع هذا العقد معاينة تامة نافية للجهالة وانها بحالة جيدة وصالحة للاستعمال . يلتزم المستأجر بالمحافظة على المأجور وسلامته ومراعاته كما يراعي الشخص ماله الخاص، وأن يمتنع عن  اجراء أي تغيير او هدم او بناء او تقسيم حجرات او فتح نوافذ او سد ابواب دون إذن خطي من المؤجر. سادساً: شروط خاصة: يلتزم الفريق الثاني بعدم تأجير العين المؤجرة موضوع العقد لغيره من الباطن ولا يجوز له التنازل لغيره عن كل او جزء من العن المؤجرة وإلا اعتبر العقد مفسوخاً من تلقاء نفسه. يلتزم الفريق الاول بتسليم العين المؤجرة موضوع العقد للفريق الثاني بداية مدة الايجار ويمكنه من الانتفاع بها بالشكل القانوني بدون مضايقة او معارضة من احد. لا يجوز للمستأجر التأخير عن دفع الاجرة في موعدها او تأجيله   . في حال امتنع المستأجر عن دفع الأجرة أو تأخر في دفعها فيحق للفريق الأول فسخ العقد تلقائياً بعد إشعار المستأجر. سابعاً:   اذا نقص شيء من المأجور او تضرر او انكسر او فقد كمفاتيح او ابواب او شبابيك نتيجة اهمال المستأجر وما يتلف بحكم الاستعمال من شبكة المياه والكهرباء وما شابه ذلك يقتضي على المستأجر اعادته  او اصلاحه او تأدية بدله للمؤجر بحيث يصير تسليم المأجور حسب هيئته الاصلية  ثامناً:  كل ما لم يذكر في هذا العقد يخضع  لأحكام  القانون المطبق في مناطق السلطة الوطنية الفلسطينية مع سريان بنوده وشروطه. تاسعاً:  شروط إضافية: 1................................................................................................................................. 2................................................................................................................................. 3................................................................................................................................. الخاتمة :  على الشروط المحررة اعلاه تم اعداد ثلاثة نسخ من هذا العقد وجرى التوقيع عليها امام شهود الحال وبرضاء وموافقة الفريقين دون ضغط او اكراه من احدهما على الاخر او م ن الغير بعد ان تفهما وأقرا مضمون  العقد وتعهدا بالعمل بموجب  شروطه وبعدم مخالفتها ، وتوضع بيد كل منهما نسخه ونسخه لدى  مكتب المحامي/  رفيق توفيق الغولة. والله خير الشاهدين ،،،،، فريق أول                                                                                 فريق ثاني شاهد                                                                                       شاهد اصادق  أنا المحامي  هاني مؤمن أبوريالة  على صحة ما جاء بالعقد   و على صحة التواقيع  الواردة فيه', 18, 4, '2025-06-04 10:24:57', '2025-06-04 10:24:57');

-- --------------------------------------------------------

--
-- بنية الجدول `failed_jobs`
--

CREATE TABLE `failed_jobs` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `uuid` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `connection` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `queue` text COLLATE utf8mb4_unicode_ci NOT NULL,
  `payload` longtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `exception` longtext COLLATE utf8mb4_unicode_ci NOT NULL,
  `failed_at` timestamp NOT NULL DEFAULT current_timestamp()
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- --------------------------------------------------------

--
-- بنية الجدول `keywords`
--

CREATE TABLE `keywords` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `word` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `category_id` bigint(20) UNSIGNED NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- إرجاع أو استيراد بيانات الجدول `keywords`
--

INSERT INTO `keywords` (`id`, `word`, `category_id`, `created_at`, `updated_at`) VALUES
(1, 'hospital', 1, NULL, NULL),
(2, 'medical', 1, NULL, NULL),
(3, 'healthcare', 1, NULL, NULL),
(4, 'treatment', 1, NULL, NULL),
(5, 'diagnosis', 1, NULL, NULL),
(6, 'surgery', 1, NULL, NULL),
(7, 'disease', 1, NULL, NULL),
(8, 'medicine', 1, NULL, NULL),
(9, 'doctor', 1, NULL, NULL),
(10, 'nurse', 1, NULL, NULL),
(11, 'education', 2, NULL, NULL),
(12, 'student', 2, NULL, NULL),
(13, 'teacher', 2, NULL, NULL),
(14, 'school', 2, NULL, NULL),
(15, 'university', 2, NULL, NULL),
(16, 'curriculum', 2, NULL, NULL),
(17, 'exam', 2, NULL, NULL),
(18, 'classroom', 2, NULL, NULL),
(19, 'learning', 2, NULL, NULL),
(20, 'lecture', 2, NULL, NULL),
(21, 'technology', 3, NULL, NULL),
(22, 'computer', 3, NULL, NULL),
(23, 'software', 3, NULL, NULL),
(24, 'hardware', 3, NULL, NULL),
(25, 'internet', 3, NULL, NULL),
(26, 'AI', 3, NULL, NULL),
(27, 'robotics', 3, NULL, NULL),
(28, 'cybersecurity', 3, NULL, NULL),
(29, 'programming', 3, NULL, NULL),
(30, 'data', 3, NULL, NULL),
(31, 'real estate', 4, NULL, NULL),
(32, 'property', 4, NULL, NULL),
(33, 'apartment', 4, NULL, NULL),
(34, 'building', 4, NULL, NULL),
(35, 'rent', 4, NULL, NULL),
(36, 'lease', 4, NULL, NULL),
(37, 'mortgage', 4, NULL, NULL),
(38, 'land', 4, NULL, NULL),
(39, 'broker', 4, NULL, NULL),
(40, 'sale', 4, NULL, NULL),
(41, 'صحي', 1, '2025-06-04 08:43:35', '2025-06-04 08:43:35'),
(43, 'العقد', 4, '2025-06-04 09:01:58', '2025-06-04 09:01:58');

-- --------------------------------------------------------

--
-- بنية الجدول `migrations`
--

CREATE TABLE `migrations` (
  `id` int(10) UNSIGNED NOT NULL,
  `migration` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `batch` int(11) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- إرجاع أو استيراد بيانات الجدول `migrations`
--

INSERT INTO `migrations` (`id`, `migration`, `batch`) VALUES
(23, '2014_10_12_000000_create_users_table', 1),
(24, '2014_10_12_100000_create_password_resets_table', 1),
(25, '2019_08_19_000000_create_failed_jobs_table', 1),
(26, '2019_12_14_000001_create_personal_access_tokens_table', 1),
(27, '2025_05_24_101405_create_categories_table', 1),
(28, '2025_05_24_102405_create_documents_table', 1),
(29, '2025_05_24_115349_add_filename_to_documents_table', 1),
(30, '2025_05_24_120203_add_filename_and_filepath_to_documents_table', 1),
(31, '2025_05_25_180852_make_category_id_nullable_in_documents', 1),
(32, '2025_05_25_182442_create_keywords_table', 1),
(33, '2025_05_25_184148_modify_keywords_nullable_in_categories_table', 1),
(34, '2025_06_01_195217_add_size_to_documents_table', 2),
(35, '2025_06_04_084929_add_unique_to_documents_filename', 3),
(36, '2025_06_04_103403_add_file_hash_to_documents_table', 4),
(37, '2025_06_04_103525_drop_unique_from_filename_on_documents_table', 5),
(38, '2025_06_04_135340_add_role_to_users_table', 6),
(39, '2025_06_04_140908_add_is_admin_to_users_table', 7);

-- --------------------------------------------------------

--
-- بنية الجدول `password_resets`
--

CREATE TABLE `password_resets` (
  `email` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `token` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `created_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- إرجاع أو استيراد بيانات الجدول `password_resets`
--

INSERT INTO `password_resets` (`email`, `token`, `created_at`) VALUES
('alaa19966new@gmail.com', '$2y$10$B9uc2CMijeZORIVryQCRZO7KF4V7eseYn29OefInxvVtMROsnVh1i', '2025-06-03 10:53:55');

-- --------------------------------------------------------

--
-- بنية الجدول `personal_access_tokens`
--

CREATE TABLE `personal_access_tokens` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `tokenable_type` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `tokenable_id` bigint(20) UNSIGNED NOT NULL,
  `name` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `token` varchar(64) COLLATE utf8mb4_unicode_ci NOT NULL,
  `abilities` text COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `last_used_at` timestamp NULL DEFAULT NULL,
  `expires_at` timestamp NULL DEFAULT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

-- --------------------------------------------------------

--
-- بنية الجدول `users`
--

CREATE TABLE `users` (
  `id` bigint(20) UNSIGNED NOT NULL,
  `name` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `email` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `email_verified_at` timestamp NULL DEFAULT NULL,
  `password` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL,
  `remember_token` varchar(100) COLLATE utf8mb4_unicode_ci DEFAULT NULL,
  `created_at` timestamp NULL DEFAULT NULL,
  `updated_at` timestamp NULL DEFAULT NULL,
  `role` varchar(255) COLLATE utf8mb4_unicode_ci NOT NULL DEFAULT 'user',
  `is_admin` tinyint(1) NOT NULL DEFAULT 0
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci;

--
-- إرجاع أو استيراد بيانات الجدول `users`
--

INSERT INTO `users` (`id`, `name`, `email`, `email_verified_at`, `password`, `remember_token`, `created_at`, `updated_at`, `role`, `is_admin`) VALUES
(1, 'علاء دلول', 'iuggaza1999996@gmail.com', NULL, '123456789', NULL, NULL, NULL, 'user', 0),
(3, 'علاء الدين يحيي محمد دلول', 'iuggaza1996@hotmail.com', NULL, '$2y$10$njh2XQj4y6/7k6KCBTZMluW.GMYdohMnB5DU2CiQczTYg5vAZOzEO', NULL, '2025-06-01 10:36:43', '2025-06-01 10:36:43', 'user', 0),
(4, 'دانا احمد عودة كرم', 'alaa19966new@gmail.com', NULL, '$2y$10$grV8pUwjX2bg4po5H4Ey6u.rSya8.J9110GPQEIIonqXB5iYmF1Oa', NULL, '2025-06-03 10:05:20', '2025-06-03 10:05:20', 'user', 1);

--
-- Indexes for dumped tables
--

--
-- Indexes for table `categories`
--
ALTER TABLE `categories`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `categories_name_unique` (`name`);

--
-- Indexes for table `documents`
--
ALTER TABLE `documents`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `documents_filename_unique` (`filename`),
  ADD UNIQUE KEY `documents_file_hash_unique` (`file_hash`),
  ADD KEY `documents_category_id_foreign` (`category_id`);

--
-- Indexes for table `failed_jobs`
--
ALTER TABLE `failed_jobs`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `failed_jobs_uuid_unique` (`uuid`);

--
-- Indexes for table `keywords`
--
ALTER TABLE `keywords`
  ADD PRIMARY KEY (`id`),
  ADD KEY `keywords_category_id_foreign` (`category_id`);

--
-- Indexes for table `migrations`
--
ALTER TABLE `migrations`
  ADD PRIMARY KEY (`id`);

--
-- Indexes for table `password_resets`
--
ALTER TABLE `password_resets`
  ADD PRIMARY KEY (`email`);

--
-- Indexes for table `personal_access_tokens`
--
ALTER TABLE `personal_access_tokens`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `personal_access_tokens_token_unique` (`token`),
  ADD KEY `personal_access_tokens_tokenable_type_tokenable_id_index` (`tokenable_type`,`tokenable_id`);

--
-- Indexes for table `users`
--
ALTER TABLE `users`
  ADD PRIMARY KEY (`id`),
  ADD UNIQUE KEY `users_email_unique` (`email`);

--
-- AUTO_INCREMENT for dumped tables
--

--
-- AUTO_INCREMENT for table `categories`
--
ALTER TABLE `categories`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=18;

--
-- AUTO_INCREMENT for table `documents`
--
ALTER TABLE `documents`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=73;

--
-- AUTO_INCREMENT for table `failed_jobs`
--
ALTER TABLE `failed_jobs`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT;

--
-- AUTO_INCREMENT for table `keywords`
--
ALTER TABLE `keywords`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=44;

--
-- AUTO_INCREMENT for table `migrations`
--
ALTER TABLE `migrations`
  MODIFY `id` int(10) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=40;

--
-- AUTO_INCREMENT for table `personal_access_tokens`
--
ALTER TABLE `personal_access_tokens`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT;

--
-- AUTO_INCREMENT for table `users`
--
ALTER TABLE `users`
  MODIFY `id` bigint(20) UNSIGNED NOT NULL AUTO_INCREMENT, AUTO_INCREMENT=5;

--
-- قيود الجداول المحفوظة
--

--
-- القيود للجدول `documents`
--
ALTER TABLE `documents`
  ADD CONSTRAINT `documents_category_id_foreign` FOREIGN KEY (`category_id`) REFERENCES `categories` (`id`) ON DELETE SET NULL;

--
-- القيود للجدول `keywords`
--
ALTER TABLE `keywords`
  ADD CONSTRAINT `keywords_category_id_foreign` FOREIGN KEY (`category_id`) REFERENCES `categories` (`id`) ON DELETE CASCADE;
COMMIT;

/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
